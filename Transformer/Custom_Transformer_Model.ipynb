{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "5cdujyw4Tzcc"
      ],
      "gpuType": "L4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Abstractive Text Summarization using Custom Transformer model with Multiheaded Attention"
      ],
      "metadata": {
        "id": "jc10m60e1SE8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TOk077ygpZM"
      },
      "source": [
        "## For Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5RM0X0a6TVMM"
      },
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mounting the google drive"
      ],
      "metadata": {
        "id": "0e_ggqOR1a-z"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mGd4mT34Tmsc",
        "outputId": "b2109ee8-8d62-4c70-8887-ccf59de6d5d2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cef7T0rTaV_"
      },
      "source": [
        "## Module Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        },
        "id": "iiOa4M86TZhk",
        "outputId": "b051c94e-f4dd-49c3-ea47-b0e9b3101f22"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import numpy as np\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from matplotlib import pyplot as plt\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import glob\n",
        "import itertools\n",
        "import pickle\n",
        "import re\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "!pip install rouge-score"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.25.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->rouge-score) (4.66.4)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=1c913bb80b28ca50fccf5629b3e465cea7151c0a394e6df550a5835c4bdf90fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Pre-Processing"
      ],
      "metadata": {
        "id": "4X3y_3sI1gqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the dataset"
      ],
      "metadata": {
        "id": "07nO1MWd2COr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = '/content/drive/MyDrive/DLProject/cnn_dailymail/train.csv'\n",
        "test_path = '/content/drive/MyDrive/DLProject/cnn_dailymail/test.csv'\n",
        "val_path = '/content/drive/MyDrive/DLProject/cnn_dailymail/validation.csv'\n",
        "\n",
        "# Load datasets\n",
        "train_data = pd.read_csv(train_path)\n",
        "test_data = pd.read_csv(test_path)\n",
        "val_data = pd.read_csv(val_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "buzzwN76runi",
        "outputId": "fd65203d-d7b5-4918-d66c-3d82107059cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive_prefix = '/content/drive/MyDrive/DLProject/cnn_dailymail/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "t2fyHObHr6ux",
        "outputId": "6357793c-c9e8-48b0-eba2-df9bee3ab18d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting into test and train data"
      ],
      "metadata": {
        "id": "ZirCdaXK2GMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting train data\n",
        "X= train_data['article'].values[:100000]\n",
        "Y = train_data['highlights'].values[:100000]\n",
        "\n",
        "# Splitting test data\n",
        "X_Test = test_data['article'].values\n",
        "Y_Test = test_data['highlights'].values\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "3KbNg4d6r2WE",
        "outputId": "69b68742-b62d-40b4-d6e9-e43bfe76e342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "kfZ30b4JTdIs",
        "outputId": "a40c220c-8dd1-46db-9512-e4d3561378c9"
      },
      "source": [
        "#setting up constants\n",
        "STOP_WORDS = set()\n",
        "EMB_SIZE = 300\n",
        "GLOVE_EMB = '/content/drive/My Drive/DLProject/glove.6B.300d.txt'\n",
        "BATCH_SIZE = 32\n",
        "NUM_EVAL_BATCHES = 4\n",
        "NUM_HEADS = 10\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "START_CHAR = 'starttoken'\n",
        "END_CHAR = 'endtoken'\n",
        "PAD_CHAR = 'padtoken'\n",
        "\n",
        "MAX_ARTICLE_LEN = 300 # the article can have at most 300 tokens\n",
        "MAX_LABEL_LEN = 40 # labels can have at most 100 tokens\n",
        "\n",
        "# NN Hyper-parameters\n",
        "E_HIDDEN_DIM = 512\n",
        "D_HIDDEN_DIM = 512\n",
        "KEY_DIM = 64\n",
        "VALUE_DIM = 64\n",
        "LR = 1e-3\n",
        "\n",
        "EP = 10000\n",
        "PRINT_EVERY_EP = 200\n",
        "SAVE_MODEL_EVERY_EP = 5000\n",
        "FORCE_CREATE_DICT = True # force to recreate the word features from scratch, but doesn't recreate the embedding dict, since it doesn't change\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cdujyw4Tzcc"
      },
      "source": [
        "## Helper Functions\n",
        "Here's a breakdown of the helper functions described in the Python script, detailing each function's role:\n",
        "\n",
        "- **`create_word_vec(input_text, prefix='train')`**:\n",
        "  - Creates dictionaries for word-to-index and index-to-word mappings.\n",
        "\n",
        "- **`sentence_to_idx(sentence, word_to_idx)`**:\n",
        "  - Converts a given sentence pair (text and label) into index format using the provided word-to-index dictionary.\n",
        "\n",
        "- **`decontracted(text)`**:\n",
        "  - Cleans the text by expanding common English contractions and removing special characters to standardize the format for easier processing.\n",
        "\n",
        "\n",
        "- **`patch_trg(trg)`**:\n",
        "  - Adjusts the target data for training by creating a modified target sequence for decoder input and a gold standard sequence for loss calculation, aligning it with the output structure expected by sequence-to-sequence models.\n",
        "\n",
        "- **`plot(train_loss, val_loss)`**:\n",
        "  - Plots training and validation loss over epochs to visually monitor the learning progress and compare performance between the training and validation phases.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfRERv1BT03_"
      },
      "source": [
        "#Convert sentences to vectors for input to the model\n",
        "def create_word_vec(input_text, prefix='train'):\n",
        "    word_idx_dict = {} # { word : the index of that word in the dictionary}\n",
        "    idx_word_dict = {} # { index of word : word }\n",
        "\n",
        "    # 1. Create mapping between words and the corresponding embedding values\n",
        "    embed_file_path = drive_prefix + f'{EMB_SIZE}d_embed_dict'\n",
        "    if os.path.exists(embed_file_path):\n",
        "        print('Embedding dictionary exists, loading from file...')\n",
        "        embedding_dict = pickle.load(open(embed_file_path, 'rb'))\n",
        "    else:\n",
        "        embedding_dict = {}\n",
        "\n",
        "        for line in glove:\n",
        "            tokens = line.split()\n",
        "            embedding_dict[tokens[0]] = np.array(tokens[1:], dtype='float32')\n",
        "        pickle.dump(embedding_dict, open(embed_file_path, 'wb'))\n",
        "        print('Saved embedding dictionary')\n",
        "\n",
        "    # 2. Tokenize the input_text and labels\n",
        "    if os.path.exists(drive_prefix + f'{prefix}_word_idx_dict') and not FORCE_CREATE_DICT:\n",
        "        print('Word-to-index dictionary exists, loading from file...')\n",
        "        word_idx_dict = pickle.load(open(drive_prefix + f'{prefix}_word_idx_dict', 'rb'))\n",
        "    if os.path.exists(drive_prefix + f'{prefix}_idx_word_dict') and not FORCE_CREATE_DICT:\n",
        "        print('Index-to-word dictionary exists, loading from file...')\n",
        "        idx_word_dict = pickle.load(open(drive_prefix + f'{prefix}_idx_word_dict', 'rb'))\n",
        "    else:\n",
        "        unique_tokens = set([])\n",
        "        for text, label in input_text:\n",
        "            unique_tokens = unique_tokens.union(word_tokenize(text))\n",
        "            unique_tokens = unique_tokens.union(word_tokenize(label))\n",
        "\n",
        "        for token in unique_tokens:\n",
        "            word_idx_dict[token] = len(word_idx_dict)\n",
        "\n",
        "        # 2.1 Add in the special tokens to the dictionary, note that the START_CHAR and END_CHAR have been added\n",
        "        # during the preprocessing stage\n",
        "        word_idx_dict[PAD_CHAR] = len(word_idx_dict)\n",
        "\n",
        "        idx_word_dict = dict(zip(word_idx_dict.values(), word_idx_dict.keys()))\n",
        "\n",
        "    # 3. Build the word vector for all the words in our dictionary\n",
        "    if os.path.exists(drive_prefix + f'{prefix}_word_vector') and not FORCE_CREATE_DICT:\n",
        "        print('Word Vector exists, loading from file...')\n",
        "        word_vector = pickle.load(open(drive_prefix + f'{prefix}_word_vector', 'rb'))\n",
        "    else:\n",
        "        word_vector = []\n",
        "        for idx, token in idx_word_dict.items():\n",
        "            if token in embedding_dict:\n",
        "                word_vector.append(embedding_dict[token])\n",
        "            # Append the special tokens to the word vector and assign random values\n",
        "            elif token in [START_CHAR, END_CHAR, PAD_CHAR]:\n",
        "                word_vector.append(np.random.normal(0, 1, EMB_SIZE))\n",
        "            # if the token doesn't have an embedding, we set to 0\n",
        "            else:\n",
        "                word_vector.append(np.zeros([EMB_SIZE]))\n",
        "\n",
        "    ## Save the dictionaries\n",
        "    pickle.dump(word_idx_dict, open(drive_prefix + f'{prefix}_word_idx_dict', 'wb'))\n",
        "    pickle.dump(idx_word_dict, open(drive_prefix + f'{prefix}_idx_word_dict', 'wb'))\n",
        "    pickle.dump(word_vector, open(drive_prefix + f'{prefix}_word_vector', 'wb'))\n",
        "\n",
        "    # The index in worvalue_dimec corresponds to the article index in the original X_Train array\n",
        "    return np.array(word_vector), word_idx_dict, idx_word_dict\n",
        "\n",
        "def sentence_to_idx(sentence, word_to_idx):\n",
        "    text, label = sentence\n",
        "    text_tokens = [word_to_idx[token] for token in word_tokenize(text)]\n",
        "    label_tokens = [word_to_idx[token] for token in word_tokenize(label)]\n",
        "    return np.array(text_tokens).astype(int), np.array(label_tokens).astype(int)\n",
        "\n",
        "def decontracted(text):\n",
        "    '''Clean text by removing unnecessary characters and altering the format of words.'''\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"that is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"how's\", \"how is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"n'\", \"ng\", text)\n",
        "    text = re.sub(r\"'bout\", \"about\", text)\n",
        "    text = re.sub(r\"'til\", \"until\", text)\n",
        "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
        "    return text\n",
        "\n",
        "def _process(text):\n",
        "    text = decontracted(text)\n",
        "    text = text.replace('\\\\r', ' ')\n",
        "    text = text.replace('\\\\\"', ' ')\n",
        "    text = text.replace('\\\\n', ' ')\n",
        "    text = re.sub(r\"\\b[a-zA-Z]\\b\", \" \", text)\n",
        "    text = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text)\n",
        "    text = re.sub('[^A-Za-z0-9]+', ' ', text)\n",
        "    text = re.sub('<[^>]*>', '', text)\n",
        "    return text\n",
        "\n",
        "def pre_process(articles, labels):\n",
        "    preprocessed_text = []\n",
        "    # tqdm is for printing the status bar\n",
        "    for i in tqdm(range(len(articles))):\n",
        "        sent = articles[i]\n",
        "        label = labels[i]\n",
        "\n",
        "        sent = _process(sent)\n",
        "        label = _process(label)\n",
        "\n",
        "        sent = sent.lower().strip().split()\n",
        "        label = label.lower().strip().split()\n",
        "\n",
        "        # trim longer items and ignore shorter ones\n",
        "        if len(sent) < MAX_ARTICLE_LEN or len(label) < MAX_LABEL_LEN:\n",
        "            continue\n",
        "\n",
        "        sent = ' '.join(sent[:MAX_ARTICLE_LEN])\n",
        "        label = ' '.join(label[:MAX_LABEL_LEN])\n",
        "        label = f\"{START_CHAR} {label} {END_CHAR}\"\n",
        "        preprocessed_text.append([sent, label])\n",
        "    return preprocessed_text\n",
        "\n",
        "#divide batches of data\n",
        "def get_batch(data):\n",
        "    while True:\n",
        "        for i in range(int((data.shape[0] - BATCH_SIZE) / BATCH_SIZE)):\n",
        "            yield data[i * BATCH_SIZE: i * BATCH_SIZE + BATCH_SIZE]\n",
        "\n",
        "def patch_trg(trg):\n",
        "\n",
        "    trg = trg.to(device)\n",
        "    trg, gold = trg[:, :-1], trg[:, 1:].contiguous().view(-1)\n",
        "    return trg, gold\n",
        "\n",
        "\n",
        "#to plot the test and validation loss\n",
        "def plot(train_loss, val_loss):\n",
        "    plt.plot(train_loss, label='Train')\n",
        "    plt.plot(val_loss, label='Val')\n",
        "    plt.title('Training vs Validation Loss')\n",
        "    plt.xlabel('Episodes')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-YdYEh_T_Au"
      },
      "source": [
        "Divide the data into traina and validation set further"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_Train, X_Val, Y_Train, Y_Val = train_test_split(X, Y, test_size=0.3, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "jcNlesEOuJtl",
        "outputId": "c8c44839-81b7-4ef5-bbdf-0e2de6ed65c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the glove embeddings to be used for tokenization process"
      ],
      "metadata": {
        "id": "dGSFB2Y24FPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open(GLOVE_EMB, 'r', encoding='utf-8') as f:\n",
        "    glove = f.readlines()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Wr5IT2mftFXa",
        "outputId": "3bbf90f6-1a97-4a95-ec01-a058c932e1fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine training and validation datasets for features (articles)\n",
        "X = np.concatenate((X_Train, X_Val))\n",
        "\n",
        "# Combine training and validation datasets for labels (highlights)\n",
        "Y = np.concatenate((Y_Train, Y_Val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "2o2K23K7tPCi",
        "outputId": "6f28cdc1-4302-4d2f-bb74-c6f0391cc32e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G24UxWDIVH2e"
      },
      "source": [
        "## Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "-FIsOKuqVHAF",
        "outputId": "196a8cf8-fa57-4132-cd0e-0afaf233c330"
      },
      "source": [
        "def get_features(processed_data, prefix):\n",
        "    # note that the prefix variable is only used for naming dictionaries that are\n",
        "    # saved to the disk\n",
        "    print(f'\\nTotal # of stories: {len(processed_data)}')\n",
        "    word_vector, word_idx_dict, idx_word_dict = create_word_vec(processed_data, prefix=prefix)\n",
        "    print(f'Word Vector Shape: {word_vector.shape}')\n",
        "    assert word_vector.shape == (len(idx_word_dict.keys()), EMB_SIZE)\n",
        "\n",
        "    return word_vector, word_idx_dict, idx_word_dict\n",
        "\n",
        "# USING THE CELL BELOW TO SPEED THINGS UP\n",
        "# # Full\n",
        "full_processed_data = pre_process(X, Y)\n",
        "full_word_vector, full_word_idx_dict, full_idx_word_dict = get_features(full_processed_data, prefix='train')\n",
        "\n",
        "# Training\n",
        "train_processed_data = pre_process(X_Train, Y_Train)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100000/100000 [01:24<00:00, 1180.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total # of stories: 56304\n",
            "Embedding dictionary exists, loading from file...\n",
            "Word Vector Shape: (194444, 300)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 70000/70000 [00:59<00:00, 1180.60it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debugging"
      ],
      "metadata": {
        "id": "O7TADrLp4QDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tmp = []\n",
        "# for sentence in train_processed_data:\n",
        "#   tmp.append(sentence_to_idx(sentence, full_word_idx_dict))\n",
        "tmp = [sentence_to_idx(sentence, full_word_idx_dict) for sentence in train_processed_data]\n",
        "# tmp_np = np.array(tmp)\n",
        "type(tmp)\n",
        "\n",
        "print(type(tmp))\n",
        "print(type(tmp[0]))\n",
        "print(type(tmp[0][0]))\n",
        "print(type(tmp[0][0][0]))\n",
        "\n",
        "tmp_np = np.array(tmp, dtype=object)\n",
        "\n",
        "print(type(tmp_np))\n",
        "print(type(tmp_np[0]))\n",
        "print(type(tmp_np[0][0]))\n",
        "print(type(tmp_np[0][0][0]))"
      ],
      "metadata": {
        "id": "my86xqcmFb9i",
        "outputId": "eaa5b9c1-8de0-43a8-baee-8a4ccdb60126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "<class 'tuple'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.int64'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.ndarray'>\n",
            "<class 'numpy.int64'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert sentences into indicies corresponding to the word vector indices\n",
        "train_data_indices = np.array([sentence_to_idx(sentence, full_word_idx_dict) for sentence in train_processed_data], dtype=object)\n",
        "\n",
        "# Validation\n",
        "val_processed_data = pre_process(X_Val, Y_Val)\n",
        "val_data_indices = np.array([sentence_to_idx(sentence, full_word_idx_dict) for sentence in val_processed_data], dtype=object)"
      ],
      "metadata": {
        "id": "FFXgK2aZAYGK",
        "outputId": "a0431c22-808b-42d6-8a91-a07c38199c25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 30000/30000 [00:25<00:00, 1188.12it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_indices.shape"
      ],
      "metadata": {
        "id": "GW2BMXcHPy6X",
        "outputId": "799ac1ed-403c-42ee-8fe5-ea5477cb0f92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(39436, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_indices[:,1]"
      ],
      "metadata": {
        "id": "jNPboTcFNSG0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Padding and processing of data\n",
        "\n",
        "- **`pad_sequences(sequences, max_len, pad_value=0)`**:\n",
        "  - Pads each sequence in a list of sequences to a specified `max_len` using a `pad_value`, ensuring all sequences have the same length.\n",
        "  - Truncates sequences that exceed a predefined maximum length (`MAX_ARTICLE_LEN`).\n",
        "\n",
        "- **`process_and_pad_data(data, word_idx_dict)`**:\n",
        "  - Converts a list of textual data into indices using the `sentence_to_idx` function which relies on a provided `word_idx_dict`.\n",
        "  - Determines the maximum length of sequences in the dataset to ensure consistent padding.\n",
        "  - Separately pads text sequences and their corresponding labels to their respective maximum lengths.\n",
        "  - Combines the padded text and label sequences into a single array for easier manipulation and returns it.\n",
        "\n",
        "- **Data Caching and Loading Blocks**:\n",
        "  - These blocks check if processed data files exist for the full dataset, training, and validation sets.\n",
        "  - If processed data exists, it loads this data from files to avoid redundant processing.\n",
        "  - If it does not exist, it processes the respective datasets (`X`, `Y`, `X_Train`, `Y_Train`, `X_Val`, `Y_Val`) using the `pre_process` function to standardize and trim the text, then processes and pads this data using `process_and_pad_data`.\n",
        "  - The processed data is then saved to files to expedite future access.\n"
      ],
      "metadata": {
        "id": "4BxOCg2PaSnU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sequences(sequences, max_len, pad_value=0):\n",
        "    \"\"\" Pad sequences to the same length with pad_value. \"\"\"\n",
        "    padded_sequences = []\n",
        "    for seq in sequences:\n",
        "        padded = list(seq) + [pad_value] * (max_len - len(seq))\n",
        "        if len(padded) > MAX_ARTICLE_LEN:\n",
        "            padded = padded[0:MAX_ARTICLE_LEN]\n",
        "        padded_sequences.append(np.array(padded, dtype=int))\n",
        "    return np.array(padded_sequences)\n",
        "\n",
        "def process_and_pad_data(data, word_idx_dict):\n",
        "    # Convert sentences to indices and find maximum sequence length\n",
        "    data_indices = [sentence_to_idx(sentence, word_idx_dict) for sentence in data]\n",
        "    # print(len(data_indices))\n",
        "    # print(len(data_indices[0]))\n",
        "    max_len = max(len(indices[0]) for indices in data_indices)  # Assuming [0] is the sequence of interest\n",
        "    # print(max_len)\n",
        "    max_len_2 = max(len(indices[1]) for indices in data_indices)\n",
        "    # print(max_len_2)\n",
        "\n",
        "    x_tmp = pad_sequences([indices[0] for indices in data_indices], max_len)\n",
        "    y_tmp = pad_sequences([indices[1] for indices in data_indices], max_len_2)\n",
        "    result = []\n",
        "    for i in range(len(x_tmp)):\n",
        "        result.append([x_tmp[i], y_tmp[i]])\n",
        "\n",
        "        # np.array(text_tokens).astype(int), np.array(label_tokens).astype(int)\n",
        "    result = np.array(result, dtype=object)\n",
        "    # print(f'x_tmp shape:{x_tmp.shape}')\n",
        "    # print(f'y_tmp shape:{y_tmp.shape}')\n",
        "    # print(f'result shape:{result.shape}')\n",
        "    return result\n",
        "    # for sentence in data_indices:\n",
        "\n",
        "\n",
        "        # result.append(np.array([pad_sequences([indices[0] for indices in data_indices], max_len), pad_sequences([indices[1] for indices in data_indices], max_len_2)], dtype=object))\n",
        "\n",
        "    # Pad sequences and return\n",
        "    # return np.array(result)\n",
        "    # return pad_sequences([indices[0] for indices in data_indices], max_len)\n",
        "\n",
        "# Check for existing data or process new data\n",
        "if os.path.exists(drive_prefix + 'full_processed_data'):\n",
        "    print('full_processed_data exists, loading from file...')\n",
        "    full_processed_data = pickle.load(open(drive_prefix + 'full_processed_data', 'rb'))\n",
        "else:\n",
        "    full_processed_data = pre_process(X, Y)\n",
        "    full_word_vector, full_word_idx_dict, full_idx_word_dict = get_features(full_processed_data, prefix='train')\n",
        "\n",
        "if os.path.exists(drive_prefix + 'train_processed_data'):\n",
        "    print('train_processed_data exists, loading from file...')\n",
        "    train_processed_data = pickle.load(open(drive_prefix + 'train_processed_data', 'rb'))\n",
        "else:\n",
        "    train_processed_data = pre_process(X_Train, Y_Train)  # Assuming X_Train and Y_Train are defined somewhere\n",
        "    # train_data_indices = np.array([sentence_to_idx(sentence, full_word_idx_dict) for sentence in train_processed_data], dtype=object)\n",
        "    train_data_indices = process_and_pad_data(train_processed_data, full_word_idx_dict)\n",
        "\n",
        "if os.path.exists(drive_prefix + 'val_processed_data'):\n",
        "    print('val_processed_data exists, loading from file...')\n",
        "    val_processed_data = pickle.load(open(drive_prefix + 'val_processed_data', 'rb'))\n",
        "else:\n",
        "    val_processed_data = pre_process(X_Val, Y_Val)  # Assuming X_Val and Y_Val are defined somewhere\n",
        "    # val_data_indices = np.array([sentence_to_idx(sentence, full_word_idx_dict) for sentence in val_processed_data], dtype=object)\n",
        "    val_data_indices = process_and_pad_data(val_processed_data, full_word_idx_dict)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "IGJzbc8KxCBR",
        "outputId": "408f59e4-5033-4e57-e7e5-204aed4bd7f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100000/100000 [01:24<00:00, 1177.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total # of stories: 56304\n",
            "Embedding dictionary exists, loading from file...\n",
            "Word Vector Shape: (194444, 300)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 70000/70000 [01:00<00:00, 1160.28it/s]\n",
            "100%|██████████| 30000/30000 [00:26<00:00, 1142.69it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data_indices.shape"
      ],
      "metadata": {
        "id": "RNxMzIdaP5eR",
        "outputId": "9a3a22cf-86b7-4981-8149-a05b92ee116f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(283, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data_indices[:,1][2]"
      ],
      "metadata": {
        "id": "Bh_iWioPX8-5",
        "outputId": "a216b5f2-ecc3-4db0-a32d-ab56bc441a97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([12335, 11746,  8714, 11817,  7964, 12248,  7836,  9979,  9792,\n",
              "       14516,   782, 10508, 13527,  8217,   617, 13620,  2845,  1492,\n",
              "       10335, 13620,  2240,  8786, 12248,  5104,  3987,  1943,  7766,\n",
              "        6397, 13307,  7964, 10614,  9979,   774,  7739, 10270,  5424,\n",
              "        7796, 11596,  3459,  3964,  5614, 14114,     0])"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save all the processed data in the google drive"
      ],
      "metadata": {
        "id": "7oHlqjbA4aKY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "wrwbrm_sAtye",
        "outputId": "ce114559-3b33-441a-9ddc-fa161fafd2db"
      },
      "source": [
        "## Using this to speed things up\n",
        "def dump(data, name):\n",
        "    pickle.dump(data, open(drive_prefix + name, 'wb'))\n",
        "\n",
        "dump(full_processed_data, 'full_processed_data')\n",
        "dump(train_processed_data, 'train_processed_data')\n",
        "dump(train_data_indices, 'train_data_indices')\n",
        "dump(val_processed_data, 'val_processed_data')\n",
        "dump(val_data_indices, 'val_data_indices')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LBGH5Cdbou90",
        "outputId": "5c104e5f-30fe-4ca6-a357-57e558a01fa4"
      },
      "source": [
        "#sanity check\n",
        "val_data_indices.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16868, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r78iOpGUWJNG"
      },
      "source": [
        "## Model\n",
        "- **Embedding Layers**:\n",
        "  - Uses a pre-trained word vector to create an embedding layer that maps each token in the input sequences to a high-dimensional space.\n",
        "\n",
        "- **Encoder and Decoder Architecture**:\n",
        "  - **Encoder**: Consists of multiple layers, each containing a multi-head self-attention mechanism that allows the encoder to attend to different parts of the input sequence simultaneously. Also, it includes a position-wise feed-forward network that applies a simple neural network to each position.\n",
        "  - **Decoder**: It is just like the encoder, but includes an extra cross-attention step where the decoder attends to the encoder's output.\n",
        "\n",
        "- **Attention Mechanisms**:\n",
        "  - Implements scaled dot-product attention, which calculates attention scores by considering how much focus to place on other parts of the input sequence for each token in the output sequence.\n",
        "  - Uses masking in the attention layers to ensure predictions for a particular position can only consider positions before it.\n",
        "\n",
        "\n",
        "- **Output Generation**:\n",
        "  - After processing through the decoder, the output is passed through a linear layer that projects the decoder's high-dimensional token representations back into the vocabulary space.\n",
        "  - This step converts the decoder output into logits representing the likelihood of each vocabulary token being the next token in the output sequence.\n",
        "\n",
        "- **Masking and Probabilities**:\n",
        "  - Applies padding masks to ignore unused portions of the input batch  and uses future sequence masking to prevent the decoder from \"seeing\" future tokens prematurely during training.\n",
        "  - Generates initial probability distributions for the start of decoding to ensure that the model starts generating output from a standard initial state.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "sSadtseMWLSZ",
        "outputId": "97c06a76-8ba7-4e57-de7a-cbc7b684ebb5"
      },
      "source": [
        "# Create the embedding layer weights based on the pre-trained word vector\n",
        "def create_pretrained_emb_layer(word_vector):\n",
        "    # vocab_size, embed_dim = word_vector.shape\n",
        "    embed_layer = nn.Embedding.from_pretrained(torch.tensor(word_vector).float(), freeze=False)\n",
        "    return embed_layer\n",
        "\n",
        "def get_pad_mask(seq, pad_idx):\n",
        "    return (seq != pad_idx).unsqueeze(-2)\n",
        "\n",
        "class PositionEmbedding(nn.Module):\n",
        "    def __init__(self, dropout=DROPOUT_RATE, max_len=500):\n",
        "        super(PositionEmbedding, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.pos_embedding = self.compute_pos_embedding(max_len)\n",
        "\n",
        "    def compute_pos_embedding(self, max_len):\n",
        "\n",
        "        pos = torch.arange(max_len).unsqueeze(1)\n",
        "        index = torch.arange(EMB_SIZE).unsqueeze(0)\n",
        "        angles = 1 / np.power(10000, (2 * (index // 2)) / np.float32(EMB_SIZE))\n",
        "\n",
        "        pos_to_angle = pos * angles\n",
        "\n",
        "        # apply sin to even indices in the array; 2i\n",
        "        pos_to_angle[:, 0::2] = np.sin(pos_to_angle[:, 0::2])\n",
        "\n",
        "        # apply cos to odd indices in the array; 2i+1\n",
        "        pos_to_angle[:, 1::2] = np.cos(pos_to_angle[:, 1::2])\n",
        "\n",
        "        # pos_embedding = pos_to_angle.unsqueeze(0).transpose(0, 1)\n",
        "        pos_embedding = pos_to_angle.unsqueeze(0)\n",
        "\n",
        "        return torch.FloatTensor(pos_embedding).to(device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        print(x.shape)\n",
        "        print(self.pos_embedding[:, :x.shape[1]].clone().detach().shape)\n",
        "        x += self.pos_embedding[:, :x.shape[1]].clone().detach() # adding another feature to the word embedding\n",
        "        return self.dropout(x)\n",
        "\n",
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_in, hidden_dim, dropout=DROPOUT_RATE):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.pos1 = nn.Linear(d_in, hidden_dim)\n",
        "        self.pos2 = nn.Linear(hidden_dim, d_in)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.normalization = nn.LayerNorm(d_in, eps=1e-6)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        residual = x\n",
        "        out = self.pos1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.pos2(out)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "\n",
        "        out = self.normalization(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, temperature, attn_dropout=DROPOUT_RATE):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.dropout = nn.Dropout(attn_dropout)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, q, k, v, mask):\n",
        "\n",
        "        attn = torch.matmul(q / self.temperature, k.transpose(2, 3))\n",
        "\n",
        "        attn = attn.masked_fill(mask == 0, -1e7)\n",
        "\n",
        "        attn = self.softmax(attn)\n",
        "\n",
        "        return torch.matmul(attn, v), self.dropout(attn)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_head, key_dim, value_dim, dropout=DROPOUT_RATE):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.key_dim = key_dim\n",
        "        self.value_dim = value_dim\n",
        "\n",
        "        self.query_layer = nn.Linear(EMB_SIZE, n_head * key_dim, bias=False)\n",
        "        self.key_layer = nn.Linear(EMB_SIZE, n_head * key_dim, bias=False)\n",
        "        self.value_layer = nn.Linear(EMB_SIZE, n_head * value_dim, bias=False)\n",
        "        self.fc = nn.Linear(n_head * value_dim, EMB_SIZE, bias=False)\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(temperature=key_dim ** 0.5).to(device)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layer_norm = nn.LayerNorm(EMB_SIZE, eps=1e-6)\n",
        "\n",
        "\n",
        "    def forward(self, q, k, v, attn_mask):\n",
        "\n",
        "        key_dim, value_dim, n_head = self.key_dim, self.value_dim, self.n_head\n",
        "        batch_size, q_len = q.size(0), q.size(1)\n",
        "        k_len = k.size(1)\n",
        "        v_len = v.size(1)\n",
        "\n",
        "        residual = q\n",
        "\n",
        "        q = self.query_layer(q).view(batch_size, q_len, n_head, key_dim)\n",
        "        k = self.key_layer(k).view(batch_size, k_len, n_head, key_dim)\n",
        "        v = self.value_layer(v).view(batch_size, v_len, n_head, value_dim)\n",
        "\n",
        "        q, k, v = q.transpose(1, 2), k.transpose(1, 2), v.transpose(1, 2)\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1)\n",
        "\n",
        "        q, attn = self.attention(q, k, v, mask=attn_mask)\n",
        "\n",
        "        q = q.transpose(1, 2).contiguous().view(batch_size, q_len, -1)\n",
        "        q = self.fc(q)\n",
        "        q = self.dropout(q)\n",
        "        q += residual\n",
        "\n",
        "        q = self.layer_norm(q)\n",
        "\n",
        "        return q, attn\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_inner, n_head, key_dim, value_dim, dropout=DROPOUT_RATE):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.slf_attn = MultiHeadAttention(n_head=n_head, key_dim=key_dim, value_dim=value_dim, dropout=dropout).to(device)\n",
        "        self.pos_ffn = PositionwiseFeedForward(EMB_SIZE, d_inner, dropout=dropout).to(device)\n",
        "\n",
        "    def forward(self, enc_input, mask=None):\n",
        "        enc_output, enc_slf_attn = self.slf_attn(enc_input, enc_input, enc_input, attn_mask=mask)\n",
        "        enc_output = self.pos_ffn(enc_output)\n",
        "        return enc_output, enc_slf_attn\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, d_inner, n_head, key_dim, value_dim, dropout=DROPOUT_RATE):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.slf_attn = MultiHeadAttention(n_head, key_dim, value_dim, dropout=dropout).to(device)\n",
        "        self.enc_attn = MultiHeadAttention(n_head, key_dim, value_dim, dropout=dropout).to(device)\n",
        "        self.pos_ffn = PositionwiseFeedForward(d_model, d_inner, dropout=dropout).to(device)\n",
        "\n",
        "    def forward(self, d_in, e_out,y_mask=None, x_mask=None):\n",
        "        d_out, d_slf_attn = self.slf_attn(d_in, d_in, d_in, attn_mask=y_mask)\n",
        "        d_out, d_e_attn = self.enc_attn(d_out, e_out, e_out, attn_mask=x_mask)\n",
        "        d_out = self.pos_ffn(d_out)\n",
        "        return d_out, d_slf_attn, d_e_attn\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, n_layers, n_head, key_dim, value_dim,d_model, d_inner, dropout=DROPOUT_RATE):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.layer_stack = nn.ModuleList([DecoderLayer(d_model, d_inner, n_head, key_dim, value_dim, dropout=dropout).to(device) for _ in range(n_layers)])\n",
        "        self.layer_norm = nn.LayerNorm(d_model, eps=1e-6)\n",
        "\n",
        "    def forward(self, y, y_mask, e_out, x_mask):\n",
        "        d_out = self.dropout(y)\n",
        "        d_out = self.layer_norm(d_out)\n",
        "        for dec_layer in self.layer_stack:\n",
        "            d_out, dec_slf_attn, dec_enc_attn = dec_layer(d_out, e_out, y_mask=y_mask, x_mask=x_mask)\n",
        "\n",
        "        return d_out,\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, n_layers, n_head, key_dim, value_dim, dropout=DROPOUT_RATE):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.layers = nn.ModuleList([ EncoderLayer(E_HIDDEN_DIM, n_head, key_dim, value_dim, dropout=dropout).to(device) for _ in range(n_layers)])\n",
        "        self.normalization = nn.LayerNorm(EMB_SIZE, eps=1e-6)\n",
        "\n",
        "    def forward(self, x, x_mask):\n",
        "        e_out = self.dropout(x)\n",
        "        e_out = self.normalization(e_out)\n",
        "\n",
        "        for enc_layer in self.layers:\n",
        "            e_out, enc_slf_attn = enc_layer(e_out, mask=x_mask)\n",
        "\n",
        "        return e_out,\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_heads, word_vector, word_idx_dict, idx_word_dict):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.word_idx_dict = word_idx_dict\n",
        "        self.pos_embedding = PositionEmbedding(max_len=MAX_ARTICLE_LEN, dropout=DROPOUT_RATE).to(device)\n",
        "        self.word_embedding = create_pretrained_emb_layer(word_vector).to(device)\n",
        "\n",
        "        self.encoder = Encoder(n_layers=3, n_head=num_heads, key_dim=KEY_DIM, value_dim=VALUE_DIM, dropout=DROPOUT_RATE).to(device)\n",
        "        self.decoder = Decoder(n_layers=3, n_head=num_heads, key_dim=KEY_DIM, value_dim=VALUE_DIM, d_model=EMB_SIZE, d_inner=D_HIDDEN_DIM, dropout=DROPOUT_RATE).to(device)\n",
        "\n",
        "        self.initial_probs = self.get_initial_probs(word_vector.shape[0], word_idx_dict[START_CHAR])\n",
        "\n",
        "        self.fc_to_logits = nn.Linear(EMB_SIZE, len(idx_word_dict), bias=False)\n",
        "\n",
        "\n",
        "    def mask_future_seq(self, seq):\n",
        "        # Our attention is focused only allowed on the past sequence, so we mask\n",
        "        sz_b, len_s = seq.size()\n",
        "        subsequent_mask = (1 - torch.triu(torch.ones((1, len_s, len_s), device=seq.device), diagonal=1)).bool()\n",
        "        return subsequent_mask\n",
        "\n",
        "    def get_initial_probs(self, vocab_size, initial_token_idx):\n",
        "        probs = torch.zeros(1, vocab_size)\n",
        "        probs[0, initial_token_idx] = 1\n",
        "        return probs.float()\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x_mask = get_pad_mask(x, self.word_idx_dict[PAD_CHAR])\n",
        "        y_mask = get_pad_mask(y, self.word_idx_dict[PAD_CHAR]) & self.mask_future_seq(y)\n",
        "\n",
        "        x = self.word_embedding(x)\n",
        "        x = self.pos_embedding(x)\n",
        "\n",
        "        y = self.word_embedding(y)\n",
        "        y = self.pos_embedding(y)\n",
        "\n",
        "\n",
        "        # x = (batchsize, seqlen, embed dim)\n",
        "        # y.shape = (batchsize, seqlen)\n",
        "\n",
        "        # x_mask = (batchsize, 1, sequence length)\n",
        "        # y_mask = (batchsize, sequence length, sequence length)\n",
        "        # e_output = (batchsize, sequence length, embedding dimension)\n",
        "        # d_output = (batchsize, sequence length, embedding dimension)\n",
        "\n",
        "        e_out, *_ = self.encoder(x, x_mask)\n",
        "        d_out, *_ = self.decoder(y, y_mask, e_out, x_mask)\n",
        "\n",
        "        # self.fc_to_logits(dec_output) = (batchsize, seqlen, vocab len)\n",
        "        logits = self.fc_to_logits(d_out) * 1 # no weight sharing\n",
        "\n",
        "        return logits.view(-1, logits.size(2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizer"
      ],
      "metadata": {
        "id": "l-iFDGw2NX8v"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "dVKUFPHn5p_O",
        "outputId": "3012d599-c20d-4fb5-9f7c-b76834f9dac1"
      },
      "source": [
        "# Reference: https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/fec78a687210851f055f792d45300d27cc60ae41/transformer/Optim.py#L4\n",
        "class ScheduledOptim():\n",
        "    '''A simple wrapper class for learning rate scheduling'''\n",
        "\n",
        "    def __init__(self, optimizer, init_lr, d_model, n_warmup_steps):\n",
        "        self._optimizer = optimizer\n",
        "        self.init_lr = init_lr\n",
        "        self.d_model = d_model\n",
        "        self.n_warmup_steps = n_warmup_steps\n",
        "        self.n_steps = 0\n",
        "\n",
        "\n",
        "    def step_and_update_lr(self):\n",
        "        \"Step with the inner optimizer\"\n",
        "        self._update_learning_rate()\n",
        "        self._optimizer.step()\n",
        "\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"Zero out the gradients with the inner optimizer\"\n",
        "        self._optimizer.zero_grad()\n",
        "\n",
        "\n",
        "    def _get_lr_scale(self):\n",
        "        d_model = self.d_model\n",
        "        n_steps, n_warmup_steps = self.n_steps, self.n_warmup_steps\n",
        "        return (d_model ** -0.5) * min(n_steps ** (-0.5), n_steps * n_warmup_steps ** (-1.5))\n",
        "\n",
        "\n",
        "    def _update_learning_rate(self):\n",
        "        ''' Learning rate scheduling per step '''\n",
        "\n",
        "        self.n_steps += 1\n",
        "        lr = self.init_lr * self._get_lr_scale()\n",
        "\n",
        "        for param_group in self._optimizer.param_groups:\n",
        "            param_group['lr'] = lr"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G5ViZ0Ooh7X"
      },
      "source": [
        "## Train Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "Rp0Sc004J8Fw",
        "outputId": "7e5c35f9-7d1f-4207-a434-f0df2d4fec77"
      },
      "source": [
        "def _construct_summary(tokens, mapping):\n",
        "    summary = []\n",
        "    for token in tokens:\n",
        "        token = token.item()\n",
        "        if token in mapping and mapping[token] != PAD_CHAR:\n",
        "            summary.append(mapping[token])\n",
        "    return summary\n",
        "\n",
        "def one_pass(x, y, gold, model, optim, criterion):\n",
        "    optim.zero_grad()\n",
        "\n",
        "    y_hat = model.forward(x, y) # (batchsize * seq len, vocab size)\n",
        "\n",
        "\n",
        "    alpha = 0.1\n",
        "    n_class = y_hat.size(1)\n",
        "\n",
        "    y_hat_hot = torch.zeros_like(y_hat).scatter(1, gold.view(-1, 1), 1)\n",
        "    y_hat_smooth = (1 - alpha) * y_hat_hot + (1 - y_hat_hot) * alpha / (n_class - 1) # y_ls = (1 - α) * y_hot + α / K\n",
        "    log_prb = F.log_softmax(y_hat, dim=1)\n",
        "\n",
        "    non_pad_mask = gold.ne(full_word_idx_dict[PAD_CHAR])\n",
        "    loss = -(y_hat_smooth * log_prb).sum(dim=1)\n",
        "    loss = loss.masked_select(non_pad_mask).sum()\n",
        "\n",
        "    pred = y_hat.max(1)[1]\n",
        "\n",
        "    pred_summary = ' '.join(_construct_summary(pred, full_idx_word_dict)[1:-1])\n",
        "    true_summary = ' '.join(_construct_summary(y.contiguous().view(-1), full_idx_word_dict)[1:-1])\n",
        "\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "\n",
        "    # optim.step() #using lr-scheduler outside in train()\n",
        "    optim.step_and_update_lr() # custom lr scheduler\n",
        "\n",
        "\n",
        "    return loss, true_summary, pred_summary\n",
        "\n",
        "## Evaluation One Pass\n",
        "def one_pass_eval(model, criterion):\n",
        "    model.eval()\n",
        "    np.random.shuffle(val_data_indices)\n",
        "    val_loader = get_batch(val_data_indices)\n",
        "    eval_loss = 0\n",
        "    eval_pass_pred = []\n",
        "    eval_pass_true = []\n",
        "    for _ in range(NUM_EVAL_BATCHES):\n",
        "        data = val_loader.__next__()\n",
        "        x, y = np.array(data[:, 0]), np.array(data[:, 1])\n",
        "        # print(data)\n",
        "        # print(x)\n",
        "        # print(y)\n",
        "        x = torch.tensor(np.array([x[i] for i in range(len(x))])).long()\n",
        "        y = torch.tensor(np.array([y[i] for i in range(len(y))])).long()\n",
        "        # print(data)\n",
        "        # print(x)\n",
        "        # print(y)\n",
        "\n",
        "        x = x.to(device)\n",
        "        y, gold = map(lambda x: x.to(device), patch_trg(y))\n",
        "        y = y.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            y_hat = model.forward(x, y) # (batchsize * seq len, vocab size)\n",
        "\n",
        "            # loss = criterion(y_hat, gold)\n",
        "            alpha = 0.1\n",
        "            n_class = y_hat.size(1)\n",
        "\n",
        "            y_hat_hot = torch.zeros_like(y_hat).scatter(1, gold.view(-1, 1), 1)\n",
        "            y_hat_smooth = (1 - alpha) * y_hat_hot + (1 - y_hat_hot) * alpha / (n_class - 1) # y_ls = (1 - α) * y_hot + α / K\n",
        "            log_prb = F.log_softmax(y_hat, dim=1)\n",
        "\n",
        "            non_pad_mask = gold.ne(full_word_idx_dict[PAD_CHAR])\n",
        "            loss = -(y_hat_smooth * log_prb).sum(dim=1)\n",
        "            loss = loss.masked_select(non_pad_mask).sum()\n",
        "            pred = y_hat.max(1)[1]\n",
        "            temp_loss = loss.item()/BATCH_SIZE\n",
        "            eval_loss += temp_loss\n",
        "\n",
        "    model.train()\n",
        "    eval_loss /=  NUM_EVAL_BATCHES\n",
        "    return eval_loss\n",
        "\n",
        "def train(model: Transformer, optim, criterion, model_name):\n",
        "    print(\"Starting to train\")\n",
        "    model.train()\n",
        "    ep_loss = []\n",
        "    ep_eval_loss = []\n",
        "    temp_loss = []\n",
        "    eps = []\n",
        "    for ep in range(1, EP + 1):\n",
        "        np.random.shuffle(train_data_indices)\n",
        "        train_loader = get_batch(train_data_indices)\n",
        "\n",
        "        data = train_loader.__next__()\n",
        "        x, y = np.array(data[:, 0]), np.array(data[:, 1])\n",
        "        # print(data.shape)\n",
        "        # print(x.shape)\n",
        "        # print(y.shape)\n",
        "        x = torch.tensor(np.array([x[i] for i in range(len(x))])).long()\n",
        "        y = torch.tensor(np.array([y[i] for i in range(len(y))])).long()\n",
        "        # print(data)\n",
        "        # print(x.size())\n",
        "        # print(y.size())\n",
        "\n",
        "        x = x.to(device)\n",
        "        y, gold = map(lambda x: x.to(device), patch_trg(y))\n",
        "        y = y.to(device)\n",
        "\n",
        "        loss, true_summary, pred_summary = one_pass(x, y, gold, model, optim, criterion)\n",
        "\n",
        "        temp_loss.append(loss.item() / BATCH_SIZE)\n",
        "\n",
        "        if ep % PRINT_EVERY_EP == 0 and ep > 0:\n",
        "            eval_loss = one_pass_eval(model, criterion)\n",
        "            ep_loss.append(sum(temp_loss)/len(temp_loss))\n",
        "            ep_eval_loss.append(eval_loss)\n",
        "            eps.append(ep)\n",
        "            temp_loss = []\n",
        "\n",
        "\n",
        "        '''if ep % SAVE_MODEL_EVERY_EP == 0 and ep > 0:\n",
        "            torch.save({\n",
        "            'epoch': ep,\n",
        "            'model': model.state_dict(),\n",
        "            'loss': loss\n",
        "            }, f'{drive_prefix}models/{model_name}/checkpoint_{ep}')'''\n",
        "\n",
        "    return model, ep_loss, ep_eval_loss, eps\n",
        "\n",
        "def predict(model, X):\n",
        "    model.eval()\n",
        "\n",
        "    pred_summaries = []\n",
        "    true_summaries = []\n",
        "\n",
        "    for i in range(0, X.shape[0]):\n",
        "      x, y = np.array(X[i:i+1, 0]), np.array(X[i:i+1, 1])\n",
        "      x = torch.tensor(np.array([x[i] for i in range(len(x))])).long()\n",
        "      y = torch.tensor(np.array([y[i] for i in range(len(y))])).long()\n",
        "\n",
        "      x = x.to(device)\n",
        "      y, gold = map(lambda x: x.to(device), patch_trg(y))\n",
        "      y = y.to(device)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          y_hat = model.forward(x, y)\n",
        "\n",
        "          pred = y_hat.max(1)[1]\n",
        "          pred_summary = ' '.join(_construct_summary(pred, full_idx_word_dict)[1:-1])\n",
        "          true_summary = ' '.join(_construct_summary(y.contiguous().view(-1), full_idx_word_dict)[1:-1])\n",
        "          pred_summaries.append(pred_summary)\n",
        "          true_summaries.append(true_summary)\n",
        "\n",
        "    return pred_summaries, true_summaries"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkmtX6K2y6-e"
      },
      "source": [
        "# Evaluation Metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "H5rIr246y59g",
        "outputId": "52c41043-3aa6-4300-99d9-fb6e742a40b7"
      },
      "source": [
        "def compute_rouge(pred_summaries, true_summaries):\n",
        "  from rouge_score import rouge_scorer\n",
        "  scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
        "  rogue1_precision = []\n",
        "  rogue1_recall = []\n",
        "  rogue1_fmeasure = []\n",
        "  rogueL_precision = []\n",
        "  rogueL_recall = []\n",
        "  rogueL_fmeasure = []\n",
        "  for i in range(len(pred_summaries)):\n",
        "    score = scorer.score(pred_summaries[i], true_summaries[i])\n",
        "    rogue1_precision.append(score['rouge1'].precision)\n",
        "    rogue1_recall.append(score['rouge1'].recall)\n",
        "    rogue1_fmeasure.append(score['rouge1'].fmeasure)\n",
        "    rogueL_precision.append(score['rougeL'].precision)\n",
        "    rogueL_recall.append(score['rougeL'].recall)\n",
        "    rogueL_fmeasure.append(score['rougeL'].fmeasure)\n",
        "\n",
        "  scores = {'rogue1_precision':rogue1_precision, 'rogue1_recall':rogue1_recall, 'rogue1_fmeasure': rogue1_fmeasure, 'rogueL_precision':rogueL_precision, 'rogueL_recall':rogueL_recall, 'rogueL_fmeasure': rogueL_fmeasure }\n",
        "  return scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "DP-nVaVRzBvP",
        "outputId": "0f4cbbe2-c3a3-48d3-9a52-d5ed254e97df"
      },
      "source": [
        "def plot_rogue(scores, label_text):\n",
        "  from matplotlib import pyplot as plt\n",
        "  import pandas as pd\n",
        "  import seaborn as sns\n",
        "  for measure in scores.keys():\n",
        "    df = pd.DataFrame({\"score\":scores['rogue1_precision'] , \"dummy\": range(len(scores['rogue1_precision']))})\n",
        "    sns.set_style(\"whitegrid\")\n",
        "    f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw= {\"height_ratios\": (0.2, 1)})\n",
        "    f.set_size_inches(11.7, 8.27)\n",
        "    mean=df['score'].mean()\n",
        "    median=df['score'].median()\n",
        "    mode=df['score'].mode()[0]\n",
        "\n",
        "    sns.boxplot(df[\"score\"], ax=ax_box)\n",
        "    ax_box.axvline(mean, color='r', linestyle='--')\n",
        "    ax_box.axvline(median, color='g', linestyle='-')\n",
        "    ax_box.axvline(mode, color='b', linestyle='-')\n",
        "\n",
        "    sns.distplot(df[\"score\"], ax=ax_hist)\n",
        "    ax_hist.axvline(mean, color='r', linestyle='--')\n",
        "    ax_hist.axvline(median, color='g', linestyle='-')\n",
        "    ax_hist.axvline(mode, color='b', linestyle='-')\n",
        "\n",
        "    plt.legend({'Mean':mean,'Median':median,'Mode':mode})\n",
        "    plt.title(measure + f'{measure} {label_text}')\n",
        "    ax_box.set(xlabel='')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "E9dWeDps0J0M",
        "outputId": "404a1ae3-2d4b-40a1-c13b-0daf690a1294"
      },
      "source": [
        "def compute_rouge_summary_stats(scores):\n",
        "  def mean_confidence_interval(data, confidence=0.95):\n",
        "    import scipy.stats\n",
        "    a = 1.0 * np.array(data)\n",
        "    n = len(a)\n",
        "    m, se = np.mean(a), scipy.stats.sem(a)\n",
        "    h = se * scipy.stats.t.ppf((1 + confidence) / 2., n-1)\n",
        "    return m, m-h, m+h\n",
        "  stats = {}\n",
        "  for key in scores.keys():\n",
        "    stats[key] = mean_confidence_interval(scores[key])\n",
        "  return stats"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "gSL3lk3s0abM",
        "outputId": "c9fd82cd-b9e8-4973-ab27-25388ee30f16"
      },
      "source": [
        "def plot_losses(ep_loss, ep_eval_loss, eps):\n",
        "  import matplotlib.pyplot as plt\n",
        "  import matplotlib.ticker as ticker\n",
        "  plt.plot(eps, ep_loss, label = 'train')\n",
        "  plt.plot(eps, ep_eval_loss, label = 'val')\n",
        "  plt.legend()\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Train Losses')\n",
        "  plt.title('Training Loss')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def patch_trg(trg):\n",
        "\n",
        "    # print(trg)\n",
        "    # print(trg[:, :-1], trg[:, 1:].contiguous().view(-1))\n",
        "    trg, gold = trg[:, :-1], trg[:, 1:].contiguous().view(-1)\n",
        "    return trg, gold\n"
      ],
      "metadata": {
        "id": "Bk_mqJAqJ7u_",
        "outputId": "17bb7b00-569c-49fd-bd0f-e26904d758f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data_indices[0][0])"
      ],
      "metadata": {
        "id": "RbMhzoxSOLZx",
        "outputId": "d4c5bd87-ac85-4077-ea69-8103c2730c72",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wI1lhtuKqH7X"
      },
      "source": [
        "## Run\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'customtransformers'\n",
        "transformer = Transformer(num_heads=NUM_HEADS,\n",
        "                          word_vector=full_word_vector,\n",
        "                          word_idx_dict=full_word_idx_dict,\n",
        "                          idx_word_dict=full_idx_word_dict).to(device)\n",
        "\n",
        "# model_optim = optim.Adam(transformer.parameters(), lr=LR)\n",
        "\n",
        "model_optim = ScheduledOptim(\n",
        "        optim.Adam(transformer.parameters(), betas=(0.9, 0.98), eps=1e-09),\n",
        "        2.0, EMB_SIZE, 3000)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=full_word_idx_dict[PAD_CHAR], reduction='sum')\n",
        "\n",
        "model, ep_loss, ep_eval_loss, eps = train(transformer, model_optim, criterion, model_name)"
      ],
      "metadata": {
        "id": "qHDQg1vD7Jkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyperparameters"
      ],
      "metadata": {
        "id": "TkpD_BJIVAn2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "print(f'EMB_SIZE : {EMB_SIZE}')\n",
        "print(f'BATCH_SIZE : {BATCH_SIZE}')\n",
        "print(f'NUM_EVAL_BATCHES : {NUM_EVAL_BATCHES}')\n",
        "print(f'NUM_HEADS : {NUM_HEADS}')\n",
        "print(f'DROPOUT_RATE : {DROPOUT_RATE}')\n",
        "print(f'MAX_ARTICLE_LEN : {MAX_ARTICLE_LEN}')\n",
        "print(f'MAX_LABEL_LEN : {MAX_LABEL_LEN}')\n",
        "print(f'E_HIDDEN_DIM : {E_HIDDEN_DIM}')\n",
        "print(f'D_HIDDEN_DIM : {D_HIDDEN_DIM}')\n",
        "print(f'KEY_DIM : {KEY_DIM}')\n",
        "print(f'VALUE_DIM : {VALUE_DIM}')\n",
        "print(f'FORCE_CREATE_DICT : {FORCE_CREATE_DICT}')"
      ],
      "metadata": {
        "id": "1Bv7zZoJVDtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oGFp1vsH0rHs"
      },
      "source": [
        "## Plot training vs validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uNhoCLW0pct",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "c9b86fe3-9ab5-4cb8-c263-df8605a8c59c"
      },
      "source": [
        "plot_losses(ep_loss, ep_eval_loss, eps)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjsAAAHHCAYAAABZbpmkAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAB7uklEQVR4nO3dd3hUVeLG8e/MpFdKQiehhISSkIAgRZqIIiIqsIIu2FBRUVFZEXB/q4CiYlssKOhioYmKigIqgohKE0QCofcaIIWS3ib398clg5GWiclMCO/neebJzG1z5iQkL+eeYjEMw0BERESkkrK6uwAiIiIi5UlhR0RERCo1hR0RERGp1BR2REREpFJT2BEREZFKTWFHREREKjWFHREREanUFHZERESkUlPYERERkUpNYUdEXG706NF07969VOe+9dZbREVFlXGJRKQy83B3AUSk4ihpiJg+fTrt2rUr59JUPKNHj2bRokWsX7/e3UURESdYtDaWiBT5+uuvz3q9YsUKXn755WLbr7rqKkJCQkr9Pvn5+RiGgZeXl9PnFhQUYLfb8fb2LvX7l5bCjsilSS07IuJw8803F3u9YcMGVqxYcdb2v8rOzsbX17fE7+Pp6Vmq8gF4eHjg4aFfXSJScuqzIyJOueOOO7jxxhvZtGkTgwYNIjY2ltdffx2AJUuWMHToUDp16kR0dDQ9evRg8uTJ2O32Ytf4a5+dQ4cOERUVxbRp0/j000/p0aMH0dHR9O/fn40bNxY791x9dqKiohg/fjxLlizhxhtvJDo6mt69e/PLL7+cVf7ffvuNfv36ERMTQ48ePZgzZ06Z9wP67rvv6NevHy1btqRdu3Y8+eSTHDt2rNgxycnJjBkzhi5duhAdHU2nTp146KGHOHTokOOYhIQE7r33Xtq1a0fLli3p3r07Y8aMKbNyilwu9N8jEXHayZMnuf/+++nduzc33XQT1atXB+Crr77Cz8+Pe+65Bz8/P1avXs2bb75JRkYGo0aNuuh1FyxYQGZmJgMHDsRisfC///2PRx99lCVLlly0NWjdunX88MMP/POf/8Tf358ZM2YwfPhwfvrpJ6pWrQrAli1buO+++wgNDeXRRx+lsLCQyZMnU61atb9fKad9+eWXjBkzhpiYGEaMGEFqairTp0/njz/+YN68eQQFBQHw6KOPsmvXLgYPHkzdunU5fvw4K1as4MiRI9SrV4/U1FTuvfdeqlatytChQwkKCuLQoUMsXry4zMoqcrlQ2BERpyUnJzNu3Dhuu+22Yttfe+01fHx8HK9vv/12nnnmGT755BOeeOKJi/bRSUxM5IcffiA4OBiAhg0bMmzYMJYvX87VV199wXN3797Nt99+S1hYGADt2rXj5ptvZuHChQwePBiAN998E5vNxieffELNmjUB6NWrFzfccINzFXAe+fn5vPrqq0RGRjJr1ixHv6IrrriCBx54gI8++ojhw4eTlpbG+vXreeqpp7j33nsd5z/wwAOO5+vXr+fUqVNMmzaNmJgYx/YnnniiTMoqcjnRbSwRcZqXlxf9+vU7a/ufg05GRgbHjx+nTZs2ZGdns2fPnote94YbbnAEHYA2bdoAcPDgwYue27FjR0fQAWjatCkBAQGOc+12O6tWreKaa65xBB2A8PBwOnfufNHrl8SmTZtITU3l9ttvL9aBulu3bjRq1Ihly5YBZj15enqyZs0aTp06dc5rBQYGArBs2TLy8/PLpHwilyu17IiI02rWrHnOVpqdO3cyadIkVq9eTUZGRrF96enpF71u7dq1i70uCj5paWlOn1t0ftG5qamp5OTkEB4eftZx59pWGomJiYDZIvVXjRo1Yt26dYAZFp988kkmTpzIVVddRWxsLN26deOWW24hNDQUgCuvvJKePXvy9ttv89FHH3HllVfSo0cP+vTpU6pRbCKXM7XsiIjT/tyCUyQtLY3Bgwezbds2hg8fzpQpU/jwww958sknASgsLLzodW022zm3l2SGjL9zrjvcfffdLFq0iBEjRuDt7c0bb7zBDTfcwJYtWwCwWCy8+eabfPrppwwePJhjx47x9NNP069fPzIzM91cepFLi8KOiJSJNWvWcPLkSV566SXuuusurr76ajp27FjstpQ7Va9eHW9vb/bv33/WvnNtK406deoAsHfv3rP27d2717G/SFhYGEOGDOGDDz5gwYIF5Ofn88EHHxQ7Ji4ujieeeIIvv/ySV199lZ07d/Ltt9+WSXlFLhcKOyJSJqxW89fJn1tS8vLymD17truKVIzNZqNjx478+OOPxYaB79+/n19//bVM3iM6Oprq1aszZ84c8vLyHNt//vlndu/eTbdu3QBzXqLc3Nxi54aFheHv7+8479SpU2e1SjVr1gyg2LVF5OLUZ0dEykSrVq0IDg5m9OjR3HHHHVgsFr7++usKdRvpkUceYfny5dx+++3cfvvtFBYWMnPmTJo0acLWrVtLdI38/Hzeeeeds7YHBwczaNAgnnzyScaMGcPgwYPp3bu3Y+h53bp1ufvuuwHYt28fd999N9dffz0RERHYbDaWLFlCSkoKvXv3Bsxh/J988gk9evQgLCyMzMxMPvvsMwICAujSpUuZ1YnI5UBhR0TKRNWqVZkyZQoTJ05k0qRJBAUFcdNNN9GhQ4diw6vdKTo6mvfff5+XX36ZN954g9q1azN8+HD27NlTotFiYIadN95446ztYWFhDBo0iH79+uHj48P777/Pq6++ip+fHz169GDkyJGOOXZq1apF7969WbVqFd988w02m41GjRoxadIkevbsCZgdlBMSEvj2229JSUkhMDCQli1b8uqrr1K/fv2yqxSRy4DWxhKRy96wYcPYtWsXP/zwg7uLIiLlQH12ROSykpOTU+z1vn37+OWXX7jyyivdVCIRKW+6jSUil5UePXrQt29f6tevz+HDh5kzZw6enp7cd9997i6aiJQThR0Ruax07tyZhQsXkpycjJeXF3FxcYwYMYIGDRq4u2giUk7UZ0dEREQqNfXZERERkUpNYUdEREQqNfXZwVyzp6CgAKvVisVicXdxREREpAQMw6CwsBAPDw/HLO7norADFBQUkJCQ4O5iiIiISCnExMTg5eV13v0KO5xZ0ycmJua8Kyf/ld1uJyEhwalzpPRU366l+nYt1bdrqb5dqzzru+jaF2rVAYUdAMetK5vN5vQ3ojTnSOmpvl1L9e1aqm/XUn27VnnW98W6oKiDsoiIiFRqCjsiIiJSqSnsiIiISKWmPjsiIiLlyG63k5+f7+5iuI3dbgfMRXid7bPj6elZJv18FHZERETKgWEYHD16lJMnT7q7KG5lGAYeHh7s37+/VHPZValShVq1av2tefAUdkRERMpBUdCpUaMGfn5+l+2ktYZhkJ2dja+vr1N1YBgGWVlZJCUlAVC7du1Sl0FhR0REpIzZ7XZH0Klevbq7i+NWRbMc+/j4OB34fH19AUhKSqJGjRqlvqWlDsoiIiJlrKiPjp+fn5tLcukrqsO/0+9JYUdERKScXK63rspSWdRhhQk77733HlFRUUyYMMGx7Y477iAqKqrY45lnnil2XmJiIkOHDiU2NpYOHTowceJECgoKXF18ERERqaAqRJ+djRs3MmfOHKKios7aN2DAAIYPH+54XXT/Dsx7og888AAhISHMmTOHpKQkRo0ahaenJyNGjHBJ2UVEROTcunfvzp133smtt97q1nK4vWUnMzOTkSNH8vzzzxMcHHzWfh8fH0JDQx2PgIAAx77ly5eza9cuXnnlFZo1a0bXrl157LHHmDVrFnl5ea78GCIiIpXCHXfcUewuy98xd+5cBgwYUCbX+jvcHnbGjx9P165d6dix4zn3z58/n3bt2nHjjTfy2muvkZ2d7dgXHx9PZGQkISEhjm2dOnUiIyODXbt2lXvZL6bAXkheQaG7iyEiIlJmDMMocXeRatWqFbsj4y5uvY21cOFCtmzZwty5c8+5/8Ybb6ROnTrUqFGD7du38+qrr7J3717efvttAFJSUooFHcDxOjk52enyFM3y6MyxFzrn9vd/49CJbH58ojPenlpZ9+8oSX1L2VF9u5bq27VcUd92ux3DMByPS8Xo0aNZs2YNa9asYfr06QC88MILPP3007z33ntMmjSJnTt38r///Y/atWvz0ksvsWHDBrKzs2nUqBEjRowo1nhRdBtrwIABGIZB06ZNee655/j5559Zvnw5NWvWZNSoUXTv3v28ZSqqQ7vdftb3rKTfQ7eFnSNHjjBhwgQ++OADvL29z3nMwIEDHc+joqIIDQ3l7rvv5sCBA4SFhZV5mRISEsr0nE2HTpJdYPDj6vXUCawQ3aMueaX5Hknpqb5dS/XtWuVd3x4eHmRnZ1NYeKaF3zAMsvNd1+Lv62l1ajTTE088wZ49e2jcuDEPPfQQALt37wbglVde4YknnqBu3boEBQVx7Ngx2rdvz4MPPoiXlxcLFizgoYce4ssvv3RMAGgYhmPIeNGdmbfffpvHHnuMRx55hE8//ZQnn3yShQsXnrMrC0Bubi75+fls27at1PXgtr/AmzdvJjU1lX79+jm22e121q5dy6xZs0hISDhr8qDY2FgA9u/fT1hYGCEhIWzcuLHYMSkpKQCEhoY6XaaYmJgST1hkt9tJSEi44DnBi34iOy2X+o2aEFP33N9EKZmS1LeUHdW3a6m+XcsV9Z2Tk8P+/fvx9fXFx8cHMP/w3zp1Nev2nyiX9zyXNuFV+eyB9iUOPH5+fnh7exMYGOhoVDhy5AgAjz/+eLEWmNq1axMXF+d43bRpU37++WdWrVrF4MGDAXPYuKenJ3BmgFG/fv0cf/tHjhzJJ598wq5du+jcufM5y2S1WvH09CQiIsJRl0WKvpcX47aw0759e+bPn19s25gxY2jUqBH333//OX8At27dCpwJMnFxcUyZMoXU1FTHDJUrV64kICCAiIgIp8tks9mc/sG/0DkBPp6Qlkt2vqFfYGWkNN8jKT3Vt2upvl2rPOvbZrNhsVgcjyLumHXnr2UoyfHn+hoTE1PsOpmZmbz99tssW7aM5ORk7HY7OTk5HDlypNi5f71OVFSU47m/vz8BAQEcP378vGUsusbf+X65LewEBAQQGRlZbJufnx9VqlQhMjKSAwcOMH/+fLp27UqVKlXYvn07L774Im3btqVp06aA2Rk5IiKCp556ipEjR5KcnMykSZMYNGgQXl5e7vhYxfh7m9Wbkat5f0RELncWi4XPH+xAdr7r+mb5etrKbGLDv3Y0njhxIitXrmTUqFGEhYXh4+PD8OHDLzrTcVFLTxGLxVLsVl95qLAdSTw9PVm1ahXTp08nKyuL2rVrc9111zFs2DDHMTabjSlTpjB27FgGDhyIr68vffv2LTYvjzsFOsJO6ae4FhGRysNiseDnVWH/9ALm39+ShI/169fTt29frr32WsBs6Tl8+HB5F69UKlSNz5gxw/G8du3azJw586Ln1K1bl/fff788i1Vq/t5mc1tGrkZYiIjIpaFu3bps2LCBQ4cO4efnd97gEx4ezuLFi+nevTsWi4VJkyaVewtNabl9np3KLMDbbKrLyNFtLBERuTQMGTIEm81G79696dChg6OD8l+NHj2aoKAgbrvtNh588EE6d+5MixYtXFzakqlQLTuVTcDplp1M9dkREZFLRMOGDfn000+LbfvzyOki9erVc8zFU2TQoEHFXi9duhTDMMjKygJg+/btZ13n999//7tFvii17JSjAB91UBYREXE3hZ1ypNFYIiIi7qewU44co7HUZ0dERMRtFHbKUdFtrMw8hR0RERF3UdgpR/6n51JIV8uOiIiI2yjslCN1UBYREXE/hZ1yFHC6z46GnouIiLiPwk45ClAHZREREbdT2ClHjrCTV4BhGG4ujYiIyOVJYaccFfXZMQzIytP6WCIiUvl1796djz76yN3FKEZhpxz5etqwWszn6rcjIiLiHgo75chisThmUU5X2BEREXELhZ1yplmURUTkUvHpp5/SqVMnCgsLi21/6KGHGDNmDAcOHOChhx6iY8eOtGrViv79+7Ny5Uo3lbbkFHbKmb+Gn4uISBHDgLxM1z2cHBxz/fXXc/LkSX777TfHtpMnT/Lrr79y0003kZWVRdeuXfnoo4/46quv6Ny5Mw8++CCJiYllXVNlysPdBajsijop6zaWiMhlzjDgg55w8LeLH1tW6reHId+DxVKiw4ODg+nSpQvz58+nQ4cOACxatIiqVavSrl07rFYrTZs2dRz/+OOPs2TJEpYuXcrgwYPL5SOUBbXslDNNLCgiImeULHS4U58+ffjhhx/Iy8sDYP78+fTu3Rur1UpmZiYTJ06kV69etGnThlatWrF792617FzuHHPtKOyIiFzeLBazlSU/y3Xv6elX4ladIt27d+f//u//WLZsGTExMfz++++MGTMGgIkTJ7Jy5UpGjRpFWFgYPj4+DB8+nPz8/PIofZlR2Cln/go7IiJSxGIBL393l+KCvL29ue6665g/fz779++nYcOGtGjRAoD169fTt29frr32WgAyMzM5fPiwO4tbIgo75UxLRoiIyKWmT58+PPDAA+zcuZObbrrJsT08PJzFixfTvXt3LBYLkyZNOmvkVkWkPjvlLNBHfXZEROTS0r59e4KDg9m7dy99+vRxbB89ejRBQUHcdtttPPjgg3Tu3NnR6lORqWWnnGlSQRERudRYrVaWL19+1vZ69eoxffr0YtsGDRpU7PXSpUvLtWyloZadcqbbWCIiIu6lsFPOHEPP8xR2RERE3EFhp5ypZUdERMS9FHbKmYaei4iIuJfCTjkrGo2lsCMicvkxnFybSs5WFnWosFPOziwXYXdzSURExFU8PT0ByMpy4WzJlVRRHRbVaWlo6Hk5+/NtrMJCA6u14q+LIiIif4/NZqNKlSokJSUB4Ofnh8XJZRsqC8MwyM3NxWq1OlUHhmGQlZVFUlISVapUwWazlboMCjvlrOg2FpgjsgJ9Sp9MRUTk0lGrVi0AR+C5XBmGQX5+Pp6enqUKfFWqVHHUZWkp7JQzbw8rNqsFe6FBZq5dYUdE5DJhsVioXbs2NWrUqPALZZYnu93Otm3biIiIcLp1xtPT82+16BRR2ClnFouFAG8PTmXnk5GbD/i4u0giIuJCNputTP5gX6rsdrPPqo+Pj9vqocJ0UH7vvfeIiopiwoQJAJw8eZLnnnuOnj170rJlS7p168bzzz9Penp6sfOioqLOeixcuNAdH+G8HHPtqJOyiIiIy1WIlp2NGzcyZ84coqKiHNuSkpJISkpi1KhRREREcPjwYcaOHUtSUhJvvvlmsfNffPFFOnfu7HgdFBTksrKXhCYWFBERcR+3h53MzExGjhzJ888/z7vvvuvYHhkZyVtvveV4HRYWxuOPP87IkSMpKCjAw+NM0YOCgggNDXVpuZ3h720222muHREREddze9gZP348Xbt2pWPHjsXCzrlkZGQQEBBQLOgAjBs3jn//+9/Ur1+f2267jf79+5eqx3fRfUVnji3JOUXDz9Oy85x6DznDmfqWv0/17Vqqb9dSfbtWedZ3Sa/p1rCzcOFCtmzZwty5cy967PHjx3nnnXcYOHBgse3Dhw+nffv2+Pr6snz5csaNG0dWVhZ33nmn0+VJSEgol3MKsjMA2L5nH/HWZKffQ84ozfdISk/17Vqqb9dSfbuWO+vbbWHnyJEjTJgwgQ8++ABvb+8LHpuRkcEDDzxA48aNeeSRR4rte/jhhx3PmzdvTnZ2NtOmTStV2ImJiSlxT3G73U5CQkKJzqm3JwEOHaZKSC3i4ho7XS5xrr7l71N9u5bq27VU365VnvVddO2LcVvY2bx5M6mpqfTr18+xzW63s3btWmbNmkVCQgI2m42MjAzuu+8+/P39mTx58kWni46NjeWdd94hLy8PLy8vp8pUmuGBJTkn0McsR2Zeof5h/U2X+xBOV1N9u5bq27VU367lzvp2W9hp37498+fPL7ZtzJgxNGrUiPvvv98RdO699168vLx49913L9oCBLB161aCg4OdDjrlKeB0B+VMdVAWERFxObeFnYCAACIjI4tt8/Pzo0qVKkRGRpKRkcGQIUPIzs7mlVdeISMjg4wMs+9LtWrVsNlsLF26lNTUVGJjY/H29mbFihVMnTqVIUOGuOMjnVeAVj4XERFxG7ePxjqfzZs3s2HDBgCuvfbaYvt+/PFH6tWrh4eHB7NmzeKFF14AzOHpo0ePZsCAAS4v74X8eTFQERERca0KFXZmzJjheN6uXTu2b99+weO7dOlCly5dyrtYf5smFRQREXGfCrNcRGVWtPJ5Zp7CjoiIiKsp7LiAv5dadkRERNxFYccFijoop6vPjoiIiMsp7LhAUZ8dDT0XERFxPYUdFygKO1l5duyFhptLIyIicnlR2HGBoqHnoE7KIiIirqaw4wLeHlY8beYq7OqkLCIi4loKOy5gsVgcrTvqtyMiIuJaCjsuUtRvRyOyREREXEthx0U0IktERMQ9FHZcREtGiIiIuIfCjotoYkERERH3UNhxEXVQFhERcQ+FHRcJ1G0sERERt1DYcZGilp0MTSooIiLiUgo7LqIOyiIiIu6hsOMigT7qsyMiIuIOCjsu4riNpbAjIiLiUgo7LuKYQVm3sURERFxKYcdFHDMoq4OyiIiISynsuEjRpILqoCwiIuJaCjsu4u9V1GfH7uaSiIiIXF4UdlykaDRWRm6+m0siIiJyeVHYcZGi0Vg5+YUU2AvdXBoREZHLh8KOi/h72xzPM3UrS0RExGUUdlzE28OGl4dZ3VoyQkRExHUUdlxIS0aIiIi4nsKOCznCjjopi4iIuIzCjgudWTJCfXZERERcRWHHhQJ1G0tERMTlFHZcqGhEllY+FxERcR2FHRcK8PEEIF1hR0RExGUqTNh57733iIqKYsKECY5tubm5jBs3jnbt2tGqVSseffRRUlJSip2XmJjI0KFDiY2NpUOHDkycOJGCgooZJhyLgSrsiIiIuEyFCDsbN25kzpw5REVFFdv+wgsv8NNPPzFp0iRmzJhBUlISjzzyiGO/3W7ngQceID8/nzlz5vDSSy/x1Vdf8eabb7r6I5RIwOnbWBkKOyIiIi7j9rCTmZnJyJEjef755wkODnZsT09P54svvmD06NF06NCB6OhoXnjhBdavX098fDwAy5cvZ9euXbzyyis0a9aMrl278thjjzFr1izy8vLc9InOL8D79G0sdVAWERFxGbeHnfHjx9O1a1c6duxYbPumTZvIz88vtr1x48bUqVPHEXbi4+OJjIwkJCTEcUynTp3IyMhg165dLim/M9RBWURExPU83PnmCxcuZMuWLcydO/esfSkpKXh6ehIUFFRse/Xq1UlOTnYc8+egAzheFx3jDLu95PPfFB3rzDn+XmbYSc/Jd+o8KV19S+mpvl1L9e1aqm/XKs/6Luk13RZ2jhw5woQJE/jggw/w9vZ2VzGKSUhIKNdzko9kA3As9aSjdUqcU5rvkZSe6tu1VN+upfp2LXfWt9vCzubNm0lNTaVfv36ObXa7nbVr1zJr1iymTZtGfn4+aWlpxVp3UlNTCQ0NBcxWnI0bNxa7btForaJjnBETE4PNZrv4gafLmpCQ4NQ5p/ySYfU68PQhLi7O6fJdzkpT31J6qm/XUn27lurbtcqzvouufTFuCzvt27dn/vz5xbaNGTOGRo0acf/991O7dm08PT1ZtWoVPXv2BGDPnj0kJiY6gkJcXBxTpkwhNTWV6tWrA7By5UoCAgKIiIhwukw2m83pb4Qz5wT5egGQmWfXP7BSKs33SEpP9e1aqm/XUn27ljvr221hJyAggMjIyGLb/Pz8qFKlimN7//79eemllwgODiYgIIDnn3+eVq1aOcJOp06diIiI4KmnnmLkyJEkJyczadIkBg0ahJeXl6s/0kUF+Gi5CBEREVdzawfli3n66aexWq0MHz6cvLw8OnXqxLPPPuvYb7PZmDJlCmPHjmXgwIH4+vrSt29fhg8f7sZSn9+ZVc8VdkRERFylQoWdGTNmFHvt7e3Ns88+Wyzg/FXdunV5//33y7toZaIo7OQWFJJXUIiXh9tH/ouIiFR6+mvrQv7eZ7Kl5toRERFxDYUdF/K0WfE+3ZqjW1kiIiKuobDjYoE+6rcjIiLiSgo7Luavlc9FRERcSmHHxYo6Kacr7IiIiLiEwo6LBahlR0RExKUUdlzMMdeOJhYUERFxCYUdFwtQB2URERGXUthxMX/NoiwiIuJSCjsuFqjbWCIiIi6lsONijqHneQo7IiIirqCw42KOoedq2REREXEJhR0X09BzERER11LYcTGNxhIREXEthR0Xc8yzk2t3c0lEREQuDwo7LnZm6Hm+m0siIiJyeVDYcTHHqufqoCwiIuISCjsudmbVc93GEhERcQWFHRcr6rOTZy8kt0CBR0REpLwp7LiYv5fN8VytOyIiIuVPYcfFPGxWfD3NwKN+OyIiIuVPYccNNNeOiIiI6yjsuEGAVj4XERFxGYUdN9CSESIiIq6jsOMG/t5mn510hR0REZFyp7DjBgHenoA6KIuIiLiCwo4bBJxu2dFtLBERkfKnsOMGRaOxdBtLRESk/CnsuIG/OiiLiIi4jMKOGwR6azFQERERV1HYcQPHPDt5CjsiIiLlTWHHDfzVsiMiIuIyCjtuEKjlIkRERFzGw51vPnv2bD755BMOHz4MQJMmTRg2bBhdu3bl0KFDXHPNNec8b9KkSfTq1QuAqKios/a//vrr9O7du/wK/jepg7KIiIjrlEnYSUtLIygoyOnzatWqxZNPPkl4eDiGYTBv3jwefvhhvvrqKxo1asTy5cuLHf/pp58ybdo0unTpUmz7iy++SOfOnR2vS1MWVyrqs5Ou21giIiLlzumw895771GvXj1uuOEGAB577DF++OEHQkJCeP/992natGmJr9W9e/dir5944gk++eQT4uPjadKkCaGhocX2L1myhF69euHv719se1BQ0FnHVmSOtbHUQVlERKTcOR125syZw6uvvgrAihUrWLlyJe+//z7fffcdL7/8Mh988EGpCmK32/n+++/JysqiVatWZ+3ftGkTW7du5Zlnnjlr37hx4/j3v/9N/fr1ue222+jfvz8Wi6VUZXD2WGfOKeLraXaVysgpoKCgoFRlvdz8nfoW56m+XUv17Vqqb9cqz/ou6TWdDjspKSnUrl0bgJ9++olevXrRqVMn6taty4ABA5y9HNu3b+e2224jNzcXPz8/Jk+eTERExFnHzZ07l8aNG9O6deti24cPH0779u3x9fVl+fLljBs3jqysLO68806ny5KQkOCSc7LyCwEoKDRY+0c8XjaFnZIqTX1L6am+XUv17Vqqb9dyZ307HXaCgoI4cuQItWvX5tdff+Xxxx8HwDCMUqW2hg0bMm/ePNLT01m0aBGjRo1i5syZxQJPTk4OCxYsYNiwYWed//DDDzueN2/enOzsbKZNm1aqsBMTE4PNZivRsXa7nYSEBKfOKVJYaMC8RQA0impOSIC302W93Pyd+hbnqb5dS/XtWqpv1yrP+i669sU4HXauu+46R6fikydPOjoLb926lfDwcKcL6uXl5TgvOjqahIQEpk+fzvjx4x3HfP/99+Tk5HDLLbdc9HqxsbG888475OXl4eXl5VRZbDab09+I0p0D/l42MvPs5BQY+sfmhNLUt5Se6tu1VN+upfp2LXfWt9NhZ8yYMdStW5cjR44wcuRIR2fh5ORk/vnPf/7tAhUWFpKXl1ds2xdffEH37t2pVq3aRc/funUrwcHBTgcdV/P39iAzz64RWSIiIuXM6bDj6enJvffee9b2u+++2+k3f+211+jSpQu1a9cmMzOTBQsWsGbNGqZNm+Y4Zv/+/axdu5b33nvvrPOXLl1KamoqsbGxeHt7s2LFCqZOncqQIUOcLourBfh4kJSeq4kFRUREylmp5tmZN28en376KQcPHuTTTz+lbt26fPTRR9SrV48ePXqU+DqpqamMGjWKpKQkAgMDiYqKYtq0aVx11VWOY7744gtq1apFp06dzi68hwezZs3ihRdeACAsLIzRo0eXqqO0qwVoYkERERGXcDrszJ49mzfffJO77rqLKVOmUFhojiwKCgri448/dirsFIWUCxkxYgQjRow4574uXbqcNcHgpcKxGKjCjoiISLlyem2smTNn8vzzz/PQQw9htZ45PTo6mh07dpRp4Sozf4UdERERl3A67Bw6dIhmzZqdtd3Ly4vs7OwyKdTlIFArn4uIiLiE02GnXr16bN269aztv/76K40bNy6TQl0OAnzUZ0dERMQVnO6zc8899zB+/HjH8PCNGzeyYMEC3nvvPZ5//vkyL2BlVXQbK11hR0REpFw5HXZuvfVWvL29mTRpEtnZ2fzrX/+iRo0aPP300/Tu3bs8ylgpBeg2loiIiEuUauj5TTfdxE033UR2djZZWVlUr169rMtV6WnlcxEREddwus9OTk6OoyOyr68vOTk5fPTRRyxfvrzMC1eZFYUdzaAsIiJSvpwOO8OGDWPevHkApKWlceutt/Lhhx8ybNgwZs+eXdblq7T8NamgiIiISzgddjZv3kybNm0AWLRoESEhIfz0009MnDiRGTNmlHkBK6tAH82zIyIi4gqluo1VtPjn8uXLue6667BarcTFxZGYmFjmBaysziwXYXdzSURERCo3p8NOWFgYS5Ys4ciRIyxfvtyxjlVqaioBAQFlXsDKyjH0PCffzSURERGp3JwOOw8//DAvv/wy3bt3JzY2llatWgGwYsWKc86sLOdWdBsrM8+OYRhuLo2IiEjl5fTQ8+uvv54rrriC5ORkmjZt6tjeoUMHpxYBvdwVtezYCw1y8gvx9bK5uUQiIiKVU6nm2QkNDSU0NBSAjIwMVq9eTcOGDbVchBP8PG1YLGAYkJ6br7AjIiJSTpy+jfXYY48xc+ZMwOys3L9/fx5//HFuuukmFi1aVOYFrKysVgv+XuqkLCIiUt6cDju///67Y+j54sWLMQyDtWvX8u9//5t33323zAtYmWnJCBERkfLndNhJT08nODgYMFc6v+666/D19aVbt27s37+/zAtYmfl7m7euNNeOiIhI+XE67NSuXZv169eTlZXFr7/+6hh6npaWhpeXV5kXsDIL8PEEFHZERETKk9MdlO+8805GjhyJn58fderUoV27dgCsXbuWyMjIMi9gZRaoJSNERETKndNhZ9CgQbRs2ZKjR4/SsWNHrFazcah+/fo8/vjjZV2+Sq3oNla6wo6IiEi5KdXQ85iYGGJiYjAMA8MwsFgsdOvWrYyLVvkFeJ++jaUOyiIiIuXG6T47APPmzaNPnz60bNmSli1b0qdPH8dK6FJyAadbdnQbS0REpPw43bLz4Ycf8sYbbzBo0CDHbat169YxduxYTp48yd13313GRay8ArTyuYiISLlzOuzMmDGDsWPHcssttzi2XXPNNTRp0oS33npLYccJRUtGKOyIiIiUH6dvYyUnJzsW//yzVq1akZycXCaFulwEalJBERGRcud02AkPD+e77747a/u3335LgwYNyqJMl40Ax8rnCjsiIiLlxenbWI8++ihPPPEEa9eupXXr1gD88ccfrF69mkmTJpV1+Sq1orWx0tWyIyIiUm6cbtnp2bMnn332GVWrVuXHH3/kxx9/pGrVqnz++edce+215VHGS1dGEpw8cN7djpYd9dkREREpN6WaZyc6OppXX3212LbU1FSmTJnCgw8+WCYFqxSm3wwn9sFDK6Bao7N2B6iDsoiISLkr1Tw755KcnMwbb7xRVperHILrQ34W/PTCOXdr1XMREZHyV2ZhR86h+7/Nrwlz4WjCWbsdYSevAMMwXFkyERGRy4bCTnmqHQst+gEG/PjcWbuL+uwYBmTl2V1cOBERkcuDW8PO7Nmz6dOnD61bt6Z169YMHDiQn3/+2bH/jjvuICoqqtjjmWeeKXaNxMREhg4dSmxsLB06dGDixIkUFFSg20Ld/w8sNti5CPavKrbL19OG1WI+VydlERGR8lHiDsovvvjiBfcfP37c6TevVasWTz75JOHh4RiGwbx583j44Yf56quvaNKkCQADBgxg+PDhjnN8fX0dz+12Ow888AAhISHMmTOHpKQkRo0ahaenJyNGjHC6POWiemNofQes+wiWjIUh34PFTDgWiwV/bw/ScwpIzy2ghlsLKiIiUjmVOOxs2bLlose0adPGqTfv3r17sddPPPEEn3zyCfHx8Y6w4+PjQ2ho6DnPX758Obt27eLDDz8kJCSEZs2a8dhjj/Hqq6/yyCOP4OXl5VR5yk3XUbBhDhxcDTt/gMiejl2Bp8OOWnZERETKR4nDzowZM8qzHNjtdr7//nuysrKKLUcxf/58vvnmG0JDQ7n66qsZNmyYo3UnPj6eyMhIQkJCHMd36tSJsWPHsmvXLpo3b16uZS6xoDpw5VBY+Sb8OB4irgWreQfRXyOyREREylWp5tkpS9u3b+e2224jNzcXPz8/Jk+eTEREBAA33ngjderUoUaNGmzfvp1XX32VvXv38vbbbwOQkpJSLOgAjtelWafLbi95J+GiY0t8TsfHsK77CMuxTRRu/Bwj5h8ABJ7upLwnOYN2Das6V+DLiNP1LX+L6tu1VN+upfp2rfKs75Je0+1hp2HDhsybN4/09HQWLVrEqFGjmDlzJhEREQwcONBxXFRUFKGhodx9990cOHCAsLCwMi9LQsLZw8PL8pxaDW+l7rZp5P/wLJvzwzGsnjQPLuAP4N2l24jyTMF6uj+PnFtpvkdSeqpv11J9u5bq27XcWd9uDzteXl6Eh4cD5szMCQkJTJ8+nfHjx591bGxsLAD79+8nLCyMkJAQNm7cWOyYlJQUgPP287mQmJgYbDZbiY612+0kJCQ4dQ7Nm2AcnI935hHiChMwWg+hcdMCvtm5jMPpBRzzrE2v6FpOl/tyUKr6llJTfbuW6tu1VN+uVZ71XXTti3F72PmrwsJC8vLyzrlv69atwJkgExcXx5QpU0hNTaV69eoArFy5koCAAMetMGfYbDanvxFOneMbBF2fgm+fxPrrK9Dqn1Tx9+fujg14c+ku3v15D71b1sGi1p3zKs33SEpP9e1aqm/XUn27ljvr263z7Lz22musXbuWQ4cOsX37dl577TXWrFlDnz59OHDgAJMnT2bTpk0cOnSIH3/8kVGjRtG2bVuaNm0KmJ2RIyIieOqpp9i2bRu//vorkyZNYtCgQRVnJNZftb4LqoRDxjH4bSoA91zVED8vG5sT01i23fm+RiIiInJ+pWrZSUtLY+PGjaSmpp61zMEtt9xS4uukpqYyatQokpKSCAwMJCoqimnTpnHVVVdx5MgRVq1axfTp08nKyqJ27dpcd911DBs2zHG+zWZjypQpjB07loEDB+Lr60vfvn2LzctT4Xh4wdX/hq+GwopJ0OYeqvpXZVC7MN7/dS9v/7SLblGhat0REREpI06HnaVLl/Lkk0+SlZVFQEBAsT/KFovFqbDzwgvnXiAToHbt2sycOfOi16hbty7vv/9+id+zQoj5B6x4A5I2w/JJcO047u/ciI9X7Wfd/hP8tvc47RtVd3cpRUREKgWnb2NNnDiR/v37s379en7//XfWrl3reKxZs6Y8ylj5WG1wzX/M579NhbQj1AjyYUCbegBM/mmXGwsnIiJSuTgddo4dO8add95ZbNkGKYXI66F+OyjIhl9eBuCBLo2xWS38ujOFDQdPurd8IiIilYTTYadTp06am6AsWCzQY6z5fN3HcHwP9av5cUtcXQDeVuuOiIhImXC6z07Xrl155ZVX2L17N5GRkXh4FL/ENddcU2aFq/TCO0Lj7rB7KfwxHXqM5aFujfly/SEWbznGtqNpNK0V5O5SioiIXNKcDjv/+Y/Z12Ty5Mln7bNYLI65cKSErrjbDDsb5kD3/xBRI4Be0bX4NuEo7/y0mzdvb3XRS4iIiMj5OR12tm3bVh7luHxF9gLfapB+BHb/BE16MKxbBN8mHGXBxkRGXBtJgxB/d5dSRETkkuXWSQUFc96dmFvN5/HmUPvousFcHRVKoQFTft7txsKJiIhc+krUsjN9+nQGDhyIt7c306dPv+Cxd955Z5kU7LLSahCsmQrbFkL2CfCtyiPdI/hpezJf/HGI4dc0oU4VjX4TEREpjRKFnY8++og+ffrg7e3NRx99dN7jLBaLwk5p1GoJNaPh2CZImAtX3s8V4dVo36gaq/cc571f9jD2phbuLqWIiMglqURhZ+nSped8LmXEYoG4QbBoDMTPhivvB+CRq5uwes9vzFl7gEe6RxAS4O3mgoqIiFx61Genomg5AKwekPgHJJkj2q6KqE5s/Srk5BcybfleNxdQRETk0lSqhUCPHj3Kjz/+yJEjR8jPzy+2b8yYMWVSsMuOf4g5q/K2BbB+JvScgMVi4ZGrI7h/+u/MWLWfB7s0JtjP090lFRERuaQ4HXZWrVrFQw89RP369dmzZw9NmjTh8OHDGIZB8+bNy6OMl4+4QWbY2fiZObuyzZNrmtYgqmYg24+l8+HKvTzeI9LdpRQREbmkOH0b67XXXmPIkCHMnz8fLy8v3nrrLZYtW0bbtm25/vrry6OMl48m14J/KGQmwa4lAFitFh69JgKAab/u5WRWnjtLKCIicslxOuzs3r2bW265BQAPDw9ycnLw9/fnscce43//+19Zl+/yYvOElgPN5/GzHJtviK5N01qBpOcW8N4ve9xUOBERkUuT02HHz8/P0U8nNDSUAwcOOPadOHGi7Ep2uYr7p/l1+/eQmQqYrTv/ui4KgA9X7CMlI9ddpRMREbnkOB12YmNjWbduHWAuCjpx4kTeffddnn76aWJjY8u8gJedmi2gdhwU5kPC547NPZrVILZeMNn5dt5dplmVRURESsrpsDNmzBhatmwJwKOPPkr79u359ttvqVu3LhMmTCjzAl6WWg02v55ePgLMCRuLWndmrN7P0VM57iiZiIjIJcep0Vh2u52jR48SFWX+0fXz82P8+PHlUrDLWnR/WPQ0HE2AIxuhthkuOzcJ4coG1Viz7zhv/7ST52+JcXNBRUREKj6nWnZsNhtDhgzh1KlT5VUeAfCrBlE3mM/jZzs2m6075tDzT9ce5ODxLHeUTkRE5JLi9G2sJk2acOjQofIoi/xZ3CDza8JnUHBmuHm7RtXp3CSEfLvBmz/udFPhRERELh1Oh53HH3+ciRMn8tNPP5GUlERGRkaxh5SRxt0hoBZkpcLORcV2jbjWbN354o9D7ElWnYuIiFxIicPO22+/TVZWFkOHDmXbtm089NBDdO3albZt29K2bVvatGlD27Zty7OslxebB8TeZj5fP6vYrlZhVenRrAaFBkxaotYdERGRCylxB+XJkydz++23M3369PIsj/xZ3CBYMQl2/gAZSRBQw7HriWsjWbI1ifkbExl2dWOa1gpyXzlFREQqsBKHHcMwALjyyivLrTDyF6GRUK8tHFoLGz+Fjo86drWoE0zvmNosTDjCfxfvYOodbdxYUBERkYrLqT47FoulvMoh51M0o3L8bDgdOIs8cW0TrBZYtPkYCYc0Qk5ERORcnJpnp2fPnhcNPGvWrPlbBZK/aNEPvh8DSVsgcT3Ube3YFVEjkFvi6vLl+sO8vng7H96jVjcREZG/cirsPProowQGBpZXWeRcfKtAsz7m0hE//B/c+Y3Zefm0x3o04esNify0PZl1+49zRXg195VVRESkAnIq7PTu3Zvq1auXV1nkfLqNMRcG3b8CfpoAPZ517Aqv7s+ANvX4ZM1BXvthB7Pvb+/GgoqIiFQ8Je6zo/46blS9Mdz0pvl8+euw44diux/p3gQvm5WVu1NZuSvFDQUUERGpuEocdoy/dI4VF4vuB1cONZ9/NRROHnTsqlvFl3+2CwNgwrdbKbAXuqOEIiIiFVKJw862bdt0C8vdrnse6rSG7BPw+d3FlpF4pHsEQT4ebE5MY/qq/e4ro4iISAXj9HIR4kYe3nDrR+ATDId/hyVn+u6EBHgzqldTAF77YTtHT+W4qZAiIiIVi1vDzuzZs+nTpw+tW7emdevWDBw4kJ9//hmAkydP8txzz9GzZ09atmxJt27deP7550lPTy92jaioqLMeCxcudMfHcY2q4XDLFPP56ndgyzeOXbe3DaN1WBUy8+yMm7/ZTQUUERGpWJwajVXWatWqxZNPPkl4eDiGYTBv3jwefvhhvvrqKwzDICkpiVGjRhEREcHhw4cZO3YsSUlJvPnmm8Wu8+KLL9K5c2fH66CgSr50QtMboONwWPkmfP0w1GwB1RtjtVqY0DeGG99aznebjrJ02zG6N63p7tKKiIi4lVtbdrp3707Xrl1p0KABDRs25IknnsDPz4/4+HgiIyN566236N69O2FhYXTo0IHHH3+cpUuXUlBQUOw6QUFBhIaGOh7e3t5u+kQudM0zUL895KbB53dBvnnbqlntIO7r1BCA/8zbTFZewYWuIiIiUum5tWXnz+x2O99//z1ZWVm0atXqnMdkZGQQEBCAh0fxYo8bN45///vf1K9fn9tuu43+/fuXaqi83W53+lhnzilbVuj3Ptb3u2E5mkDhd6Mwer8OwCNXN2LBxkQOn8xm0uIdjLo+yk1lLDvur+/Li+rbtVTfrqX6dq3yrO+SXtNiuHlM+fbt27ntttvIzc3Fz8+P1157ja5du5513PHjx+nfvz833XQTTzzxhGP75MmTad++Pb6+vixfvpy33nqLkSNHcuedd5a4DHa7nfj4+LL4OC4XmLSWJr+NxoLB3lZPc7xeDwB+T8zhxRUnsVnglWurEx7s6eaSioiIlI+4uDhsNtt597s97OTl5XHkyBHS09NZtGgRn3/+OTNnziQiIsJxTEZGBvfccw/BwcG8++67eHqe/w/3G2+8wZdffuno6FwSRWEnJibmgpX113MSEhKcOqe8WJa9gPXXVzE8/Si8dwmEmqOyHpq1nh+2HKNVWBU+u78dVuulOzFkRarvy4Hq27VU366l+nat8qzvomtfLOy4/TaWl5cX4eHhAERHR5OQkMD06dMZP348YAad++67D39/fyZPnnzBoAMQGxvLO++8Q15eHl5eXk6VxWazOf2NKM05Ze7qp+HQGix7f8H2zSNw349gtTLu5has2JXC+gMn+fyPRMfEg5eyClHflxHVt2upvl1L9e1a7qzvCjfPTmFhIXl55mR5GRkZ3HvvvXh6evLuu++WqOPx1q1bCQ4OdjroXNKsNuj3PngFQuIfsHEOALWDfRlxndlf56XvtpKcnuvOUoqIiLiFW8POa6+9xtq1azl06BDbt2/ntddeY82aNfTp04eMjAyGDBlCVlYWEyZMICMjg+TkZJKTkx0dkpYuXcrnn3/Ojh072L9/P7Nnz2bq1KkMHjzYnR/LPQJrQZcnzedLxkGuOR/RXR3CaVEniLScAiYs3OLGAoqIiLiHW29jpaamMmrUKJKSkggMDCQqKopp06Zx1VVX8dtvv7FhwwYArr322mLn/fjjj9SrVw8PDw9mzZrFCy+8AEBYWBijR49mwIABLv8sFUL7h2DdR3BiL/z6OvR4Fg+blRf6xnDLOyuYF5/IP66oT6cmIe4uqYiIiMu4NewUhZRzadeuHdu3b7/g+V26dKFLly5lXaxLl4c39JwAc/4JqyZD6zuhWkNi61fhzvbhfLxqP//5ehPfPdYZH0/dpxYRkctDheuzI39T1A3QqBvYc2Hxfxyb/9UzihqB3uxNyeSdZbvdVz4REREXU9ipbCwW6PkiWKywdT7s/QWAIB9PnunTHIC3lu5kwcZEd5ZSRETEZRR2KqOazaHNvebz78eA3VwyondMbf7ZLgzDgCc+jeeXHcluLKSIiIhrKOxUVlc/DT5V4Ngm+ONjACwWC8/dHE3vlrXJtxs8MGMdfxw44d5yioiIlDOFncrKr5oZeACWPg/ZZqixWS38d0AcnZuEkJ1v554P17L9aLobCyoiIlK+FHYqszZDzKUjso/Dzy87Nnt5WJl6xxW0CqvCqex87pj2GwePZ7mxoCIiIuVHYacys3nC9S+az9e8B8k7HLv8vDz48O62RNUMJCk9l8HTfiMpPcdNBRURESk/CjuVXePuENkLCgtg0dPFdlXx82L6vVdSv5ov+1OzuOuDtZzKyoWDayEx3j3lFRERKWMKO5eDnhPA6gm7FsOOH4rtqhnkw8y7W9PbfxuDkv9L4atRMK0HvNcNdixyT3lFRETKkMLO5aB6Y3MpCYBFY6AgD/KyzHl4vnyA8A9aMtk+nsEeP1K18AQFeAAGfHEfJF94FmsREZGKzq3LRYgLdRkJGz6B1F3wwXWQtA0Kss/s9w8luW4PxmwNZ2V+JN9UnURE9kb45Da4fyn4VnVf2UVERP4GtexcLnyC4JpnzOeJ682gUyUMOjwC93wP/9pO6D+nMHjwveRZfRl44iFSbDXg+B6YO8QxMaGIiMilRi07l5O4QZBxDOz50PRGqBVjLi/xJ92iajBl8BU88skf3Jn1OF96j8Nn91JY8qzZ90dEROQSo5ady4nVZt7OuvppqN3yrKBTpEfzmswZ2oFjfpE8nne6r8+qtyF+tgsLKyIiUjYUduSc4upX4cthHdle7WreKOgHQOE3j5nD0kVERC4hCjtyXuHV/fnioY78UnsI39vbYi3MI2fmbZCmFdNFROTSobAjF1TN34tZQzvybcSzbC2sj09uCsnv98fI0/ISIiJyaVDYkYvy8bTx3zs6sSjmvxw3AghN30LCu3dhtxe6u2giIiIXpbAjJWKzWnjsHz1YdcV/yTdstDzxA/MmP0V6Tr67iyYiInJBCjtSYhaLhd43DWBbq38D0Df1f3z86r9YuTPZzSUTERE5P4UdcVrMLf/iWLO7sVoMHin4mLTpt/PiV7+RlaeJB0VEpOJR2JFSqTlgErk9X6XA4sn1trXctv4OHvnvDNbtP+7uoomIiBSjsCOlY7Hg3eF+PO5bRI5fHRpajzE56yk+eW8iL363lZx8u7tLKCIiAijsyN9V9wp8HllBfsNr8LXk8arnFMJXPE3/t5aScOiUu0snIiKisCNlwK8annfMhav/jYGFf3os5aWTT/LIO1/x38U7yNcQdRERcSOFHSkbVit0fQrL4C8o9K1GjHUfX3s+zcafPuOWySvYfjTd3SUUEZHLlFY9l7IVcQ3WB36Bz++myuHf+dDrFfanfMyOd8JJbxhHq7adsNVqAdUamQuTioiIlDOFHSl7VerDPd/BD/+HseY9wq1JhJME+9fC/vfNYzx8ILQp1GxhPmIGQECoe8stIiKVksKOlA8PL7jhZSzdRmMcTSD+9xXs2byWxsY+Ii2H8SvIgSPx5gNgzXsw5AcIrOnGQouISGWksCPly68alkZdadWoK6Ensnhq7kZW704mzHKMm+uc5N6ILIK2fw4n9sHM/nDPQvAJdnepRUSkElEHZXGZelX9mHlvO569KYajHnV543AzrlrdjoWtpmD414BjCTBnEOTnuLuoIiJSiSjsiEtZrRbu6tiA7x7rQuuwKqTnFvDwdycZ5fsMBZ4BsO9X+PJ+KNSkhCIiUjbcGnZmz55Nnz59aN26Na1bt2bgwIH8/PPPjv25ubmMGzeOdu3a0apVKx599FFSUlKKXSMxMZGhQ4cSGxtLhw4dmDhxIgUFWqOpomsY4s/nD3ZkdK+meNmsfHaoGndkPkY+HrD1G4yFT4JhuLuYIiJSCbg17NSqVYsnn3ySL7/8ki+++IL27dvz8MMPs3PnTgBeeOEFfvrpJyZNmsSMGTNISkrikUcecZxvt9t54IEHyM/PZ86cObz00kt89dVXvPnmm+76SOIEm9XCg10bs2xkN+7qEM46awzD8x6m0LBgWfcBe794BkOBR0RE/ia3hp3u3bvTtWtXGjRoQMOGDXniiSfw8/MjPj6e9PR0vvjiC0aPHk2HDh2Ijo7mhRdeYP369cTHxwOwfPlydu3axSuvvEKzZs3o2rUrjz32GLNmzSIvL8+dH02cUKeKL+NujubXp66mdofbGF94DwANN73Ju6/+myVbjin0iIhIqVWYPjt2u52FCxeSlZVFq1at2LRpE/n5+XTs2NFxTOPGjalTp44j7MTHxxMZGUlISIjjmE6dOpGRkcGuXbtc/RHkb6oZ5MMzfZoz7KkX+bXOEAAezHiHL2ZO5ubJq1hxMPvCS0/Y8yvGra/cDNi3HFa8AXPvhTXvu7tEIiKXNbcPPd++fTu33XYbubm5+Pn5MXnyZCIiIti6dSuenp4EBQUVO7569eokJycDkJKSUizoAI7XRcc4w24veafYomOdOUdKprqfJx2HvELW13n4JczkDc/J3HUsgNePtGDGpmUMaFOf21rXoE7ePixH4uHIBixHNkDSZvAPobDf/6B+e9cU1p4HSVuwJK6Hw+uwJP4BKTuwGGdCmbHpCwobXQNVw11TpjKgn2/XUn27lurbtcqzvkt6TbeHnYYNGzJv3jzS09NZtGgRo0aNYubMmW4pS0JCgkvOkRIKv4tGyfupevRXPvR+nbcL/0GdnMNEr9xLjVUHsVnO0RE9LRHL9FvY2/ppTtbuUm5F8848TNjG1wk4vglrYf5Z+3N9a5BVpSneGYfwS99D8ncvcrj5g+VWnvKin2/XUn27lurbtdxZ324PO15eXoSHm//jjY6OJiEhgenTp9OrVy/y8/NJS0sr1rqTmppKaKi5rEBISAgbN24sdr2i0VpFxzgjJiYGm61k6zXZ7XYSEhKcOkdKIeZTjFn/wOfASp60zCj2E3vK8COhsCH7vSMJaXIlba68imorX8C683sa/T4Oo+eLGFcOLfsypezEOmMkloyjABg+VaBOK4w6rTHqXgF1WuERUJMggB2L4NPbqXl4EaH/eA28/Mu+POVAP9+upfp2LdW3a5VnfRdd+2LcHnb+qrCwkLy8PKKjo/H09GTVqlX07NkTgD179pCYmEhcXBwAcXFxTJkyhdTUVKpXrw7AypUrCQgIICIiwun3ttlsTn8jSnOOOMHmD7d/gjHvYdJTjxAQ2Qlr3Vbs84pk+laY+8ch0jIKYD14bUykV4snGd2kGrV3zsayaDSkJ0KPceaq7GUhaRtM7wOZSVCjOdz6EZaQSLBYsJzr+KieULUBlhP7sG3+AtrcUzblcBH9fLuW6tu1VN+u5c76dmvYee211+jSpQu1a9cmMzOTBQsWsGbNGqZNm0ZgYCD9+/fnpZdeIjg4mICAAJ5//nlatWrlCDudOnUiIiKCp556ipEjR5KcnMykSZMYNGgQXl5e7vxoUpZ8q1A4YDo74+PN773NRgPgmSYw8vqmzN+YyKzfDrDh4Em+3pjE1/RmdICVBwtmwso3Ie0w3PIueHj/vXIc2wIf94GsFKgZDXd+Df4hFz7HaoMrh8Kip831v664GyznjEUiIlJO3Bp2UlNTGTVqFElJSQQGBhIVFcW0adO46qqrAHj66aexWq0MHz6cvLw8OnXqxLPPPus432azMWXKFMaOHcvAgQPx9fWlb9++DB8+3F0fSVzM18vGgDb1GdCmPpsOn+LTtQeZF3+YlzJuYLs1kJc938Nz0xckHz1I4F2f4hNYrXRvdDQBPr4Jso9DrZZm0PEr4bXiBsHS5yFpizlDdMPy60skIiJnc2vYeeGFFy6439vbm2effbZYwPmrunXr8v77GtorEF03mOi6wfy7dzMWbT7K57+HcPeeKkzxnERoyhp2vtaFec0ncW2HNsTWC8ZS0haWxHiYcQtkn4A6reCOr8C3askL5lsFYm+H36fBb1MVdkREXKzC9dkR+bt8PG3cHFeXm+PqcvB4DPN+ieb6+EdpwkEGb76fe9Y/RWpAE7o0CaVrVCidI0Ko6n+e256H18GMvpBzCuq2gcFfmOHFWVcONcPO9m/h5AGoEva3PqOIiJRchZlUUKQ81K/mxx233Ej1x34mM7gJtS3H+dLrWV7ImUD1De/y4ZzPaP/8d/R9ZwWTluxg/YET2AtPT0x4cC1Mv8UMOvXbnW7RqVK6gtRoCo26gVEIa//n/Pkr34J3OsK2b0v3/iIilzG17MhlwVo1DP8HF8Ond+C371eutf3BtbY/AMg2vFh/JIK1iU15dWkUe7ybcWu9kzySOAYveyY5ddrhcfvnePgEXeRdLuLKB2DPMlj3MXQdDV5+JTtv91L44f/M53Nuh/YPQ4+x4KFO+CIiJaGwI5cP36pw5zeQ+AfsXwkHVsGBVfhmn6CjbQsd2QJAQaEV+wErXpYCVtqbc++eB7BPWEnDEH8a1/AnIjSAxjUCiKgRQFTNQDxsJWwgjewJVcLh5H5I+ByuuOvi52SmwFenJyOs0cKcJXr1ZDi4Gv7x4SU1K7OIiLso7MjlxWqFem3Mx1XDobAQUnbAgZWwfxXGgZV4nDqEB4Vs87uCV7zGYKQWkJdfyPZj6Ww/ll7sctX9vegVU4s+LevQtkE1rNYLdHq22uDK+81WmjXvQes7LzwM3TBg3jDIOAahzeD+H81WnnkPmX2JpnaGm9+BZjeWUeWIiFROCjtyebNazf40NZpCmyHmxIAnD0LqTpqGd+IrDy8KCw0On8xmV3IGu5My2HX6sf1YOqmZecxcfYCZqw9QM8ib3jF1uDG2Nq3qVzn3aK9Wg+GnF+DYJti/Ahp0On/ZfpsKOxeBzRv+MQ08faFpb3hwOXx+Dxz+HT4dBO0egmvH67aWiMh5KOyI/FWV+ubjNKvVQv1qftSv5sfVUTUc2/PthazcncqCDYl8v/kox9Jy+WDFXj5YsZd6VX3p3bI2fVrWoUWdoDPBx7cqtBwI6z40w8z5ws7RBFj8H/N5zwlQs8WfyhcG93wHP46DVW/Db++eua1VrWFZ14aIyCVPYUeklDxtVrpGhtI1MpTn+0bz644U5m9MZPGWYxw6kc3Un/cw9ec9hAZ606JOENF1gmlRJ4i4qDupve5D2LbAbEX6U7ACIC8T5g4xV1SPugHa3nf2m3t4mSGoQWeY9yAkroepXeDmt6H5za6pABGRS4TCjkgZ8Paw0aN5TXo0r0l2np2fticxf0MiS7clkZyey7LtySzbnuw4fo5PNO2NTSyfM5GkK0fTtkE16lc7PTrr+zFmP6LA2nDT2xfu1xN1PTzwK3xxLxz8DT670+wLdN0E+Lujx0REKgmFHZEy5utl44aY2twQU5vsPDtbjqSxJfEUmxPT2JR4ih1HM/gg71rae22i+ZGvuPeza8izeNGnZR2ebrCdWn98DFig71Twr37xN6xSH+5eCEufgxVvwB/TYddSuPktaNy93D+viEhFp7AjUo58vWxcEV6VK8LPLC+RV1DIrqPtyZz1KdWyExleI55Xkq7k9w0b8d02GixwsvXDVGnUteRvZPM0Oyk3uQ6+fhhO7DNnfr7ibrjuefAOLPPPJiJyqdAMyiIu5uVhpXm9qvh3MufPedjvRxY83J6Pgt8n2JJFfGFj2q1ux1NzN3DweJZzF2/QCR5aaS5PAbDuI3Pm5T3LyvQziIhcShR2RNyl1R3g4QtHE4he/iiRuQnYPQOYXe8ZcgttfPb7Ia5+dRljvkzg8Mnskl/Xyx9ueAXumm+O3Dp1AKbfDAtGQG5G+X0eEZEKSmFHxF38qkHLAebz7QsBsN34Oi8PvYUvHupI5yYhFBQafLLmAN1e+Yn/m5fAriQnwkrDLvDQqjOjuX6fBu92gL2/lPEHOY9Th+DnV8w5gVa/C8k7zIkSRURcTH12RNyp3QPwx8fm85YDIXYgAFeEV2XGve1Ys/c4/128g1V7Uh2TF7ZvVI1B7cLp2aIWXh4X+f+KdwD0fg2a3QRfP2KuuP5xH3PpicBaZx4BtYq/9g0p3ecpyIVtC2H9THO2Z06Hm81fml+D60PENdD4GmjUFXyCS/c+IiJOUNgRcaeaLaDDI5CyE2549azdVzasxidD27NydwofLN/L0m1JrN5znNV7jhMS4MWtbepze9swwqpfZFHRRl1h2Er44T/mhIZJm83HediAGJ9QrNvbQO3Y04+WEFT33EPhj2w0A07CZ5B94sz2Bp3NfkQHVpvrkZ06aPYjWvcRWGxQ/0oz+DS+Gqo1MiddvNBQexGRUlDYEXG3nhMuekjHxiF0bBxC4sls5qw9yJw1B0hKz+XdZbuZ8vNuujQJZVC7MLo3rXH+hUm9A6HPJOj0OKTuhvSjkHHU/Fr0KHptz8MrJxl2fGc+ivhVh1otz4SfrOPmUPejG88cE1QX4v5pPqo1OrM9L8tcImPXEtj1I6TudCzGyk/Pm8dYPcE/FAJq/OlRE/xPPw+/CgJrOl3FInJ5U9gRuYTUqeLLiGsjebR7BD9uTWLWb/v5dWcKP+9I5ucdydQK8uG2K+tz+5Vh1AzyOfdFqjYwH+djGNgzUti1agFNArKwHtsERzZA8jbISoU9P5mPP7N5met2tRoMja42Fz39Ky8/aHKt+QA4sR92/2gGnwOrzGsX5kN6ovk4F6unOUP0lUPNViG1AolICSjsiFyCPG1Wro+uxfXRtdifmsnsNQf4/PdDHE3LYdKSnby1dBfXNa/JHe3D6dC4+rkXJT0fiwX8qpFRvSVGXBzYTgeX/Bzz1teRjWZLzpENgAVibjU7WvtVc+5DVA2HNkPMB5j9fTKTISPp9OMYZP7p+fE95pphm+aaj9qxcOUDEN0fPM8T7EREUNgRueSFV/dnTK9mjLg2ku83HWXm6v2s3XeC7zYd5btNR2kU6s/gduH0v6Iewb6epX8jTx+oe4X5KA8e3hBcz3ycT2I8rHkPEuaaYevrYfDD/8EVd0Gbe89eZ6y08nPMJTtqxaj1SKQS0NBzkUrC28PGzXF1+fzBjnz/eGcGtw/D38vGnuRMxi/YQrsXljBq7kY2HT7l7qKWXp04uOUdGLEVrnkWgupB9nFY/l94oyXMGQSHfv9773HqMLx/NUztbI5cS95eJkUXEfdR2BGphJrWCuL5W2L47d89eO6WaJrWCiQnv5BPfz/IjW8t54Y3fmXqz7s5csqJyQorEv/q0HkEPLYBBs405xQyCs2V5KddB6veKd2cPknbzPOTtpiv9/0K714Fi581V6MXkUuSwo5IJRbg7cEd7cP57rHOfP5gB26Oq4OnzcKWI2m8+N02Or60lNveW8Unaw5wKivf3cV1ns0DmvUxZ4sethpa9AXDDovGwFcPQL4TYW7/KvigJ6QdgupNzMVVI3uZnaZXTIK3r4Qt35Q8RGUkw9pp5qSKq94Be0GpPuIl7egm+PYp+PxuWD/LHL0n4gbqsyNyGbBYLLRtUI22Daoxtk8Lvt10hK/XJ7Jm33HHvD3Pfr2ZblGh3BxXl26RJVhtvaKp0Qz+8SHUbweL/g0bPzVHkA2cdfG+PFvnwxf3QUEO1GsL//zM7HDdoBNs+xa+G2Uuu/HZHRBxLfSaCNUbn32djCTY+g1snmcOszcKze2bv4SNc+DmyWY/IHc5dcj8PId/h/bDzNuCZS0vEzZ/Bb9/aL5Pkc1fgdUDGnY1R9Q1vdFsoZNLkz0fdv8EO3+A6hFmvzlPX3eX6rwUdkQuM1X9vRjULpxB7cI5fDKbb+IT+Tr+MNuOpvPDlmP8sOUYAd42rqztxYNVT9C2oZOjudzJYoH2D5mTNX5+t9mJ+b1uMOBjM7icy9r/wbcjzWAS2Qv+8YE5TL5I0xugUTf49TVY+SbsWgzv/GLOV9TpCchJMwPOlq+LBxyAOq3M9/1jxpmyXPU4dBnpmhFkhgHHNsP2b81bfEc2nNm3eync96M5Kq4sHE0wJ4vc+BnkppnbrB5mqAmJNGfWTtpsTjew+0dY8IRZN81vNlvnAmqY59jzIf2IGcxOHTInoix6npturikX9091HHcHw4DDf5j/kdj0BWSlnNm3/HXz38MVd1fI0GMxDC1WY7fbiY+PJy4uDpvtHPODlNE5Unqq7/K3/Wg68+IP8018YrGFRxuH+jOwbX36ta5HSIC3G0vopJMHzA7LRzeaszVf/6I5P0/RH0nDgJ8mwC+vmK9b3wW9XzdvjZ1Pyi749skz8wz5VTdnjC4WcFpDi1vMP+JF8xmlH4PvRpqBCMw//je9BWHtgTL++bYXwMHVZrjYthBO7v/TTov5ntknzFavGs1hyCLwCSrde+VlwqYvzZDz51acqg3MP3pxg86EGDBnCt/ytfn480SUWMyAmn3CDDp/rs9zif4H3Ph6qZcb0e8TJ6XuhoTPzZBzfM+Z7X4hENUL9vxstnyCufRM5xHmv6fTgb4867uk11bYQWHnUqD6dp3CQoM1e1OYujiB1YfzyM63A+BhtdCjWU0Gtq1Pl8hQbNZL4H/WeVkw/zFzGQuA2H/Cjf81WxwWPGYucQHQbQx0HVWy1gLDgC3z4Punz0x+WPcKaH7L6YBzgZaSLd+YYSnjGGAxw9c1z2D38C2bn+8Nc+D7MeYItSIePtC4O0TdAJHXQ0Do6RFn3c0ZsyOuhdvnXDjkncuRDTBrgHkNONOKc8Xd5q0q60W6hB7fY9bH1m/g8Lri+2xe5kzcwfXM9dSKpiRISzTDqWGHKuHmbct6zk+FoN8nJVCQa86OvmFO8SDr4QvNbjTX8mvUDWyeUJAH8bPM1s9TB83jAuucDj13Yrd4KOxUBAo7FZ/q27WK6rtx02i+3XSMT38/yIaDJx37awX5cGubevRvXY8GIf7uK2hJGAasmgyL/2O2GNRpZf6PdNdisFjN1pw29zh/3dx0s1NzaJRzt4KyT5hzAxUFreD62Hu/Tnx69b/3871/FXx8IxQWmGuMRfYyb8E17g5e5/geHf4DPrwBCrKh3UPQ66WSv9eeZTBnMOSlQ3AYtB1ydiuOM04eMG+DBdQ0Q41/jfOHpYNr4Ysh5jlWD+j+H+g4/OLh6k9K9PskPwfys5yfLLMyyEwxW0UPrjZfW6zmzOgtB5ozpXsHnPu8glzz5/rX1yDtsLktqC6FVz1BvCWa2CuuVNhxJ4Wdik/17Vrnqu9tR9P4dO1Bvlp/mJN/GrnVpEYA3ZvVoHtUDa4Ir3r+tbncbfdPMPeeMwuVeviYLQNNb3BfeeYPN/9oA8lhN1Ltjg+xeXo5f630ozC1i9liFN0f+r5XspaaLV/DZ3eaz3u/Bm3vu/g5CXPhqwfNUWoNOsNts1y/en32SVjwuNnpGcw/xH2nlnjdtAv+PinIM2/L/fKy+T7XPGMu1utEmCpTpw6ZQfbAKji4xqzr1neYLYnl0e8reQfMvhVO7APvYOg2yrxt6MyadEWtQr++Zt6WBDKqtsD3kV8VdtxJYafiU3271oXqO7fAzg+bj/HZ7wdZuTsVe+GZXyHBvp50jQzlmmY16BoZShW/UvzhLk8n9plDwdMSYcB0CGvn3vLkZcLS5zFWv4sFg8JWd2C96S3nOt/a8+Hjm+DASghtBvctOf//vM/ll1dh6XNmv6bBc82WoPNZ/S58P9p83vwW6PeeOfO1OxiG+Qf1u1Fm65R/KPSdAhE9LnrqOX++CwvNfik/TfhLPyfMeZxumQLBdcvhg/xJYaE5c/eBlacDzuozfWH+yreauRZdm3uKL7j7d+xZBp/eCbmnzH5X//zMbLksrfwc+ONjjJVvkmGrit/DPyvsuJPCTsWn+natktb3qax8ft6ZzE/bkvhpe1KxFh+rBa4Ir8o1zWpyc1wdagdXkBEahmHezjrXYqVuYk/4EusX92KhENo/DD0nlDzwfP80rJ4MXoEwdBmERDj35oZhttRsnGP+T/6+xWf/gTMMWDLWnG8IzL5G179UMeoweTvMHQLHNpmvOz5q3tq6QAgr9vNttZrDp38cf+YaATXNPlxWm9kHKj8LfKpAn0nmXE5/V26GGahO7DMXxD2xD07sNWf//nN/KzBvIdVqCeEdzWkVUnfC7x+Z80EVaXwNtL0XmvR0vu9VkXUfwcJ/mbdB67c3W+z8Q0p3rb+oCB2UNfRcREot2M+Tm2LrcFNsHeyFBusPnGDptiSWbkti29F01u47wdp9J3j5+210bhLKgDb16dG8Bt4ebvwjabGYrRgVSfOb2b9rCw02vGIGF58g6Db64udt+tI8HqDvu84HHTDr46Y3zT+4B1fD7AFw39Izc+DY8+GbR2HDJ+br7v+Bzv+qOEO/Q6PMIfQ//B+sfR9WvgVr3oea0eY8QrXjzK+hTc3OtH92cLXZqnVglfnaO9icUqDdA2f6OYV3gi/vg8T15nQGOxeb8yx5B168bIV2OPibOcz/+J4z4ebPQ7b/ysMX6rWBsA4Q3sGc9+mv73XVE2ZA+30a7PrxzHD+oHpmB/HWd0BgrRJUHmZr0pJnzWkVwFzY96a3K93iumrZQS07lwLVt2uVRX0fPpnN0q3HWLDxCL/tPfO/1Sp+ntwSV5db29SjRR0X9/WooIrqu1Xeb1gXjTE3XjcBOj5y/pOStpkjqvIzzflNeoz9e4XITDGvd3I/hHWEO+eZ/8v/7K7TnbltZihqNfjvvU952roAFo44PdrtLzx8HAGosEZz0n7/nCrHVp3Z1+4Bcw6kc3VItufDspfMuWSMQvMWT7/3of6VZx+bn23eDtq2ALZ/B1mp5y6rTxXzOlXDzZFlVcOhVizUjgUPJ27/Ht9jTuC4fuafWoUsZsBrfI15W69e23O3+ORlwpdDzbICdHsauj5V5kG2IrTsuDXsTJ06lR9++IE9e/bg4+NDq1atePLJJ2nUyLz/eOjQIa655ppznjtp0iR69eoFQFTU2fcUX3/9dXr37l2icijsVHyqb9cq6/rel5LJ3HWHmLvuEEfTchzbW9QJYkCb+twcV6fi9e9xoWL1veJ1WPq8uaPPm+bMtH+Vk2YGk9Sd5jDvwV+W/vbFnyVtg2nXmpMCRv/DvLVyeJ3Z2nDrRxB1/d9/j/JWWGiWO3E9HImHxHhzmHzRRId/YlhsWFrfYd6yCqpz8WvvXwlfPmD2o7HYzMkhu4w0r71jkRkadi81b3sV8akCTa6D2i1Ph5oGZrAp607d+Tlmh/Pfp5mtSX/mHQSNup4OP9dAlTBIOwKfDDTrxuYFN78DLW8t2zKddtmHnXvvvZfevXsTExOD3W7n9ddfZ+fOnSxcuBA/Pz/sdjvHjxe/f/npp58ybdo0li9fjr+/2cwYFRXFiy++SOfOnR3HBQUF4e1dso5zCjsVn+rbtcqrvu2FBst3pfDZ7wdZvPkYeXZz8jgvm5XmdYKIrRdMTL0qtKwXTOPQgEtjLp8ycFYfksXPnL6tYIF/TDNHWBUxDHPZiq3zzbloHvilzPpWALBrCcy69czEfr5VzY6q52rFuFT8JQAZRzaSmu9D1Zuew1bDyQ64Oadg4ZNn5m4Krm92eDfsZ44Jrm8O0W7a27wd9dfbZ+Ut/agZunb9aH79az+gkEgzMGccNSfGvG22Y4LL8lARwo5b++xMmzat2OuXXnqJDh06sHnzZtq2bYvNZiM0NLTYMUuWLKFXr16OoFMkKCjorGNFpGKxWS10jQyla2QoJzLz+Dr+MJ/+foitR9KIP3iS+IMnAXMkjJ+Xjeg6wcTUC6ZlvWBa1qtCeDU/rJU9AFkscO14yMuA3z8wbzN4+p9pVVn5phl0rJ7miLKyDDpg3vbo9bI5+WFwfRj8xd8bkVMRWK3mWmbVG0PMPyi029kfH0/V6qXo4+QTDP3fN1trFo44M4lezegzAadWS/f2aQqsZS6pEfdPs9/QkXgz+Oz6EQ6tMUd8AYREwT8/hWoN3VdWF6lQHZTT09MBCA4+d/Pepk2b2Lp1K88888xZ+8aNG8e///1v6tevz2233Ub//v2dXs/Hbrdf/KC/HOvMOVJ6qm/XckV9B/nYuKN9GHe0D2NfaiYbD51i0+E0Nh4+xebENLLy7KzZd5w1+878r9TPy0aTGgFE1gwgqmYgkTUDiKwZSEiA16Wzftc5nLO+r38ZS0461k2fY3x2J4X//AwMA+uSsViAwutfwqjdCsrje3TFEHNETnB9s3NsJft3VyY/3y36Qf32WA7+hlGn1ZmlQcBsSapIasWZj07/Mlum9v6M5cQ+jNZ3meGtnL+/5fn7pKTXrDAdlAsLC3nooYdIS0vjk08+OecxY8eOZc2aNXz77bfFtk+ePJn27dvj6+vL8uXLeeuttxg5ciR33nlnid67qBlMRCoGu2FwOK2A3ScK2H0in13H89l3Mp/88/wNCfKyUD/Yk/BgD0L9bQR6WQnwspz+aiXQy4K/lxWPS61VqLCAxr+Po8qxFdhtPhTavPHMO0VKvZ7sjyv7jqQil6oKfRvrz8aNG8fOnTuZPXv2Offn5OSwYMEChg0bdta+hx9+2PG8efPmZGdnM23atBKHnSIxMTFO9dlJSEhw6hwpPdW3a1WE+v7rikcF9kL2pWax41gGO46lsyMpg+1H09l/PIu0PIPNyXlsTs674DUDvG0E+3oSEuBN/Wq+1K/qR1g1P+pX8yWsqh+1gn3c0k/ogvXd8nOMObdj2/szNnsORs0Yqg7+gKoVcGXpS0VF+Pm+nJRnfRdd+2IqRNgZP348y5YtY+bMmdSqde65Ab7//ntycnK45ZZbLnq92NhY3nnnHfLy8vDyKvkID5vN5vQ3ojTnSOmpvl2rItW3zWYjqnYwUbWL3+bOybez63Tw2X4snWNpOZzIyudUVh4ns/M5mZVPWk4+hgEZuXYycu0cPpnDhkOnznoPT5uFulV8qV/Nj8ahAfRuWZs24VVddovsnPVt84fbPzHneEndjWXgDGw+TsyQLOdVkX6+LwfurG+3hh3DMHjuuedYvHgxM2bMoH79+uc99osvvqB79+5Uq3bxRdm2bt1KcHCwU0FHRC5NPp42ousGE133/EN57YUGadn5p8NPHsfScjh4PJsDx7M4cDyLg8ezOHgii3y7wb7ULPalZvHrzhQ+WrmP8Op+9G9dj36t61Kvqp8LP9mfePnDoM/NkVi6dSXiNLeGnXHjxrFgwQLeeecd/P39SU5OBiAwMBAfnzOzN+7fv5+1a9fy3nvvnXWNpUuXkpqaSmxsLN7e3qxYsYKpU6cyZMgQl30OEanYbFYLVf29qOrvBZx7lXZ7ocHRtBwOpJrhZ82+43ybcIT9qVm8vngHry/eQcfG1enfuh69Ymrh5+WGX58KOiKl4tawU9QR+Y477ii2/cUXX6Rfv36O11988QW1atWiU6dOZ13Dw8ODWbNm8cILLwAQFhbG6NGjGTBgQDmWXEQqG5vVvIVVt4ovHRpXZ0Db+oy7qQXfbzrKF38cYuXuVMfjma83cUNMbfpfUY82FXmldxEB3Bx2tm/fXqLjRowYwYgRI865r0uXLnTp0qUsiyUiAoC/twf9r6hH/yvqcfB4Fl+tP8zcdYc4cDyLz9cd4vN1h/D3stE6vCptG1SjTYOqtKpfFV8v9QMRqUgqRAdlEZGKrn41P4Zf04RHu0fw+/4TzP39EN9uOkJ6TgG/7kzh153m4o6eNgvRdYNp26CaGYDCq56+fSYi7qKwIyLiBIvF4ggyL/SLYfvRdNbuO+54HEvLZf2Bk6w/cJL3ftkDgK+nDW9PKz4e5/7qbbPiVZDBDR5Had84hJCAki11IyIlo7AjIlJKNquF5nWCaF4niLs6NsAwDA6dyGbN3uP8vv84a/YeZ3dyJtn5drLz7UD+Ba/37a54ABqH+nNlw+q0b1SNKxtWo3aw5tQR+TsUdkREyojFYqF+NT/qV/Oj/xX1ADiVnU9adj65BXZy8gsdX3Py7eQWmF8zc/NZuXkfe9KtbD+Wwe7kTHYnZ/LJmgMA1K/my5UNzPDTJTKUmkE+FyqGiPyFwo6ISDkK9vUk2PfCq17b7Xaaex0nLi6O9Fw7a/edYM3eVH7be5xNh09x8Hg2B48f4os/DgHQtFagY0HVKxpUxdtDHaJFLkRhR0SkAqni58W1zWtybfOaAGTkFrBu/wl+25PKit2pbDx0km1H09l2NJ2pv+zB19NGx8bV6XI6/DQIOfc8QiKXM4UdEZEKLMDbw9GKA3A8M49fdybz845kftmRQkpGLj9uS+LHbUkA1An2oXYVX6r7e1E9wJuQAK8/PTdfhwR4U8XP85JeKV7EGQo7IiKXkGr+XtwcV5eb4+pSWGiw9Wgav+xI4ecdSazbf4LEUzkknsq56HUCvD0Ir+5HgxB/GlT3o0F1/9PP/QkJ8FIQkkpFYUdE5BJltVpoUSeYFnWCeahbYzJyC9iSmEZqRi4pmXmkZuSSmpFHamYuKRmnX2fmcTIrn4zcAjYnprE5Me2s6xYFobYNqtGzRS3aNtAs0XJpU9gREakkArw9uLLhxRdLzi2wc/B4FntTstifmsnelEz2p2axNyWTxFPZxYLQRyv3Uc3fi+ua16RndC06Nq6uDtFyyVHYERG5zHh72IioEUhEjcCz9hUFoZ3HMvhxWxKLtxzjeGYec9YeZM7agwR6e9C9WQ2ub1GLrlGhxRZELSw0yMwrICO3gIycAtJPf/W0WWleO4hgvwuPShMpLwo7IiLi8Ocg1CumNvn2QtbsPc73m46yaPNRktJz+To+ka/jE/HxtFK3ii+ZuXYz4OQWXPDaYdX8iK4bRHTdYKLrBBNdN5hqWkpDXEBhR0REzsvTZuWqiBCuighh3E0tWH/wBN9vOsr3m49y8Hg2u5MzzzrHw2ohwMeDAG/zkZFbwKET2Rw4nsWB41l8m3DUcWzdKr60qBNEs9pBhAR6U9XPk6p+XlTx86SavxdV/bzw8dRtM/l7FHZERKRErFYLV4RX44rwajx9QzO2H0vneEYegT6ejnAT6OOBt4f1rNFcJ7Py2JyYxqbDp0g4fIrNiWnsTcnk8MlsDp/M5octx877vj6eVqr6eVE9wIvoOsG0aVCNKxtUo341X40akxJR2BEREadZLBaa1goq8fFV/LwcLURF0nPyHQFod3IGxzPzOJGVz8msPI5nml8LCg1y8gs5ciqHI6dy2HQ4jTlrDwJQM8jbEXzaNKhK01pB2KwKP3I2hR0REXGLQB9P2jeqTvtG1c+53zAM0nMLOJmZz4msPI6cymH9gROs3XechMOnOJaWy8KNR1i48Yh5PW8PWodXpXqAF4WFBnYDCg3DfF5oUHj6dYG9EK+CTPp6HKVTZI2LLuchlz6FHRERqZAsFgtBPp4E+XgSVt2P2PpwfXQtALLz7Gw4dJK1e4+zdv8J/th/gvTcAn7ekVzi6y/ZG4/NaqF1WJXTs1TXoEWdIKxqHap0FHZEROSS4+tlK9YqVGAvZNvRdNYfOEF2vh2rxYLVYsFmtWC1mP2NrBYLNosFwyjkl4Q9bD1hYU9KJmv3nWDtvhO8+sMOqvt7OdYZa1kvGHuhQW5BIXn2QvJPf80rKCTfXkhuQSG20xM7Nqjup/5DFZjCjoiIXPI8bFZzSHvd4Isea7fbibClEBcXR+KpXH7eYa41tnJXCqmZeXy1/jBfrT/s1PtX8/eiVf0qtA6vSquwKsTWq4K/t/7EVhT6ToiIyGWrfjU/BrcPZ3D7cPIKClm3/wQ/70hm2fYkDh7PwtPDipfNipfH6Yet+NesPDtbjqRxPDOv2IKsVgtE1Qqi9engY7FAZm4BmXnmnESZuQVk5tpPbzNf+3t7EBLgTWjgmQVbz7z2ppq/lzpgl5LCjoiICODlYaVD4+p0aFyd0b2alvi83AI7WxLT+OPASf44cIL1pxdk3Xokja1H0pj124EyKZ/VAjWDfByLtjYMMRdwbRjiT1h1Py3jcQEKOyIiIn+Dt4eNVmFVaRVWlXtpCMDR0yPH/jhwgq1H0rFZLQR4e+DvbcP/9GSLfl4eBJx+7eflQWZuAckZuaSk55KSkXv6eR4pGbkcz8qj0MAxBH/VntRiZbBYoE6wL41C/alX1Y8agd6EBHoTerplqOirr9flGYgUdkRERMpYrWAfesXUpldM7TK5XoG9kOOZeRw8kc2+lEz2nV7AdV9qJvtSssjILXBM0HghAd4ehAR4Uc3fCy8PK54285ach81y1nNvDxv1qvrSuEYAjUP9qRPse8mOVFPYERERqeA8bFZqBPlQI8iHK8KrFttnGAbJGbnsS8liX0omh05mmy1DRS1E6eYjt6DQsYbZvtQsp8vg62mjUag/jUMDzEcN87mfl418eyH5dqPY14LTX3PzCyDLXlZVUSoKOyIiIpcwi8VCjUAfagT6cGXDauc8xjAMMnILTgegPI5n5p0OJmYoyfvL8wK7QVZ+AftTstidnMG+1Eyy8+1sTkxjc2Ka02Ws6mOlRwfj737UUlPYERERqeQsFguBPp4E+njSKNT58wvshRw4nsXu5Ex2J2ewOymDXckZ7E3JJL+gEE8PKx5WK142Cx42K56nb4V52qx4WKFpkN2t8xAp7IiIiMgFedisNAoNoFFoANdS06lz7XY78fHx5VOwErK69d1FREREypnCjoiIiFRqCjsiIiJSqSnsiIiISKWmsCMiIiKVmsKOiIiIVGpuDTtTp06lf//+tGrVig4dOjBs2DD27NlT7Jg77riDqKioYo9nnnmm2DGJiYkMHTqU2NhYOnTowMSJEykoKHDlRxEREZEKyq3z7KxZs4ZBgwYRExOD3W7n9ddf595772XhwoX4+fk5jhswYADDhw93vPb19XU8t9vtPPDAA4SEhDBnzhySkpIYNWoUnp6ejBgxwqWfR0RERCoet4adadOmFXv90ksv0aFDBzZv3kzbtm0d2318fAgNPfeUj8uXL2fXrl18+OGHhISE0KxZMx577DFeffVVHnnkEby8vMr1M4iIiEjFVqFmUE5PTwcgODi42Pb58+fzzTffEBoaytVXX82wYcMcrTvx8fFERkYSEhLiOL5Tp06MHTuWXbt20bx58xK/v91e8oXKio515hwpPdW3a6m+XUv17Vqqb9cqz/ou6TUrTNgpLCzkhRdeoHXr1kRGRjq233jjjdSpU4caNWqwfft2Xn31Vfbu3cvbb78NQEpKSrGgAzheJycnO1WGhIQEp8tdmnOk9FTfrqX6di3Vt2upvl3LnfVdYcLOuHHj2LlzJ7Nnzy62feDAgY7nUVFRhIaGcvfdd3PgwAHCwsLKtAwxMTHYbLYSHWu320lISHDqHCk91bdrqb5dS/XtWqpv1yrP+i669sVUiLAzfvx4li1bxsyZM6lVq9YFj42NjQVg//79hIWFERISwsaNG4sdk5KSAnDefj7nY7PZnP5GlOYcKT3Vt2upvl1L9e1aqm/Xcmd9u3XouWEYjB8/nsWLF/Pxxx9Tv379i56zdetW4EyQiYuLY8eOHaSmpjqOWblyJQEBAURERJRPwUVEROSS4daWnXHjxrFgwQLeeecd/P39HX1sAgMD8fHx4cCBA8yfP5+uXbtSpUoVtm/fzosvvkjbtm1p2rQpYHZGjoiI4KmnnmLkyJEkJyczadIkBg0aVOKRWIZhAOqgXJGpvl1L9e1aqm/XUn27lis6KBf9HT8fi3GxI8pRVFTUObe/+OKL9OvXjyNHjjBy5Eh27txJVlYWtWvXpkePHgwbNoyAgADH8YcPH2bs2LGsWbMGX19f+vbty7/+9S88PEqW5fLy8tRRTURE5BIVExNzwQYOt4adiqKwsJCCggKsVisWi8XdxREREZESMAyDwsJCPDw8sFrP3zNHYUdEREQqNS0EKiIiIpWawo6IiIhUago7IiIiUqkp7IiIiEilprAjIiIilZrCjoiIiFRqCjsiIiJSqSnsiIiISKWmsFMKs2bNonv37sTExHDrrbeeteq6nG3q1Kn079+fVq1a0aFDB4YNG8aePXuKHZObm8u4ceNo164drVq14tFHH3WsYF8kMTGRoUOHEhsbS4cOHZg4cSIFBQXFjvntt9/o27cv0dHRXHvttXz55Zfl/vkquvfee4+oqCgmTJjg2Kb6LlvHjh3jySefpF27drRs2ZI+ffoUW4bGMAzeeOMNOnXqRMuWLbn77rvZt29fsWucPHmSf/3rX7Ru3Zo2bdrw9NNPk5mZWeyYbdu28c9//pOYmBi6du3K+++/74qPV6HY7XYmTZpE9+7dadmyJT169GDy5MnF1kdSff89a9eu5cEHH6RTp05ERUWxZMmSYvtdWb/fffcd119/PTExMfTp04eff/7Z+Q9kiFMWLlxotGjRwpg7d66xc+dO4//+7/+MNm3aGCkpKe4uWoU2ZMgQ44svvjB27NhhbN261bj//vuNbt26GZmZmY5jnnnmGaNr167GypUrjYSEBGPAgAHGwIEDHfsLCgqMG2+80bj77ruNLVu2GMuWLTPatWtnvPbaa45jDhw4YMTGxhovvviisWvXLmPGjBlGs2bNjF9++cWln7ci2bBhg3H11Vcbffr0MZ5//nnHdtV32Tl58qRx9dVXG6NHjzY2bNhgHDhwwPj111+N/fv3O46ZOnWqccUVVxiLFy82tm7dajz44ING9+7djZycHMcx9957r3HTTTcZ8fHxxtq1a41rr73WGDFihGN/enq60bFjR+Nf//qXsWPHDmPBggVGy5YtjTlz5rj087rbu+++a1x55ZXGTz/9ZBw8eND47rvvjLi4OOPjjz92HKP6/nuWLVtmvP7668YPP/xgREZGGosXLy6231X1u27dOqNZs2bG+++/b+zatcv473//a7Ro0cLYvn27U59HYcdJ//jHP4xx48Y5XtvtdqNTp07G1KlT3ViqS09qaqoRGRlprFmzxjAMw0hLSzNatGhhfPfdd45jdu3aZURGRhrr1683DMP8x9e0aVMjOTnZcczs2bON1q1bG7m5uYZhGMbLL79s9O7du9h7Pf7448aQIUPK+RNVTBkZGcZ1111nrFixwhg8eLAj7Ki+y9Yrr7xi3H777efdX1hYaFx11VXG//73P8e2tLQ0Izo62liwYIFhGGfqf+PGjY5jfv75ZyMqKso4evSoYRiGMWvWLKNt27aO+i967549e5b1R6rQhg4daowZM6bYtkceecT417/+ZRiG6rus/TXsuLJ+H3vsMWPo0KHFynPrrbca//nPf5z6DLqN5YS8vDw2b95Mx44dHdusVisdO3Zk/fr1bizZpSc9PR2A4OBgADZt2kR+fn6xum3cuDF16tQhPj4egPj4eCIjIwkJCXEc06lTJzIyMti1a5fjmA4dOhR7r06dOjmucbkZP348Xbt2LVavoPoua0uXLiU6Oprhw4fToUMHbrnlFj777DPH/kOHDpGcnFysvgMDA4mNjXX87li/fj1BQUHExMQ4junYsSNWq9Vxqzw+Pp42bdoUW925U6dO7N27l1OnTpX3x6wwWrVqxerVq9m7dy9g3gpZt24dXbp0AVTf5c2V9VtWv2M8nDr6MnfixAnsdjvVq1cvtr169epn9T+R8yssLOSFF16gdevWREZGApCSkoKnpydBQUHFjq1evTrJycmOY/78hxdwvL7YMRkZGeTk5ODj41Mun6kiWrhwIVu2bGHu3Lln7VN9l62DBw/yySefcM899/Dggw+SkJDA888/j6enJ3379nXU17l+dxT1k0pJSaFatWrF9nt4eBAcHFysvuvVq1fsmKL6T0lJcfznobIbOnQoGRkZ9OrVC5vNht1u54knnuCmm24CUH2XM1fW77l+x/z5fUpKYUdcbty4cezcuZPZs2e7uyiV1pEjR5gwYQIffPAB3t7e7i5OpWcYBtHR0YwYMQKA5s2bs3PnTubMmUPfvn3dXLrK57vvvmP+/Pm89tprREREsHXrVl588UVq1Kih+pZz0m0sJ1StWhWbzUZqamqx7ampqWclTzm38ePHs2zZMj7++GNq1arl2B4SEkJ+fj5paWnFjk9NTSU0NNRxzF/TfNHrix0TEBBw2bQyAGzevJnU1FT69etH8+bNad68OWvWrGHGjBk0b95c9V3GQkNDady4cbFtjRo1IjEx0bEfuODvjpCQEI4fP15sf0FBAadOnSrR9+Ry+h308ssvM3ToUHr37k1UVBS33HILd911F1OnTgVU3+XNlfV7rmNK8zdXYccJXl5etGjRglWrVjm2FRYWsmrVKlq1auXGklV8hmEwfvx4Fi9ezMcff0z9+vWL7Y+OjsbT07NY3e7Zs4fExETi4uIAiIuLY8eOHcX+ga1cuZKAgAAiIiIcx6xevbrYtVeuXOm4xuWiffv2zJ8/n3nz5jke0dHR9OnTx/Fc9V12Wrdu7eg/UmTfvn3UrVsXgHr16hEaGlqsvjMyMtiwYYPjd0erVq1IS0tj06ZNjmNWr15NYWEhLVu2BMz6/v3338nPz3ccs3LlSho2bHhZ3VLJycnBYrEU22az2RxDz1Xf5cuV9Vtmv2Oc6s4sxsKFC43o6Gjjyy+/NHbt2mX85z//Mdq0aVNsxIqc7dlnnzWuuOIK47fffjOSkpIcj+zsbMcxzzzzjNGtWzdj1apVRkJCgjFw4MBzDoUeMmSIsXXrVuOXX34x2rdvf86h0BMnTjR27dplzJw587IcCn0ufx6NZRiq77K0YcMGo3nz5sa7775r7Nu3z/jmm2+M2NhY4+uvv3YcM3XqVKNNmzbGkiVLjG3bthkPPfTQOYfq3nLLLcaGDRuM33//3bjuuuuKDdVNS0szOnbsaIwcOdLYsWOHsXDhQiM2NvayGAr9Z6NGjTI6d+7sGHr+ww8/GO3atTNefvllxzGq778nIyPD2LJli7FlyxYjMjLS+PDDD40tW7YYhw8fNgzDdfW7bt06o3nz5sa0adOMXbt2GW+++aaGnrvKjBkzjG7duhktWrQw/vGPfxjx8fHuLlKFFxkZec7HF1984TgmJyfHGDt2rNG2bVsjNjbWePjhh42kpKRi1zl06JBx3333GS1btjTatWtnvPTSS0Z+fn6xY1avXm3cfPPNRosWLYxrrrmm2Htczv4adlTfZWvp0qXGjTfeaERHRxvXX3+98emnnxbbX1hYaEyaNMno2LGjER0dbdx1113Gnj17ih1z4sQJY8SIEUZcXJzRunVrY/To0UZGRkaxY7Zu3WrcfvvtRnR0tNG5c+fLctqL9PR04/nnnze6detmxMTEGNdcc43x+uuvFxvCrPr+e1avXn3O39mjRo0yDMO19fvtt98a1113ndGiRQujd+/exrJly5z+PBbD+NOUkyIiIiKVjPrsiIiISKWmsCMiIiKVmsKOiIiIVGoKOyIiIlKpKeyIiIhIpaawIyIiIpWawo6IiIhUago7IiLnEBUVxZIlS9xdDBEpA1r1XEQqnNGjR/PVV1+dtb1Tp05MmzbNDSUSkUuZwo6IVEidO3fmxRdfLLbNy8vLTaURkUuZbmOJSIXk5eVFaGhosUfRSshRUVHMnj2b++67j5YtW3LNNdfw/fffFzt/+/bt3HnnnbRs2ZJ27drxn//8h8zMzGLHzJ07l969exMdHU2nTp0YP358sf0nTpzg4YcfJjY2luuuu44ff/yxfD+0iJQLhR0RuSS98cYb9OzZk6+//po+ffowYsQIdu/eDUBWVhb33nsvwcHBzJ07l0mTJrFy5Uqee+45x/mzZ89m/PjxDBgwgPnz5/POO+8QFhZW7D3efvttevXqxTfffEOXLl148sknOXnypCs/poiUAYUdEamQli1bRqtWrYo9pkyZ4th//fXXc+utt9KwYUMef/xxoqOjmTFjBgALFiwgLy+PiRMnEhkZSYcOHXjmmWf4+uuvSUlJAeDdd9/lnnvu4a677qJhw4a0bNmSu+++u1gZ+vbty4033kh4eDgjRowgKyuLjRs3uqwORKRsqM+OiFRI7dq1Y+zYscW2Fd3GAmjVqlWxfXFxcWzduhWA3bt3ExUVhZ+fn2N/69atKSwsZO/evVgsFpKSkujQocMFyxAVFeV47ufnR0BAAMePHy/tRxIRN1HYEZEKydfXl/Dw8HK5tre3d4mO8/T0LPbaYrFQWFhYHkUSkXKk21gickmKj48v9nrDhg00btwYgMaNG7N9+3aysrIc+//44w+sVisNGzYkICCAunXrsmrVKlcWWUTcRGFHRCqkvLw8kpOTiz3+fAvp+++/Z+7cuezdu5c333yTjRs3MnjwYAD69OmDl5cXo0ePZseOHaxevZrnnnuOm2++mZCQEAAeffRRPvzwQ6ZPn86+ffvYvHmzo8+PiFQuuo0lIhXSr7/+SqdOnYpta9iwoWOI+aOPPsq3337LuHHjCA0N5bXXXiMiIgIwb4FNmzaNCRMm8I9//ANfX1+uu+46Ro8e7bhW3759yc3N5aOPPuLll1+mSpUqXH/99a77gCLiMhbDMAx3F0JExBlRUVFMnjyZHj16uLsoInIJ0G0sERERqdQUdkRERKRS020sERERqdTUsiMiIiKVmsKOiIiIVGoKOyIiIlKpKeyIiIhIpaawIyIiIpWawo6IiIhUago7IiIiUqkp7IiIiEilprAjIiIildr/A71uty2MPA6RAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0jeSAHc0vfn"
      },
      "source": [
        "## Compute training vs validation rouge scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_summaries_train, true_summaries_train = predict(model, train_data_indices)\n",
        "pred_summaries_val, true_summaries_val = predict(model, val_data_indices)"
      ],
      "metadata": {
        "id": "b7OeQexk7EYS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-kMyz1m1Y9K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "ec3ab053-ea71-4337-9762-b500f72fca59"
      },
      "source": [
        "scores_train =  compute_rouge(pred_summaries_train, true_summaries_train)\n",
        "scores_val  = compute_rouge(pred_summaries_val, true_summaries_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uORi6W3F11sy"
      },
      "source": [
        "##  Rouge scores Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Llrnv9_1yn0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "9decf176-c689-4772-b4f5-f6731aed2258"
      },
      "source": [
        "compute_rouge_summary_stats(scores_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rogue1_precision': (0.3525736696950816,\n",
              "  0.3517959245235172,\n",
              "  0.35335141486664595),\n",
              " 'rogue1_recall': (0.3525736696950816,\n",
              "  0.3517959245235172,\n",
              "  0.35335141486664595),\n",
              " 'rogue1_fmeasure': (0.3525736696950816,\n",
              "  0.3517959245235172,\n",
              "  0.35335141486664595),\n",
              " 'rogueL_precision': (0.2813920324190128,\n",
              "  0.2807105100795196,\n",
              "  0.282073554758506),\n",
              " 'rogueL_recall': (0.2813920324190128, 0.2807105100795196, 0.282073554758506),\n",
              " 'rogueL_fmeasure': (0.2813920324190128,\n",
              "  0.2807105100795196,\n",
              "  0.282073554758506)}"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OtCLukw_19hR"
      },
      "source": [
        "## Rouge Scores Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS_w0dxw2CLK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "outputId": "2529b1f3-1b0f-451d-8b7b-1f39c4be33de"
      },
      "source": [
        "compute_rouge_summary_stats(scores_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'rogue1_precision': (0.3072103622387896,\n",
              "  0.3060759200075915,\n",
              "  0.30834480446998774),\n",
              " 'rogue1_recall': (0.3072103622387896,\n",
              "  0.3060759200075915,\n",
              "  0.30834480446998774),\n",
              " 'rogue1_fmeasure': (0.3072103622387896,\n",
              "  0.3060759200075915,\n",
              "  0.30834480446998774),\n",
              " 'rogueL_precision': (0.23593382187082484,\n",
              "  0.23498998761580395,\n",
              "  0.23687765612584574),\n",
              " 'rogueL_recall': (0.23593382187082484,\n",
              "  0.23498998761580395,\n",
              "  0.23687765612584574),\n",
              " 'rogueL_fmeasure': (0.23593382187082484,\n",
              "  0.23498998761580395,\n",
              "  0.23687765612584574)}"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1XAu5Lrg4jx"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}
