{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6rTJ4hWNeSly"
      },
      "source": [
        "# Step 1: Setting Up the Environment\n",
        "We are configuring for a natural language processing project using TensorFlow and Keras. It includes:\n",
        "\n",
        "1. **Core Libraries**: Importing `numpy` and `pandas` for data manipulation.\n",
        "2. **Text Processing**: Using `pickle` for object serialization, and `nltk` for downloading stopwords to filter out common words.\n",
        "3. **Neural Network Setup**: Utilizing TensorFlow and Keras to build and train Recurrent Neural Networks (RNNs), with layers like `LSTM` (a type of RNN layer) and `Embedding` designed for sequence-based tasks such as text classification or generation.# 1. Import the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m7ckPaqueSlz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pickle\n",
        "import string\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input,LSTM,Embedding,Dense,Concatenate,TimeDistributed,Bidirectional,Attention, Lambda\n",
        "from tensorflow.keras.models import Model, Sequential,load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import tensorflow.keras.backend as k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPuda5aWg04H",
        "outputId": "124eefda-328a-4ff1-8463-8f89fd060d31"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 52,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDHP3JeUeSl0"
      },
      "source": [
        "# Step 2: Implementing Custom Attention Layer\n",
        "\n",
        "This code snippet defines a custom Attention Layer using TensorFlow, designed for sequence-based tasks like machine translation. It initializes trainable weights (`W_a`, `U_a`, `V_a`) to compute attention weights and context vectors from encoder and decoder outputs. The `call` method processes inputs through energy and context computation steps:\n",
        "\n",
        "- **Energy Step**: Calculates attention scores by combining and activating the products of encoder sequences with weights.\n",
        "- **Context Step**: Computes context vectors by applying these scores to encoder sequences.\n",
        "\n",
        "The layer outputs these context vectors and scores, critical for models that require understanding the importance of different parts of the input sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "syQKxiXxeSl1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.python.keras import backend as K\n",
        "\n",
        "logger = tf.get_logger()\n",
        "\n",
        "class AttentionLayer(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        assert isinstance(input_shape, list)\n",
        "\n",
        "        self.W_a = self.add_weight(name='W_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.U_a = self.add_weight(name='U_a',\n",
        "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "        self.V_a = self.add_weight(name='V_a',\n",
        "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
        "                                   initializer='uniform',\n",
        "                                   trainable=True)\n",
        "\n",
        "        super(AttentionLayer, self).build(input_shape) \n",
        "    def call(self, inputs):\n",
        "\n",
        "        assert type(inputs) == list\n",
        "        encoder_out_seq, decoder_out_seq = inputs\n",
        "\n",
        "        logger.debug(f\"encoder_out_seq.shape = {encoder_out_seq.shape}\")\n",
        "        logger.debug(f\"decoder_out_seq.shape = {decoder_out_seq.shape}\")\n",
        "\n",
        "        def energy_step(inputs, states):\n",
        "\n",
        "            logger.debug(\"Running energy computation step\")\n",
        "\n",
        "            if not isinstance(states, (list, tuple)):\n",
        "                raise TypeError(f\"States must be an iterable. Got {states} of type {type(states)}\")\n",
        "\n",
        "            encoder_full_seq = states[-1]\n",
        "            W_a_dot_s = K.dot(encoder_full_seq, self.W_a)\n",
        "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)\n",
        "            logger.debug(f\"U_a_dot_h.shape = {U_a_dot_h.shape}\")\n",
        "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
        "            logger.debug(f\"Ws_plus_Uh.shape = {Ws_plus_Uh.shape}\")\n",
        "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
        "            e_i = K.softmax(e_i)\n",
        "            logger.debug(f\"ei.shape = {e_i.shape}\")\n",
        "\n",
        "            return e_i, [e_i]\n",
        "\n",
        "        def context_step(inputs, states):\n",
        "\n",
        "            logger.debug(\"Running attention vector computation step\")\n",
        "            if not isinstance(states, (list, tuple)):\n",
        "                raise TypeError(f\"States must be an iterable. Got {states} of type {type(states)}\")\n",
        "\n",
        "            encoder_full_seq = states[-1]\n",
        "            c_i = K.sum(encoder_full_seq * K.expand_dims(inputs, -1), axis=1)\n",
        "            logger.debug(f\"ci.shape = {c_i.shape}\")\n",
        "            return c_i, [c_i]\n",
        "\n",
        "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
        "        fake_state_e = K.sum(encoder_out_seq, axis=2)\n",
        "\n",
        "        last_out, e_outputs, _ = K.rnn(\n",
        "            energy_step, decoder_out_seq, [fake_state_e], constants=[encoder_out_seq]\n",
        "        )\n",
        "\n",
        "        last_out, c_outputs, _ = K.rnn(\n",
        "            context_step, e_outputs, [fake_state_c], constants=[encoder_out_seq]\n",
        "        )\n",
        "\n",
        "        return c_outputs, e_outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "\n",
        "        return [\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
        "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tozQ2xeeekFl",
        "outputId": "d95bfeba-ccba-4454-a4c0-c6073d745705"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex7U0431eSl2"
      },
      "source": [
        "# Step 3: Loading and Preprocessing Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Downloaded our CNN/Daily Mail Dataset from Kaggle and have mounted it in our drives. Loading the data from the drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04ueVeaZeSl2"
      },
      "outputs": [],
      "source": [
        "df_train=pd.read_csv('/content/drive/MyDrive/DLProject/cnn_dailymail/train.csv')\n",
        "df_test=pd.read_csv('/content/drive/MyDrive/DLProject/cnn_dailymail/test.csv')\n",
        "df_val=pd.read_csv('/content/drive/MyDrive/DLProject/cnn_dailymail/validation.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "reOwWrEjeSl2",
        "outputId": "9d3ffa3a-2e53-4737-c1a9-9e169c1d1b12"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"df_train\",\n  \"rows\": 10000,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10000,\n        \"samples\": [\n          \"11b23c98cc7ee6d263ee5f9fcf5d92c67880d9bc\",\n          \"0d7132aa9a30671b65ebb66f637f980989517b3a\",\n          \"04fb35909f9611b19de4baec58c8e08bdd9d544d\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"article\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9997,\n        \"samples\": [\n          \"Washington (CNN) -- At first glance, it's a hoax: a storefront for a Museum of Unnatural History, complete with a bizarre animal skeleton visible through the front window and unicorn tears for sale. It's perfect for luring in curious children. But when volunteers get kids past the upside-down coyote hanging from the ceiling and the owl with an elephant trunk, they shape young minds with after-school tutoring, writing workshops and books written by kids. \\\"I think the kids are tricked in and then, as they are talking to the person at the front desk, they are learning about what's going on in the tutoring center,\\\" said Gerald Richards, the chief executive of 826 National, the nonprofit network of tutoring centers that look nothing like classrooms. The Washington \\\"museum,\\\" which set up shop in the Columbia Heights neighborhood in October, was the latest branch to open. Each of the eight centers has an imaginative storefront that helps raise money and encourage creativity among students. In Seattle, kids are lured into the writing lab by the Greenwood Space Travel Supply Co., which sells intergalactic peace treaties and travel cups. In Ann Arbor, Michigan, Liberty Street Robot Supply & Repair sells robot emotion upgrades, fruit powered clocks and robodentures. The original chapter opened eight years ago at 826 Valencia St. in San Francisco's Mission District. (As it spread, the original address inspired the name 826 National.) It was co-founded by Dave Eggers, author of \\\"A Heartbreaking Work of Staggering Genius,\\\" and founder of the publishing house, McSweeney's. Early on, organizers at 826 Valencia St. learned the space they'd rented was zoned for retail, so they needed to come up with something to sell. The space looked a bit like an old pirate ship. \\\"Someone thought, 'Why don't we sell pirate gear and see what happens?'\\\" Richards said. Since then, it has been fronted by The Pirate Supply Store, where shoppers can purchase hooks, peg legs and plank-by-the-foot. Now, Richards said, profits from stores account for 20 percent of the teaching that goes on in the writing center. And the imagination enticements work. 826 National is on track to serve 24,000 students this year through its tutoring centers and school outreach programs. Sisters Teah and Janea Green have been visiting 826DC most afternoons since it opened. \\\"This place is awesome,\\\" said Janea, 10, as she worked on her multiplication homework with volunteer Matt Lemanski. \\\"I really, really need help with my homework and my parents are busy sometimes.\\\" \\\"Sometimes we do poetry, sometimes we do math,\\\" said Teah, 9. When she doesn't have homework, she works on the story she is writing about a mermaid named Katopia. Writing is the primary focus at 826 centers, which also work with middle school and high school students. It also publishes books written by students. Their latest, \\\"I Live Real Close to Where You Used to Live,\\\" is a collection of student letters to First Lady Michelle Obama. It will be in bookstores on December 15. The letters run the gamut from asking if Obama cooks with an Easy-Bake Oven at the White House, to a suggestion that she use robots to keep drugs off the streets. The book is a companion to one that came out last year that contains letters to President Barack Obama, \\\"Thanks and Have Fun Running the Country.\\\" Profits from the book sales are used to support the tutoring centers. Kathleen Yancey, an English professor at Florida State University, said the 826 National centers complement the learning that goes on in classrooms. Schools are about college and job readiness, and tests that prove it, Yancey said. Those demands don't leave much time for playful reading or imaginative writing, even if it could help some students learn. \\\"If you want people to engage in activity, you find a way for them to have fun with it. 826 is a genius at that,\\\" said Yancey, a past president of the National Council of Teachers of English. \\\"If [students] don't take any pleasure in it, they won't actually practice it. When they have fun, they won't quit on you.\\\" Fun is what draws students in, too. \\\"It's OK to be strange,\\\" said Richards, the 826 leader. \\\"It's OK to be magical, it's OK to be whimsical.\\\"\",\n          \"The Twelve Days Of Christmas don't have much appeal to the modern woman - most ladies these days would prefer a cashmere sweater or a designer handbag to a partridge in a pear tree. But it's just as well, as the price of keeping\\u00a0your true love happy is\\u00a0$27,673.22 (\\u00a317,766) if you bought everything on the list once. That is up by one per cent on 2013's total, which was $27,393.17 (\\u00a317,606.56) US company PNC Wealth Mgmt has worked out the cost of the gifts in the song every year since 1984, when it totalled $12,673 (\\u00a38,160), it is now\\u00a0$27,673 (\\u00a317,766) The increase is the smallest since 2002, when the cost fell 7.6 per cent. PNC Wealth Management has worked out the cost of the twelve days of gifts every year since 1984, when the items totalled $12,673.56 (\\u00a38,160.17). They also broke down the prices for each individual item. Two turtle doves, cost $125 (\\u00a380.29), the same as last year, while five gold rings cost $750 (\\u00a3481.76) Nine ladies dancing (per performance) cost $7,553 (\\u00a34,851.67), while seven swans a-swimming are worth $7,000 (\\u00a34,496.45) While many of the gifts were the same cost as last year, like the two turtle doves, the seven swans-a-swimming and the nine ladies dancing, several had also gone up in value. 10 lords-a-leaping had gone up from $5,243 (\\u00a33,365.97) last year, to $5,348 (\\u00a33,433.38) now, although Femail thinks it would be tough to find any real lords to leap for that price. The price of a partridge (who sits in the pear tree) has gone up by a third this year - it is now\\u00a0$20 (\\u00a312.85) instead of $15 (\\u00a39.64) \\u2014 Partridge, $20 (\\u00a312.85); last year: $15 (\\u00a39.64) \\u2014 Pear tree, $188 (\\u00a3120.76); last year: $184 (\\u00a3118.19) \\u2014 Two turtle doves, $125 (\\u00a380.29); last year: same . \\u2014 Three French hens, $181 (\\u00a3116.27); last year: $165 (\\u00a3106) \\u2014 Four calling birds (canaries), $600 (\\u00a3385.41); last year: same . \\u2014 Five gold rings, $750 (\\u00a3481.76); last year: same . \\u2014 Six geese-a-laying, $360 (\\u00a3231.25); last year: $210 (\\u00a3134.89) \\u2014 Seven swans a-swimming, $7,000 (\\u00a34,496.45); last year: same . \\u2014 Eight maids a-milking, $58 (\\u00a337.26); last year: same . \\u2014 Nine ladies dancing (per performance), $7,553 (\\u00a34,851.67); last year: same . \\u2014 10 lords a-leaping (per performance), $5,348 (\\u00a33,433.38); last year: $5,243 (\\u00a33,365.97) \\u2014 11 pipers piping (per performance), $2,635 (\\u00a31,691.65); last year: same . \\u2014 12 drummers drumming (per performance), $2,855 (\\u00a32855); last year: same .\",\n          \"By . Daily Mail Reporter . PUBLISHED: . 22:59 EST, 16 February 2014 . | . UPDATED: . 22:59 EST, 16 February 2014 . A childhood friend of embattled New Jersey Governor Chris Christie has been dragged into the Bridgegate scandal, with emails revealing he gave a key Port Authority official a tour of the traffic chaos. In the latest twist, emails show Port Authority police officer Thomas 'Chip' Michaels was on the George Washington Bridge with David Wildstein, the official who ordered the road closures, when the disaster unfolded. The messages also reveal Michaels was aware of the plan to flood Fort Lee with traffic the . day before the controversial lane closings in September last year and kept Wildstein updated. It is not clear if Michaels will be implicated in the scandal. Thrown under the bus: It has been revealed that Chris Christie's long time friend, Port Authority police officer Thomas 'Chip' Michaels (left), was involved in the infamous George Washington Bridge lane closures . They go way back: New Jersey Governor Chris Christie (pictured) and Thomas Michaels reportedly grew up together in Livingston, New Jersey, and Michaels recently coached Christie's son in little league hockey . Michaels grew up with Christie in Livingston and even coached the Governor's son in Little League hockey. He joined the Port Authority Police Department in 1998, according to payroll records. The troubling emails were today released by MSNBC's Steve Kornacki and can be found at the bottom of this page. On September 8, a day before the lane closures, Michaels emailed . Port Authority Captain Darcy Licorish, asking, 'Is there going to be a new . traffic pattern installed for Monday the 9th?' In a 7.28am email on the first day of the traffic snarl, Wildstein informed Robert Durando, general manager of the bridge, 'Going to take a ride with chip and see how it looks.' 'Want me to pik u up. Its fkd up here (sic),' Michaels, a 15-year Port Authority officer, told Wildstein in a text before the tour. Michaels later texted Wildstein, 'I may have idea to mak ths beter (sic)'. The text-message exchanges between . Michaels and Wildstein were included in documents subpoenaed by a New . Jersey legislative committee investigating the bridge scandal. Scandalous: Emails released today show Michaels took David Wildstein (pictured) on a tour of the George Washington Bridge traffic snarl . The Fort Lee lane closure scandal has thrown Christie's presidential campaign into disarray, with mounting evidence showing his appointees conspired to . create traffic jams in Fort Lee, New Jersey on September 9 last year. The problems began after two toll lanes at a New . York-bound, toll plaza entrance to the George Washington . Bridge were closed to traffic from . Fort Lee before rush hour. The closures caused massive traffic snarls for days. Various reasons for the closure have been reported, including that it was political retribution against Fort Lee Mayor Mark Sokolich, a Democrat, for not endorsing Christie in the 2013 gubernatorial election. The Fort Lee lane closure scandal: Evidence shows appointees of New Jersey Governor Chris Christie conspired to create traffic jams in Fort Lee, New Jersey, starting at a New York-bound, toll plaza entrance to the George Washington Bridge (pictured) Christie has repeatedly denied knowing of the bridge debacle until it was over. But Wildstein's . lawyer has recently claim that 'evidence exists' Christie knew of the . lane closures while they were occurring. Wildstein resigned in December. Meanwhile, Michaels' brother, Jeffrey Michaels, is a top Republican lobbyist whose business Optimus Partners L.L.C. has flourished under the Christie administration. MSNBC reported Jeffrey Michaels served as the chief of staff to Republican Gov. Donald DiFrancesco just . over a decade ago before transitioning into lobbying. He also advised Christie in his 2009 run for governor. 'I\\u2019ve known him for a long time,' Jeffrey Michaels said in a TV interview after the campaign. 'We went to high school with \\u2013 we \\u2013 our families knew each other . from Livingston, and just stayed in close contact with him over the . years and was very pleased to help his campaign out with policy.' Local . reports and I.R.S. filings show Jeffrey Michaels has donated . extensively to pro-Christie groups. The donations include $25,000 to a PAC-created . to push the governor\\u2019s agenda and $20,450 since October 2012 to the . Republican Governors Association, which Christie heads. Documents and Emails Shed Light on the Lane Closures .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"highlights\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 9986,\n        \"samples\": [\n          \"Luis Suarez has completed his move to Barcelona, according to the Spanish club .\\nBarcelona sporting director Andoni Zubizarreta confirmed the news Wednesday .\\nDiego Costa has joined Chelsea from Atletico Madrid .\\nCosta will be joined by former Atletico teammate Filipe Luis.\",\n          \"Top seed Roger Federer loses against Tomas Berdych in U.S. Open quarterfinals .\\nAndy Murray comes from behind to beat Marin Cilic and reach semifinals .\\nAndy Roddick retires after losing in four sets to Juan Martin Del Potro in fourth round .\\nDefending champion Novak Djokovic will play Argentine star in quarterfinals .\",\n          \"Philip Hammond was speaking to CNN at United Nations summit in New York .\\nHe said the hunt for ISIS executioner with a London accent was moving on .\\nTold Situation Room host Wolf Blitzer: 'We are narrowing down the field'\\nTen days ago, Mr Hammond admitted he didn't know where hostages were .\\nCondemned terrorists for still holding Salford taxi driver Alan Henning, 47 .\\nSaid: 'He was a humanitarian worker who went with aid convoy to do good'\\nWords came as Mr Henning's wife was sent recording of him begging for life .\\nFile sent to Barbara Henning in apparent retaliation for US attacks on ISIS .\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "df_train"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-a113331a-ab25-4a53-899c-9dff9be18beb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>article</th>\n",
              "      <th>highlights</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0001d1afc246a7964130f43ae940af6bc6c57f01</td>\n",
              "      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n",
              "      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0002095e55fcbd3a2f366d9bf92a95433dc305ef</td>\n",
              "      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n",
              "      <td>Criminal complaint: Cop used his role to help ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>00027e965c8264c35cc1bc55556db388da82b07f</td>\n",
              "      <td>A drunk driver who killed a young woman in a h...</td>\n",
              "      <td>Craig Eccleston-Todd, 27, had drunk at least t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0002c17436637c4fe1837c935c04de47adb18e9a</td>\n",
              "      <td>(CNN) -- With a breezy sweep of his pen Presid...</td>\n",
              "      <td>Nina dos Santos says Europe must be ready to a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0003ad6ef0c37534f80b55b4235108024b407f0b</td>\n",
              "      <td>Fleetwood are the only team still to have a 10...</td>\n",
              "      <td>Fleetwood top of League One after 2-0 win at S...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a113331a-ab25-4a53-899c-9dff9be18beb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a113331a-ab25-4a53-899c-9dff9be18beb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a113331a-ab25-4a53-899c-9dff9be18beb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-417b7916-f152-407e-a1b8-1aecbf7a9d70\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-417b7916-f152-407e-a1b8-1aecbf7a9d70')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-417b7916-f152-407e-a1b8-1aecbf7a9d70 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                         id  \\\n",
              "0  0001d1afc246a7964130f43ae940af6bc6c57f01   \n",
              "1  0002095e55fcbd3a2f366d9bf92a95433dc305ef   \n",
              "2  00027e965c8264c35cc1bc55556db388da82b07f   \n",
              "3  0002c17436637c4fe1837c935c04de47adb18e9a   \n",
              "4  0003ad6ef0c37534f80b55b4235108024b407f0b   \n",
              "\n",
              "                                             article  \\\n",
              "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
              "1  (CNN) -- Ralph Mata was an internal affairs li...   \n",
              "2  A drunk driver who killed a young woman in a h...   \n",
              "3  (CNN) -- With a breezy sweep of his pen Presid...   \n",
              "4  Fleetwood are the only team still to have a 10...   \n",
              "\n",
              "                                          highlights  \n",
              "0  Bishop John Folda, of North Dakota, is taking ...  \n",
              "1  Criminal complaint: Cop used his role to help ...  \n",
              "2  Craig Eccleston-Todd, 27, had drunk at least t...  \n",
              "3  Nina dos Santos says Europe must be ready to a...  \n",
              "4  Fleetwood top of League One after 2-0 win at S...  "
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train= df_train[:10000]\n",
        "df_test= df_test[:10000]\n",
        "df_val= df_val[:10000]\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The dictionary, `contraction_mapping`, maps common English contractions to their expanded forms, aiding in text normalization for natural language processing tasks. For instance, it converts \"can't\" to \"cannot\" and \"you're\" to \"you are\", ensuring consistency in word forms for better analysis or processing of text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u7g7YKOif88z"
      },
      "outputs": [],
      "source": [
        "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n",
        "                           \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n",
        "                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
        "                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n",
        "                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n",
        "                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
        "                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n",
        "                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n",
        "                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n",
        "                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n",
        "                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
        "                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n",
        "                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n",
        "                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
        "                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n",
        "                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n",
        "                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n",
        "                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n",
        "                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n",
        "                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
        "                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
        "                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n",
        "                           \"you're\": \"you are\", \"you've\": \"you have\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This `text_cleaner` function is designed for cleaning text data in two possible ways, indicated by the `num` parameter: when `num` is 0, it removes stopwords from the text after cleaning; if not, it retains all words. The function lowers the text case, removes parenthetical text and double quotes, expands contractions using a predefined dictionary, eliminates possessive endings, and filters out non-alphabetic characters. It also discards single-character words to focus on more meaningful content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vqToe9LIf-FV"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def text_cleaner(text,num):\n",
        "    newString = text.lower()\n",
        "    newString = re.sub(r'\\([^)]*\\)', '', newString)\n",
        "    newString = re.sub('\"','', newString)\n",
        "    newString = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in newString.split(\" \")])\n",
        "    newString = re.sub(r\"'s\\b\",\"\",newString)\n",
        "    newString = re.sub(\"[^a-zA-Z]\", \" \", newString)\n",
        "    if(num==0):\n",
        "        tokens = [w for w in newString.split() if not w in stop_words]\n",
        "    else:\n",
        "        tokens=newString.split()\n",
        "    long_words=[]\n",
        "\n",
        "    for i in tokens:\n",
        "        if len(i)>1:\n",
        "            long_words.append(i)\n",
        "    return (\" \".join(long_words)).strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below code blocks apply the text_cleaner function to clean and preprocess text data from different data sets: training, testing, and validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WHIyVUFgCE3"
      },
      "outputs": [],
      "source": [
        "cleaned_text = []\n",
        "cleaned_text_test=[]\n",
        "cleaned_text_valid=[]\n",
        "\n",
        "for t in df_train['article']:\n",
        "    cleaned_text.append(text_cleaner(t,0))\n",
        "\n",
        "for t in df_test['article']:\n",
        "    cleaned_text_test.append(text_cleaner(t,0))\n",
        "\n",
        "for t in df_val['article']:\n",
        "    cleaned_text_valid.append(text_cleaner(t,0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rItkpZb-hIxv"
      },
      "outputs": [],
      "source": [
        "cleaned_summary = []\n",
        "cleaned_summary_test = []\n",
        "cleaned_summary_valid = []\n",
        "\n",
        "for t in df_train['highlights']:\n",
        "    cleaned_summary.append(text_cleaner(t,1))\n",
        "\n",
        "for t in df_test['highlights']:\n",
        "    cleaned_summary_test.append(text_cleaner(t,1))\n",
        "\n",
        "for t in df_val['highlights']:\n",
        "    cleaned_summary_valid.append(text_cleaner(t,1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri3PA9T3hng7"
      },
      "outputs": [],
      "source": [
        "df_train['text']=cleaned_text\n",
        "df_train['summary']=cleaned_summary\n",
        "\n",
        "df_test['text']=cleaned_text_test\n",
        "df_test['summary']=cleaned_summary_test\n",
        "\n",
        "df_val['text']=cleaned_text_valid\n",
        "df_val['summary']=cleaned_summary_valid"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below Code Blocks have Data Preprocessing steps where we have dropped NA blocks and also added start and finih to our summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAKCRIuFi8qU"
      },
      "outputs": [],
      "source": [
        "df_train.replace('', np.nan, inplace=True)\n",
        "df_train.dropna(axis=0,inplace=True)\n",
        "\n",
        "df_test.replace('', np.nan, inplace=True)\n",
        "df_test.dropna(axis=0,inplace=True)\n",
        "\n",
        "df_val.replace('', np.nan, inplace=True)\n",
        "df_val.dropna(axis=0,inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "mZW9yl-gjBUV",
        "outputId": "cc01af0d-d953-4165-b547-f5e0487dc123"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "text_word_count = []\n",
        "summary_word_count = []\n",
        "\n",
        "# populate the lists with sentence lengths\n",
        "for i in df_train['text']:\n",
        "      text_word_count.append(len(i.split()))\n",
        "\n",
        "for i in df_train['summary']:\n",
        "      summary_word_count.append(len(i.split()))\n",
        "\n",
        "length_df = pd.DataFrame({'text':text_word_count, 'summary':summary_word_count})\n",
        "\n",
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNljAs_seSl3"
      },
      "outputs": [],
      "source": [
        "max_text_len=300\n",
        "max_summary_len=100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zlbKzx_jLZ9"
      },
      "outputs": [],
      "source": [
        "df_train['summary'] = df_test['summary'].apply(lambda x : 'sostok '+ x + ' eostok')\n",
        "df_test['summary'] = df_test['summary'].apply(lambda x : 'sostok '+ x + ' eostok')\n",
        "df_val['summary'] = df_test['summary'].apply(lambda x : 'sostok '+ x + ' eostok')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The below code blocks preprocesses text data for machine learning by converting text to sequences, pruning rare words, and padding sequences to a uniform length for model training. It uses a `Tokenizer` to create and refine a vocabulary based on word frequency, ensuring that the dataset is optimized for relevance and consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSzxpTZWjTaf"
      },
      "outputs": [],
      "source": [
        "x_train=df_train['text'].to_numpy()\n",
        "y_train=df_train['summary'].to_numpy()\n",
        "x_test=df_test['text'].to_numpy()\n",
        "y_test=df_test['summary'].to_numpy()\n",
        "x_validate=df_val['text'].to_numpy()\n",
        "y_validate=df_val['summary'].to_numpy()\n",
        "\n",
        "x_tokenizer = Tokenizer()\n",
        "x_tokenizer.fit_on_texts(list(x_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJ6JkY_jjZ6P",
        "outputId": "25de9383-bc28-488e-999b-f14e9a6a15bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 53.49472950017437\n",
            "Total Coverage of rare words: 1.949848018235144\n"
          ]
        }
      ],
      "source": [
        "thresh=4\n",
        "\n",
        "cnt=0\n",
        "tot_cnt=0\n",
        "freq=0\n",
        "tot_freq=0\n",
        "\n",
        "for key,value in x_tokenizer.word_counts.items():\n",
        "    tot_cnt=tot_cnt+1\n",
        "    tot_freq=tot_freq+value\n",
        "    if(value<thresh):\n",
        "        cnt=cnt+1\n",
        "        freq=freq+value\n",
        "\n",
        "print(\"% of rare words in vocabulary:\", (cnt/tot_cnt)*100)\n",
        "print(\"Total Coverage of rare words:\", (freq/tot_freq)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBGhFntnjwPx"
      },
      "outputs": [],
      "source": [
        "x_tokenizer = Tokenizer(num_words=tot_cnt-cnt) # num_words: the maximum number of words to keep, based on word frequency.\n",
        "x_tokenizer.fit_on_texts(list(x_train))\n",
        "\n",
        "x_train_seq = x_tokenizer.texts_to_sequences(x_train)\n",
        "x_test_seq = x_tokenizer.texts_to_sequences(x_test)\n",
        "x_val_seq = x_tokenizer.texts_to_sequences(x_validate)\n",
        "x_train = pad_sequences(x_train_seq,maxlen=max_text_len,padding='post')\n",
        "x_test = pad_sequences(x_test_seq,maxlen=max_text_len,padding='post')\n",
        "x_validate = pad_sequences(x_val_seq,maxlen=max_text_len,padding='post')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pvRh8FLVjxo6",
        "outputId": "845e34c9-fa9f-4d0d-9b14-379a25c1184e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "41340"
            ]
          },
          "execution_count": 69,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_vocab = x_tokenizer.num_words +1\n",
        "x_vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NL2Z5XCij0XU"
      },
      "outputs": [],
      "source": [
        "y_tokenizer = Tokenizer()\n",
        "y_tokenizer.fit_on_texts(list(y_train))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVVi4Hn1j27F",
        "outputId": "3420f763-a5ab-4c33-99eb-b915f48136b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "% of rare words in vocabulary: 65.40604208448731\n",
            "Total Coverage of rare words: 5.9311357576226\n"
          ]
        }
      ],
      "source": [
        "thresh=4\n",
        "\n",
        "count=0\n",
        "total_count=0\n",
        "frequency=0\n",
        "total_frequency=0\n",
        "\n",
        "for key,value in y_tokenizer.word_counts.items():\n",
        "    total_count=total_count+1\n",
        "    total_frequency=total_frequency+value\n",
        "    if(value<thresh):\n",
        "        count=count+1\n",
        "        frequency=frequency+value\n",
        "\n",
        "print(\"% of rare words in vocabulary:\",(count/total_count)*100)\n",
        "print(\"Total Coverage of rare words:\",(frequency/total_frequency)*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvzbVvvfj53v",
        "outputId": "383b9c0a-14a8-4c62-bd17-e1b424344488"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Size of vocabulary in Y = 10868\n"
          ]
        }
      ],
      "source": [
        "#prepare a tokenizer for reviews on training data\n",
        "y_tokenizer = Tokenizer(num_words=total_count-count)\n",
        "y_tokenizer.fit_on_texts(list(y_train))\n",
        "\n",
        "#convert text sequences into integer sequences (i.e one hot encode the text in Y)\n",
        "y_train_seq    =   y_tokenizer.texts_to_sequences(y_train)\n",
        "y_test_seq   =   y_tokenizer.texts_to_sequences(y_test)\n",
        "y_val_seq   =   y_tokenizer.texts_to_sequences(y_validate)\n",
        "#padding zero upto maximum length\n",
        "y_train    =   pad_sequences(y_train_seq, maxlen=max_text_len, padding='post')\n",
        "y_test   =   pad_sequences(y_test_seq, maxlen=max_text_len, padding='post')\n",
        "y_validate   =   pad_sequences(y_val_seq, maxlen=max_text_len, padding='post')\n",
        "#size of vocabulary\n",
        "y_vocab  =   y_tokenizer.num_words +1\n",
        "print(\"Size of vocabulary in Y = {}\".format(y_vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This code filters out training, testing and validaiton examples where the target sequence contains exactly two non-zero entries, storing the indices of such examples in `ind`. It then removes these specific entries from both the `x` and `y` arrays to refine the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dy7m0ZInj89L"
      },
      "outputs": [],
      "source": [
        "ind=[]\n",
        "for i in range(len(y_train)):\n",
        "    count=0\n",
        "    for j in y_train[i]:\n",
        "        if j!=0:\n",
        "            count=count+1\n",
        "    if(count==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_train=np.delete(y_train,ind, axis=0)\n",
        "x_train=np.delete(x_train,ind, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VCJ3lo1-kBZm"
      },
      "outputs": [],
      "source": [
        "ind=[]\n",
        "for i in range(len(y_test)):\n",
        "    count=0\n",
        "    for j in y_test[i]:\n",
        "        if j!=0:\n",
        "            count=count+1\n",
        "    if(count==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_test=np.delete(y_test,ind, axis=0)\n",
        "x_test=np.delete(x_test,ind, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLcDeWhZlAku"
      },
      "outputs": [],
      "source": [
        "ind=[]\n",
        "for i in range(len(y_validate)):\n",
        "    count=0\n",
        "    for j in y_validate[i]:\n",
        "        if j!=0:\n",
        "            count=count+1\n",
        "    if(count==2):\n",
        "        ind.append(i)\n",
        "\n",
        "y_val=np.delete(y_validate,ind, axis=0)\n",
        "x_val=np.delete(x_validate,ind, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trqd42ZJeSl3"
      },
      "outputs": [],
      "source": [
        "word2id = x_tokenizer.word_index\n",
        "id2word = x_tokenizer.index_word\n",
        "vocab_size = x_vocab\n",
        "emded_size=300\n",
        "window_size=5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26O88v4ieSl4"
      },
      "source": [
        "## Produce Batches\n",
        "\n",
        " Below function, generate_context_word_pairs, generates context and target word pairs from a given corpus using a specified window size, for use in training word embedding models like Word2Vec. For each word in the corpus, it identifies the surrounding words within the window as the context and uses the current word as the target. The context words are padded to a fixed length for consistent input size, and the target words are one-hot encoded based on the vocabulary size. The function yields these context-target pairs for training, and the accompanying test code iterates through these pairs, printing the first 10 where the context contains no padding (indicated by 0)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Zq0FwFAeSl4",
        "outputId": "05c1fbc9-423e-4a07-8722-3ec6ee431081"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Context (X): ['associated', 'press', 'published', 'est', 'october', 'est', 'october', 'bishop', 'fargo', 'catholic'] -> Target (Y): updated\n",
            "Context (X): ['press', 'published', 'est', 'october', 'updated', 'october', 'bishop', 'fargo', 'catholic', 'diocese'] -> Target (Y): est\n",
            "Context (X): ['published', 'est', 'october', 'updated', 'est', 'bishop', 'fargo', 'catholic', 'diocese', 'north'] -> Target (Y): october\n",
            "Context (X): ['est', 'october', 'updated', 'est', 'october', 'fargo', 'catholic', 'diocese', 'north', 'dakota'] -> Target (Y): bishop\n",
            "Context (X): ['october', 'updated', 'est', 'october', 'bishop', 'catholic', 'diocese', 'north', 'dakota', 'exposed'] -> Target (Y): fargo\n",
            "Context (X): ['updated', 'est', 'october', 'bishop', 'fargo', 'diocese', 'north', 'dakota', 'exposed', 'potentially'] -> Target (Y): catholic\n",
            "Context (X): ['est', 'october', 'bishop', 'fargo', 'catholic', 'north', 'dakota', 'exposed', 'potentially', 'hundreds'] -> Target (Y): diocese\n",
            "Context (X): ['october', 'bishop', 'fargo', 'catholic', 'diocese', 'dakota', 'exposed', 'potentially', 'hundreds', 'church'] -> Target (Y): north\n",
            "Context (X): ['bishop', 'fargo', 'catholic', 'diocese', 'north', 'exposed', 'potentially', 'hundreds', 'church', 'members'] -> Target (Y): dakota\n",
            "Context (X): ['fargo', 'catholic', 'diocese', 'north', 'dakota', 'potentially', 'hundreds', 'church', 'members', 'fargo'] -> Target (Y): exposed\n",
            "Context (X): ['catholic', 'diocese', 'north', 'dakota', 'exposed', 'hundreds', 'church', 'members', 'fargo', 'grand'] -> Target (Y): potentially\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "def generate_context_word_pairs(corpus, window_size, vocab_size):\n",
        "    context_length = window_size*2\n",
        "    for words in corpus:\n",
        "        sentence_length = len(words)\n",
        "        for index, word in enumerate(words):\n",
        "            context_words = []\n",
        "            label_word   = []\n",
        "            start = index - window_size\n",
        "            end = index + window_size + 1\n",
        "\n",
        "            context_words.append([words[i]\n",
        "                                 for i in range(start, end)\n",
        "                                 if 0 <= i < sentence_length\n",
        "                                 and i != index])\n",
        "            label_word.append(word)\n",
        "\n",
        "            x = pad_sequences(context_words, maxlen=context_length)\n",
        "            y = tf.keras.utils.to_categorical(label_word, vocab_size)\n",
        "            yield (x, y)\n",
        "\n",
        "\n",
        "# Test this out for some samples\n",
        "i = 0\n",
        "for x, y in generate_context_word_pairs(corpus=x_train, window_size=window_size, vocab_size=vocab_size):\n",
        "    if 0 not in x[0]:\n",
        "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\n",
        "\n",
        "        if i == 10:\n",
        "            break\n",
        "        i += 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttlIjsp0eSl5"
      },
      "source": [
        "# Step 4: Creating the Word2Vec model\n",
        "\n",
        "In this step of the notebook, we set up and train a Continuous Bag of Words (CBOW) model using TensorFlow and Keras, optimized to run on a GPU. Key aspects include:\n",
        "\n",
        "- **Session and Device Configuration**: Begins by clearing any previous TensorFlow sessions and specifies GPU usage to enhance performance.\n",
        "- **Model Architecture**: Constructs the CBOW model with an Embedding layer for dense word representations, a Lambda layer to average these embeddings, and a Dense layer for predicting the target word using softmax.\n",
        "- **Model Compilation and Summary**: Compiles the model with categorical crossentropy and RMSprop, and prints a summary to verify the architecture.\n",
        "- **Weight Utilization**: After training, extracts and reuses the weights from the Embedding layer in a new setup, facilitating the application of learned embeddings to other NLP tasks.\n",
        "\n",
        "This configuration is essential for efficiently learning word contexts and embeddings that capture semantic relationships in text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vTy96kheSl5",
        "outputId": "0a7a339a-42c3-402f-9a5d-1d97233ccff0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 10, 300)           12402000  \n",
            "                                                                 \n",
            " lambda (Lambda)             (None, 300)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 41340)             12443340  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24845340 (94.78 MB)\n",
            "Trainable params: 24845340 (94.78 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as k\n",
        "from tensorflow.keras.layers import Input,LSTM,Embedding,Dense,Concatenate,TimeDistributed,Bidirectional,Attention, Lambda\n",
        "from tensorflow.keras.models import Model, Sequential,load_model\n",
        "\n",
        "k.clear_session()\n",
        "# with strategy.scope():\n",
        "with tf.device('/gpu:2'):\n",
        "    cbow = Sequential();\n",
        "    cbow.add(Embedding(input_dim=vocab_size, output_dim=emded_size, input_length=window_size*2));\n",
        "    cbow.add(Lambda(lambda x: k.mean(x, axis=1), output_shape=(emded_size,)));\n",
        "    cbow.add(Dense(vocab_size, activation='softmax'));\n",
        "    cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop');\n",
        "\n",
        "# view model summary\n",
        "print(cbow.summary());"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LRWqzYlyeSl5"
      },
      "outputs": [],
      "source": [
        "with tf.device('/gpu:2'):\n",
        "    weights = cbow.get_weights()[0]\n",
        "    weights = weights[1:]\n",
        "    weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa498SzdeSl6"
      },
      "outputs": [],
      "source": [
        "wv_layer = Embedding(vocab_size-1,\n",
        "                     300,\n",
        "                     mask_zero=False,\n",
        "                     weights=[weights],\n",
        "                     input_length=max_text_len,\n",
        "                     trainable=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ou5wKsXeeSl7"
      },
      "source": [
        "# Step 5: Creating RL Model\n",
        "\n",
        "Creating the architecture of RL model with the latent_dim of 256 and embedding dimensionality of 300 using bi-directional LSTM layers as encoders and decoders as stated in research paper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-621YhqDeSl7",
        "outputId": "08d2897a-0125-4b4a-cbc5-50086b5b5895"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer Encoder_BiLSTM_Layer1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer Encoder_BiLSTM_Layer1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer Encoder_BiLSTM_Layer1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer Encoder_BiLSTM_Layer2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer Encoder_BiLSTM_Layer2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer Encoder_BiLSTM_Layer2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
            "WARNING:tensorflow:Layer Decoder_LSTM_Layer will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"PG_Model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " Encoder_Inputs (InputLayer  [(None, 300)]                0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " embedding_1 (Embedding)     (None, 300, 300)             1240170   ['Encoder_Inputs[0][0]']      \n",
            "                                                          0                                       \n",
            "                                                                                                  \n",
            " bidirectional (Bidirection  [(None, 300, 512),           1140736   ['embedding_1[0][0]']         \n",
            " al)                          (None, 256),                                                        \n",
            "                              (None, 256),                                                        \n",
            "                              (None, 256),                                                        \n",
            "                              (None, 256)]                                                        \n",
            "                                                                                                  \n",
            " Decoder_Inputs (InputLayer  [(None, None)]               0         []                            \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " bidirectional_1 (Bidirecti  [(None, 300, 512),           1574912   ['bidirectional[0][0]']       \n",
            " onal)                        (None, 256),                                                        \n",
            "                              (None, 256),                                                        \n",
            "                              (None, 256),                                                        \n",
            "                              (None, 256)]                                                        \n",
            "                                                                                                  \n",
            " Decoder_Embedding_Inputs (  (None, None, 300)            3260400   ['Decoder_Inputs[0][0]']      \n",
            " Embedding)                                                                                       \n",
            "                                                                                                  \n",
            " concatenate_2 (Concatenate  (None, 512)                  0         ['bidirectional_1[0][1]',     \n",
            " )                                                                   'bidirectional_1[0][3]']     \n",
            "                                                                                                  \n",
            " concatenate_3 (Concatenate  (None, 512)                  0         ['bidirectional_1[0][2]',     \n",
            " )                                                                   'bidirectional_1[0][4]']     \n",
            "                                                                                                  \n",
            " Decoder_LSTM_Layer (LSTM)   [(None, None, 512),          1665024   ['Decoder_Embedding_Inputs[0][\n",
            "                              (None, 512),                          0]',                          \n",
            "                              (None, 512)]                           'concatenate_2[0][0]',       \n",
            "                                                                     'concatenate_3[0][0]']       \n",
            "                                                                                                  \n",
            " Attention_Layer (Attention  ((None, None, 512),          524800    ['bidirectional_1[0][0]',     \n",
            " Layer)                       (None, None, 300))                     'Decoder_LSTM_Layer[0][0]']  \n",
            "                                                                                                  \n",
            " Concat_layer (Concatenate)  (None, None, 1024)           0         ['Decoder_LSTM_Layer[0][0]',  \n",
            "                                                                     'Attention_Layer[0][0]']     \n",
            "                                                                                                  \n",
            " time_distributed (TimeDist  (None, None, 10868)          1113970   ['Concat_layer[0][0]']        \n",
            " ributed)                                                 0                                       \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 31707272 (120.95 MB)\n",
            "Trainable params: 31707272 (120.95 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "k.clear_session()\n",
        "\n",
        "latent_dim = 256\n",
        "embedding_dim = 300\n",
        "\n",
        "with tf.device('/gpu:2'):\n",
        "    # Encoder\n",
        "    encoder_inputs = Input(shape=(max_text_len, ),name='Encoder_Inputs')\n",
        "\n",
        "    # Embedding layer\n",
        "    enc_emb = wv_layer(encoder_inputs)\n",
        "\n",
        "    # Encoder LSTM 1\n",
        "    encoder_lstm1 = Bidirectional(LSTM(latent_dim, return_sequences=True,\n",
        "                         return_state=True, dropout=0.2,\n",
        "                         recurrent_dropout=0.2,name='Encoder_BiLSTM_Layer1'))\n",
        "    (encoder_output1, forward_state_h1, forward_state_c1,backward_state_h1,backward_state_c1) = encoder_lstm1(enc_emb)\n",
        "    state_h1=Concatenate()([forward_state_h1,backward_state_h1])\n",
        "    state_c1=Concatenate()([forward_state_c1,backward_state_c1])\n",
        "\n",
        "    # Encoder LSTM 2\n",
        "    encoder_lstm2 = Bidirectional(LSTM(latent_dim, return_sequences=True,\n",
        "                         return_state=True, dropout=0.2,\n",
        "                         recurrent_dropout=0.2,name='Encoder_BiLSTM_Layer2'))\n",
        "    (encoder_outputs, forward_state_h2, forward_state_c2,backward_state_h2,backward_state_c2) = encoder_lstm2(encoder_output1)\n",
        "    state_h=Concatenate()([forward_state_h2,backward_state_h2])\n",
        "    state_c=Concatenate()([forward_state_c2,backward_state_c2])\n",
        "\n",
        "    # Set up the decoder, using encoder_states as the initial state\n",
        "    decoder_inputs = Input(shape=(None, ),name='Decoder_Inputs')\n",
        "\n",
        "    # Embedding layer\n",
        "    dec_emb_layer = Embedding(y_vocab, embedding_dim, trainable=True, name='Decoder_Embedding_Inputs')\n",
        "    dec_emb = dec_emb_layer(decoder_inputs)\n",
        "\n",
        "    # Decoder LSTM1\n",
        "    decoder_lstm = LSTM(latent_dim*2, return_sequences=True,\n",
        "                        return_state=True, dropout=0.2,\n",
        "                        recurrent_dropout=0.2,name='Decoder_LSTM_Layer')\n",
        "    (decoder_outputs, decoder_fwd_state, decoder_back_state) = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
        "\n",
        "    # Attention Layer\n",
        "    attn_layer = AttentionLayer(name='Attention_Layer')\n",
        "    attn_out,attn_states=attn_layer([encoder_outputs,decoder_outputs])\n",
        "\n",
        "    decoder_concat_input=Concatenate(axis=-1,name='Concat_layer')([decoder_outputs,attn_out])\n",
        "    # Dense layer\n",
        "    decoder_dense = TimeDistributed(Dense(y_vocab, activation='softmax',name='TimeDistribution_Layer'))\n",
        "    decoder_outputs = decoder_dense(decoder_concat_input)\n",
        "\n",
        "    # Define the model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_outputs,name='PG_Model')\n",
        "\n",
        "    model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xr0rzIs8l0zM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import math\n",
        "# learning rate schedule\n",
        "def step_decay(epoch):\n",
        "    initial_lrate = 0.002\n",
        "    drop = 0.75\n",
        "    epochs_drop = 1.0\n",
        "    lrate = initial_lrate * math.pow(drop, math.floor((epoch)/epochs_drop))\n",
        "    return lrate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYcKzyEHeSl7"
      },
      "source": [
        "## Adam Optimizer\n",
        "\n",
        "This line configures a model to use the Adam optimizer with a low learning rate of 0.0001 for finer training control, and specifies `sparse_categorical_crossentropy` as the loss function for handling classification tasks with integer labels; it also sets up a learning rate scheduler to adjust the rate based on the `step_decay` function, enhancing optimization efficiency over training epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCVnOieaeSl8"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy')\n",
        "lrate = LearningRateScheduler(step_decay,verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mcq4xoxeSl8"
      },
      "source": [
        "## Early Stopping\n",
        "\n",
        "It will stop training the model if the val_loss does not decreases after 4 consecutive epochs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwWC2xNdeSl8"
      },
      "outputs": [],
      "source": [
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=4)\n",
        "\n",
        "checkpoint_filepath = 'RL Model'\n",
        "checkpoint= tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    verbose=1,\n",
        "    save_best_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kp1z6ddCeSl9",
        "outputId": "d49d43ee-21e3-4d41-a8b4-dff509d3192d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "157/157 [==============================] - ETA: 0s - loss: 2.7248\n",
            "Epoch 1: val_loss improved from inf to 1.27628, saving model to RL Model\n",
            "157/157 [==============================] - 973s 6s/step - loss: 2.7248 - val_loss: 1.2763\n",
            "Epoch 2/20\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.2183\n",
            "Epoch 2: val_loss improved from 1.27628 to 1.25725, saving model to RL Model\n",
            "157/157 [==============================] - 940s 6s/step - loss: 1.2183 - val_loss: 1.2572\n",
            "Epoch 3/20\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.2018\n",
            "Epoch 3: val_loss improved from 1.25725 to 1.24358, saving model to RL Model\n",
            "157/157 [==============================] - 934s 6s/step - loss: 1.2018 - val_loss: 1.2436\n",
            "Epoch 4/20\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.1900\n",
            "Epoch 4: val_loss improved from 1.24358 to 1.23199, saving model to RL Model\n",
            "157/157 [==============================] - 928s 6s/step - loss: 1.1900 - val_loss: 1.2320\n",
            "Epoch 5/20\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.1807\n",
            "Epoch 5: val_loss improved from 1.23199 to 1.22372, saving model to RL Model\n",
            "157/157 [==============================] - 921s 6s/step - loss: 1.1807 - val_loss: 1.2237\n",
            "Epoch 6/20\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.1725\n",
            "Epoch 6: val_loss improved from 1.22372 to 1.21675, saving model to RL Model\n",
            "157/157 [==============================] - 923s 6s/step - loss: 1.1725 - val_loss: 1.2168\n",
            "Epoch 7/20\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.1656\n",
            "Epoch 7: val_loss improved from 1.21675 to 1.21095, saving model to RL Model\n",
            "157/157 [==============================] - 923s 6s/step - loss: 1.1656 - val_loss: 1.2110\n",
            "Epoch 8/20\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.1593\n",
            "Epoch 8: val_loss improved from 1.21095 to 1.20675, saving model to RL Model\n",
            "157/157 [==============================] - 918s 6s/step - loss: 1.1593 - val_loss: 1.2067\n",
            "Epoch 9/20\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.1541\n",
            "Epoch 9: val_loss improved from 1.20675 to 1.20394, saving model to RL Model\n",
            "157/157 [==============================] - 918s 6s/step - loss: 1.1541 - val_loss: 1.2039\n",
            "Epoch 10/20\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.1497\n",
            "Epoch 10: val_loss improved from 1.20394 to 1.20178, saving model to RL Model\n",
            "157/157 [==============================] - 918s 6s/step - loss: 1.1497 - val_loss: 1.2018\n",
            "Epoch 11/20\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.1462\n",
            "Epoch 11: val_loss improved from 1.20178 to 1.20015, saving model to RL Model\n",
            "157/157 [==============================] - 918s 6s/step - loss: 1.1462 - val_loss: 1.2002\n",
            "Epoch 12/20\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.1434\n",
            "Epoch 12: val_loss improved from 1.20015 to 1.19988, saving model to RL Model\n",
            "157/157 [==============================] - 918s 6s/step - loss: 1.1434 - val_loss: 1.1999\n",
            "Epoch 13/20\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.1410\n",
            "Epoch 13: val_loss improved from 1.19988 to 1.19901, saving model to RL Model\n",
            "157/157 [==============================] - 918s 6s/step - loss: 1.1410 - val_loss: 1.1990\n",
            "Epoch 14/20\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.1390\n",
            "Epoch 14: val_loss improved from 1.19901 to 1.19866, saving model to RL Model\n",
            "157/157 [==============================] - 917s 6s/step - loss: 1.1390 - val_loss: 1.1987\n",
            "Epoch 15/20\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.1377\n",
            "Epoch 15: val_loss did not improve from 1.19866\n",
            "157/157 [==============================] - 903s 6s/step - loss: 1.1377 - val_loss: 1.1989\n",
            "Epoch 16/20\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.1362\n",
            "Epoch 16: val_loss did not improve from 1.19866\n",
            "157/157 [==============================] - 900s 6s/step - loss: 1.1362 - val_loss: 1.1995\n",
            "Epoch 17/20\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.1345\n",
            "Epoch 17: val_loss did not improve from 1.19866\n",
            "157/157 [==============================] - 900s 6s/step - loss: 1.1345 - val_loss: 1.1993\n",
            "Epoch 18/20\n",
            "157/157 [==============================] - ETA: 0s - loss: 1.1331\n",
            "Epoch 18: val_loss did not improve from 1.19866\n",
            "157/157 [==============================] - 900s 6s/step - loss: 1.1331 - val_loss: 1.1997\n",
            "Epoch 18: early stopping\n"
          ]
        }
      ],
      "source": [
        "with tf.device('/gpu:2'):\n",
        "    history = model.fit(\n",
        "        [x_train, y_train[:, :-1]],\n",
        "        y_train.reshape(y_train.shape[0], y_train.shape[1], 1)[:, 1:],\n",
        "        epochs=20,\n",
        "        callbacks=[es,checkpoint],\n",
        "        batch_size=64,\n",
        "        shuffle=True,\n",
        "        use_multiprocessing=True,\n",
        "        workers=-1,\n",
        "        validation_data=([x_validate, y_validate[:, :-1]],\n",
        "                         y_validate.reshape(y_validate.shape[0], y_validate.shape[1], 1)[:\n",
        "                         , 1:]),\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "a5whYJ94eSl9",
        "outputId": "2a62e1b9-f75a-4319-ed28-c678cd40de9f"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from matplotlib import pyplot\n",
        "pyplot.plot(history.history['loss'], label='train')\n",
        "pyplot.plot(history.history['val_loss'], label='test')\n",
        "pyplot.legend()\n",
        "pyplot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ux7OK5YeSl9"
      },
      "source": [
        "Next, lets build the dictionary to convert the index to word for target and source vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qxvILxGzeSl9"
      },
      "outputs": [],
      "source": [
        "reverse_target_word_index = y_tokenizer.index_word\n",
        "reverse_source_word_index = x_tokenizer.index_word\n",
        "target_word_index = y_tokenizer.word_index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HS6ssVggeSl-"
      },
      "source": [
        "## Inference\n",
        "\n",
        "Set up the inference for the encoder and decoder:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Lv1ha5ceSl-"
      },
      "outputs": [],
      "source": [
        "# Encode the input sequence to get the feature vector\n",
        "encoder_model = Model(inputs=encoder_inputs, outputs=[encoder_outputs, state_h, state_c])\n",
        "\n",
        "# Decoder setup\n",
        "# Below tensors will hold the states of the previous time step\n",
        "decoder_state_input_h = Input(shape=(latent_dim*2,))\n",
        "decoder_state_input_c = Input(shape=(latent_dim*2,))\n",
        "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim*2))\n",
        "\n",
        "# Get the embeddings of the decoder sequence\n",
        "dec_emb2= dec_emb_layer(decoder_inputs)\n",
        "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
        "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
        "\n",
        "#attention inference\n",
        "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
        "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
        "\n",
        "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
        "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
        "\n",
        "# Final decoder model\n",
        "decoder_model = Model(\n",
        "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
        "    [decoder_outputs2] + [state_h2, state_c2])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWBOXwtVeSl-"
      },
      "source": [
        "Let us define the functions to convert an integer sequence to a word sequence for summary as well as the reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrznTwzaeSl_"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(input_seq):\n",
        "    # Encode the input as state vectors.\n",
        "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
        "\n",
        "    # Generate empty target sequence of length 1.\n",
        "    target_seq = np.zeros((1,1))\n",
        "\n",
        "    # Populate the first word of target sequence with the start word.\n",
        "    target_seq[0, 0] = target_word_index['sostok']\n",
        "\n",
        "    stop_condition = False\n",
        "    decoded_sentence = ''\n",
        "    while not stop_condition:\n",
        "\n",
        "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
        "\n",
        "        # Sample a token\n",
        "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
        "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
        "\n",
        "        if(sampled_token!='eostok'):\n",
        "            decoded_sentence += ' '+sampled_token\n",
        "\n",
        "        # Exit condition: either hit max length or find stop word.\n",
        "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
        "            stop_condition = True\n",
        "\n",
        "        # Update the target sequence (of length 1).\n",
        "        target_seq = np.zeros((1,1))\n",
        "        target_seq[0, 0] = sampled_token_index\n",
        "\n",
        "        # Update internal states\n",
        "        e_h, e_c = h, c\n",
        "\n",
        "    return decoded_sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w3dmX90geSl_"
      },
      "outputs": [],
      "source": [
        "def seq2summary(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
        "            newString=newString+reverse_target_word_index[i]+' '\n",
        "    return newString\n",
        "\n",
        "def seq2text(input_seq):\n",
        "    newString=''\n",
        "    for i in input_seq:\n",
        "        if(i!=0):\n",
        "            newString=newString+reverse_source_word_index[i]+' '\n",
        "    return newString"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aW3kD91DoNYJ",
        "outputId": "6fe55e92-8241-4cc3-b101-d5faa6c6c2b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install rouge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTlypdcmeSl_"
      },
      "source": [
        "# 6. Rouge Scores\n",
        "\n",
        "Calculating the Rouge scores and saving it in CSV file for Graph Plotting and Analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzZbowf9eSl_",
        "outputId": "3c385f70-0745-4a49-8e7e-4fadcaca9b6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 431ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 435ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 438ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 446ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 428ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 436ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 433ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 436ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 430ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 440ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "{'rouge-1': {'r': 0.05448050378401006, 'p': 0.6, 'f': 0.09924692308142043}, 'rouge-2': {'r': 0.0032258064516129032, 'p': 0.016666666666666666, 'f': 0.005405405133674228}, 'rouge-l': {'r': 0.04908188482606907, 'p': 0.5333333333333333, 'f': 0.08935444996314085}}\n"
          ]
        }
      ],
      "source": [
        "from rouge import Rouge\n",
        "ROUGE = Rouge()\n",
        "Original_summary = list()\n",
        "Predicted_summary = list()\n",
        "x_test=x_test[:10]\n",
        "for i in range(len(x_test)):\n",
        "    Review = seq2text(x_test[i])\n",
        "    Original_summary.append(seq2summary(y_test[i]))\n",
        "    Predicted_summary.append(decode_sequence(x_test[i].reshape(1,max_text_len)))\n",
        "\n",
        "scores = ROUGE.get_scores(Predicted_summary,Original_summary,avg=True)\n",
        "print(scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRu4n_cQeSl_",
        "outputId": "0322cfba-b59a-415a-b508-aed9aa15f1db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    rouge-1   rouge-2   rouge-l\n",
            "r  0.054481  0.003226  0.049082\n",
            "p  0.600000  0.016667  0.533333\n",
            "f  0.099247  0.005405  0.089354\n"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame(scores)\n",
        "print(df)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "6b07439ecfe0abbcae4d197daa9d8514f9745d93ca336d1633c7b3ec373a5ae2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
